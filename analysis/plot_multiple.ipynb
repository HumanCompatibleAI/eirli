{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import copy\n",
    "import html\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import json\n",
    "import glob\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from numpy import trapz\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "import seaborn as sns \n",
    "from math import sqrt \n",
    "import pandas as pd \n",
    "import math\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "runs_directory = \"/userhome/30/xchen3/il-representations/runs\"\n",
    "\n",
    "path_translations = {\n",
    "    # when loading things like 'encoder_path' and 'policy_path' from configs,\n",
    "    # replace the thing on the left with the thing on the right\n",
    "#     \"/data/il-representations/\": gfs_mount,\n",
    "#     \"/root/il-rep/runs/\": os.path.join(gfs_mount, cluster_subpath),\n",
    "#     \"/home/sam/repos/il-representations/cloud/runs/\": os.path.join(gfs_mount, cluster_subpath)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search and Organize experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parent_relpath(sample_parent_file, local_root_dir):\n",
    "    \"\"\"Get root-relative path to a 'parent' directory, such as the directory\n",
    "    containing a saved encoder or policy. This is somewhat tricky because\n",
    "    we need to replace paths that might be different on svm/perceptron or on\n",
    "    a laptop compared to what they were on GCP. e.g. inside the Ray docker\n",
    "    container, '/root/il-rep/runs' maps to 'cluster-data/' in the GFS volume.\n",
    "    The `path_translations` variable handles all the necessary changes.\"\"\"\n",
    "    for prefix, replacement in path_translations.items():\n",
    "        if sample_parent_file.startswith(prefix):\n",
    "            sample_parent_file = replacement + sample_parent_file[len(prefix):]\n",
    "\n",
    "    full_path = os.path.abspath(sample_parent_file)\n",
    "    full_dir = os.path.dirname(full_path)\n",
    "    rel_dir = os.path.relpath(full_dir, local_root_dir)\n",
    "\n",
    "    return rel_dir\n",
    "\n",
    "class SubexperimentRun:\n",
    "    \"\"\"A SubexperimentRun associates all the information associated\n",
    "    with a run of a particular Sacred sub-experiment. That means a\n",
    "    run (single execution) of the 'repl', 'il_train', or 'il_test'\n",
    "    experiments.\"\"\"\n",
    "    def __init__(self, subexp_dir, experiment_dir_root):\n",
    "        # Subexperiment dir is used as a unique identifier.\n",
    "        # We strip out the leading 'experiment_dir_root' to shorten identifiers.\n",
    "        subexp_dir = os.path.abspath(subexp_dir)\n",
    "        experiment_dir_root = os.path.abspath(experiment_dir_root)\n",
    "        self.ident = os.path.relpath(subexp_dir, experiment_dir_root)\n",
    "        self.subexp_dir = subexp_dir\n",
    "        self.experiment_dir_root = experiment_dir_root\n",
    "\n",
    "        # usually paths are like, e.g., 'chain_runs/repl/42' or\n",
    "        # 'chain_runs/il_train/13'; if we take the second last component,\n",
    "        # we should get the mode\n",
    "        self.mode = os.path.split(os.path.split(subexp_dir)[0])[1]\n",
    "        assert self.mode in {'repl', 'il_train', 'il_test'}, (mode, subexp_dir)\n",
    "\n",
    "        # Load experiment config\n",
    "        config_path = os.path.join(subexp_dir, 'config.json')\n",
    "        with open(config_path, 'r') as fp:\n",
    "            self.config = json.load(fp)\n",
    "\n",
    "        # Store a path to relevant progress.csv file (only for il_train/repl)\n",
    "        progress_path = os.path.join(subexp_dir, \"progress.csv\")\n",
    "        if os.path.exists(progress_path):\n",
    "            self.progress_path = progress_path\n",
    "        else:\n",
    "            self.progress_path = None\n",
    "\n",
    "        # Store a path to relevant eval.json file\n",
    "        eval_json_path = os.path.join(subexp_dir, \"eval.json\")\n",
    "        if os.path.exists(eval_json_path):\n",
    "            self.eval_json_path = eval_json_path\n",
    "        else:\n",
    "            self.eval_json_path = None\n",
    "\n",
    "        # Infer the .ident attribute for the parent experiment\n",
    "        # (if it exists)\n",
    "        if self.mode == 'il_train' and self.config.get('encoder_path') is not None:\n",
    "            encoder_relpath = get_parent_relpath(\n",
    "                self.config['encoder_path'], experiment_dir_root)\n",
    "            # The relpath is going to be something like\n",
    "            # \"chain_runs/10/repl/5/checkpoints/representation_encoder\".\n",
    "            # We heuristically remove the last two parts.\n",
    "            # (this definitely breaks on Windows…)\n",
    "            encoder_relpath = '/'.join(encoder_relpath.split('/')[:-2])\n",
    "            self.parent_ident = encoder_relpath\n",
    "        elif self.mode == 'il_test':\n",
    "            policy_relpath = get_parent_relpath(\n",
    "                self.config['policy_dir'], experiment_dir_root)\n",
    "            self.parent_ident = policy_relpath\n",
    "        else:\n",
    "            # \"repl\" runs and \"il_train\" runs without an encoder_path\n",
    "            # have no parents\n",
    "            assert self.mode == 'repl' \\\n",
    "              or (self.mode == 'il_train' and self.config.get('encoder_path') is None), \\\n",
    "               (self.mode, self.config.get('encoder_path'))\n",
    "            self.parent_ident = None\n",
    "            \n",
    "        # HACK: adding a use_repl key so that we can see whether il_train runs used repL\n",
    "        if self.mode == 'il_train':\n",
    "            self.config['use_repl'] = self.parent_ident is not None\n",
    "\n",
    "    def get_merged_config(self, index):\n",
    "        \"\"\"Get a 'merged' config dictionary for this subexperiment and\n",
    "        all of its parents. The dict will have a format like this:\n",
    "        \n",
    "        {\"benchmark\": {…}, \"il_train\": {…}, \"il_test\": {…}, \"repl\": {…}}\n",
    "        \n",
    "        Note that some keys might not be present (e.g. if this is a `repl` run,\n",
    "        it will not have the `il_train` key; if this is an `il_train` run with\n",
    "        no parent, then the `repl` key will be absent).\"\"\"\n",
    "        config = {self.mode: dict(self.config)}\n",
    "        extract_keys = ('env_cfg', 'venv_opts', 'env_data')\n",
    "        for extract_key in extract_keys:\n",
    "            if extract_key in config[self.mode]:\n",
    "                # move 'benchmark' key to the top because that ingredient name is\n",
    "                # shared between il_train, and il_test experiments\n",
    "                config[extract_key] = config[self.mode][extract_key]\n",
    "                del config[self.mode][extract_key]\n",
    "        parent = self.get_parent(index)\n",
    "        if parent is not None:\n",
    "            # TODO: merge this properly, erroring on incompatible duplicate\n",
    "            # keys. I think Cody has code for this.\n",
    "            config.update(parent.get_merged_config(index))\n",
    "        return config\n",
    "    \n",
    "    def get_parent(self, index):\n",
    "        if self.parent_ident is None:\n",
    "            return None\n",
    "        return index.get_subexp(self.parent_ident)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.ident)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, SubexperimentRun):\n",
    "            return NotImplemented\n",
    "        return self.ident == other.ident\n",
    "\n",
    "class SubexperimentIndex:\n",
    "    \"\"\"An index of subexperiments. For now this just supports\n",
    "    looking up experiments by identifier. Later it might support\n",
    "    lookup by attributes.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.subexp_by_ident = {}\n",
    "        \n",
    "    def add_subexp(self, subexp):\n",
    "        if subexp.ident in self.subexp_by_ident:\n",
    "            raise ValueError(\"duplicate subexperiment:\", subexp)\n",
    "        self.subexp_by_ident[subexp.ident] = subexp\n",
    "        \n",
    "    def get_subexp(self, ident):\n",
    "        return self.subexp_by_ident[ident]\n",
    "    \n",
    "    def search(self, **attrs):\n",
    "        \"\"\"Find a subexperiment with attributes matching the values\n",
    "        given in 'attrs'.\"\"\"\n",
    "        results = []\n",
    "        for subexp in self.subexp_by_ident.values():\n",
    "            for k, v in attrs.items():\n",
    "                if getattr(subexp, k) != v:\n",
    "                    break\n",
    "            else:\n",
    "                results.append(subexp)\n",
    "        return results\n",
    "\n",
    "def get_experiment_directories(root_dir, skip_skopt=True):\n",
    "    \"\"\"Look for directories that end in a sequence of numbers, and contain a\n",
    "    grid_search subdirectory.\"\"\"\n",
    "    expt_pat = re.compile(r'^.*/(il_test|il_train|repl)/\\d+$')\n",
    "    ignore_pat = re.compile(r'^.*/(grid_search|_sources)$')  # ignore the grid_search subdir\n",
    "    expt_dirs = set()\n",
    "    for root, dirs, files in os.walk(root_dir, followlinks=True, topdown=True):\n",
    "        if ignore_pat.match(root):\n",
    "            del dirs[:]\n",
    "            continue\n",
    "            \n",
    "        # check whether tihs is a skopt dir\n",
    "        if skip_skopt and 'grid_search' in dirs:\n",
    "            gs_files = os.listdir(os.path.join(root, 'grid_search'))\n",
    "            if any(s.startswith('search-alg-') for s in gs_files):\n",
    "                # this is a skopt dir, skip it\n",
    "                print(\"skipping skopt directory in\", root)\n",
    "                del dirs[:]\n",
    "                continue\n",
    "\n",
    "        found_match = False\n",
    "        for d in dirs:\n",
    "            d_path = os.path.abspath(os.path.join(root, d))\n",
    "            m = expt_pat.match(d_path)\n",
    "            if m is None:\n",
    "                continue  # no match\n",
    "            expt_dirs.add(d_path)\n",
    "            found_match = True\n",
    "\n",
    "        if found_match:\n",
    "            del dirs[:]  # don't recurse\n",
    "    return sorted(expt_dirs)\n",
    "\n",
    "# Find all experiment directories (i.e. directories containing a grid_search\n",
    "# subdir)\n",
    "def load_all_subexperiments(root_dir, skip_skopt=True):\n",
    "    \"\"\"Find all experiment run subdirectories, and create SubexperimentIndex objects for them.\"\"\"\n",
    "    print(\"Searching for experiment directories (might take a minute or two)\")\n",
    "    all_expt_directories = get_experiment_directories(root_dir, skip_skopt=skip_skopt)\n",
    "    print(\"Loading experiments (might take another minute or two)\")\n",
    "    index = SubexperimentIndex()\n",
    "    for expt_dir in all_expt_directories:\n",
    "        subexp = SubexperimentRun(expt_dir, root_dir)\n",
    "        index.add_subexp(subexp)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subexp_index = load_all_subexperiments(runs_directory, skip_skopt=True)\n",
    "print('Discovered', len(subexp_index.subexp_by_ident), 'subexperiments')\n",
    "\n",
    "test_expts = subexp_index.search(mode='il_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot return curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_dict(eval_files, eval_mode=None):\n",
    "    \"\"\"\n",
    "    Given a set of eval_files, read the files one by one, and return a sorted result_dict \n",
    "    with keys 'n_update' (list) and 'return_mean' (list). The param eval_mode is for seleting \n",
    "    train/test level for Procgen.\n",
    "    \"\"\"\n",
    "    result_dict = {'n_update': [], 'return_mean': []}\n",
    "\n",
    "    for eval_file in eval_files:\n",
    "        with open(eval_file) as f:\n",
    "            test_result = json.load(f)\n",
    "            \n",
    "        policy_name = test_result['policy_path'].split('/')[-1]\n",
    "        nupdate = int(policy_name.split('_')[-2])\n",
    "        \n",
    "        if eval_mode:\n",
    "            assert eval_mode in test_result.keys()\n",
    "            return_mean = test_result[eval_mode]['return_mean']\n",
    "        else:\n",
    "            return_mean = test_result['return_mean']\n",
    "            \n",
    "        result_dict['return_mean'].append(return_mean)\n",
    "        result_dict['n_update'].append(nupdate)\n",
    "        \n",
    "        # The results might not be sorted according to nupdates, so we make sure\n",
    "        # they are sorted correctly here.\n",
    "        sorted_idx = sorted(range(len(result_dict['n_update'])), key=lambda k: result_dict['n_update'][k])\n",
    "        for key, value in result_dict.items():\n",
    "            result_dict[key] = [result_dict[key][idx] for idx in sorted_idx]\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def get_eval_files(test_dir):\n",
    "    return [os.path.join(test_dir, f) for f in os.listdir(test_dir) if 'eval' in f]\n",
    "\n",
    "        \n",
    "def get_result_and_add_to_dict(eval_files, exp_ident, target_dict, algo, task, eval_mode=None):\n",
    "    result_dict = get_result_dict(eval_files, eval_mode=eval_mode)\n",
    "    if exp_ident not in target_dict.keys():\n",
    "        target_dict[exp_ident] = dict(algo=algo, task=task, result_dict=[result_dict])\n",
    "    else:\n",
    "        target_dict[exp_ident]['result_dict'].append(result_dict)\n",
    "    return target_dict\n",
    "    \n",
    "        \n",
    "# Organize the results and index using exp_ident\n",
    "exp_dict = dict()\n",
    "for test_expt in test_expts:\n",
    "    eval_files = get_eval_files(test_expt.subexp_dir)\n",
    "    if len(eval_files) < 20:\n",
    "        continue\n",
    "    parent_config_path = os.path.join(Path(test_expt.config['policy_dir']).parent.absolute(),\n",
    "                                      'config.json')\n",
    "    if not os.path.exists(parent_config_path):\n",
    "        continue\n",
    "        \n",
    "    with open(parent_config_path, 'r') as f:\n",
    "        parent_config = json.load(f)\n",
    "    task_name = parent_config['env_cfg']['task_name']\n",
    "    \n",
    "\n",
    "    algo = parent_config['algo']\n",
    "    if 'encoder_path' in parent_config.keys() and parent_config['encoder_path'] is not None:\n",
    "        repl_config_path = os.path.join(Path(parent_config['encoder_path']).parent.parent.parent.absolute(),\n",
    "                                      'config.json')\n",
    "        with open(repl_config_path, 'r') as f:\n",
    "            repl_config = json.load(f)\n",
    "        algo = repl_config['algo']\n",
    "    exp_ident = f\"{algo}-{task_name}\"\n",
    "\n",
    "    benchmark_name = parent_config['env_cfg']['benchmark_name']\n",
    "    \n",
    "    if benchmark_name == 'procgen':\n",
    "        exp_dict = get_result_and_add_to_dict(eval_files, exp_ident+'-train_level', exp_dict, \n",
    "                                              algo, task_name+'-train_level', eval_mode='train_level')\n",
    "        exp_dict = get_result_and_add_to_dict(eval_files, exp_ident+'-test_level', exp_dict, \n",
    "                                              algo, task_name+'-test_level', eval_mode='test_level')\n",
    "    else:\n",
    "        exp_dict = get_result_and_add_to_dict(eval_files, exp_ident, exp_dict, algo, task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lineplot_from_df(df, x, y, title=None, y_label=None, save_path=None):\n",
    "    plt.figure()\n",
    "    ax = sns.lineplot(x=x, y=y, data=df)\n",
    "    sns.set(style='darkgrid')\n",
    "    \n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    if y_label:\n",
    "        ax.set_ylabel(y_label)\n",
    "    \n",
    "    fig = ax.get_figure()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path)\n",
    "\n",
    "\n",
    "score_df = pd.DataFrame()\n",
    "count = 0\n",
    "for exp_ident, exp_info in exp_dict.items():\n",
    "    # exp_result is a list of dict with keys 'n_update' and 'return_mean'. len(exp_result) is the number of \n",
    "    # random seeds.\n",
    "    algo, task, exp_result = exp_info['algo'], exp_info['task'], exp_info['result_dict']\n",
    "    merged_df = pd.DataFrame()\n",
    "    merged_df = merged_df.append([pd.DataFrame(result) for result in exp_result], ignore_index=True)\n",
    "    mean_df = merged_df.groupby('n_update').mean()\n",
    "    mean = np.mean(list(mean_df['return_mean']), axis=0)\n",
    "    score_df = score_df.append(pd.DataFrame(dict(algo=algo, task=task, mean=mean), index=[count]))\n",
    "    count += 1\n",
    "\n",
    "    lineplot_from_df(mean_df, x='n_update', y='return_mean', title=f'{exp_ident}({len(exp_result)})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print a table of mean return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df.pivot_table(index=['task'], columns='algo', values='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "train_folder = Path(policy_dir).parent.absolute()\n",
    "progress_path = os.path.join(train_folder, 'progress.csv')\n",
    "progress_df = pd.read_csv(progress_path)\n",
    "\n",
    "ax = sns.lineplot(x='n_updates', y='loss', data=progress_df)\n",
    "ax.set_title(f\"{benchmark_name}-{train_exp_ident}\")\n",
    "\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(f\"{test_dir}/loss_curve.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
