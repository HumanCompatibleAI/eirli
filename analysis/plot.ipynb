{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import copy\n",
    "import html\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import json\n",
    "import glob\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "# FIXME(sam): make cluster subpath selectable with a dropdown. Also, automatically infer\n",
    "# gfs_mount from the hostname (have sensible default for svm/perceptron)\n",
    "\n",
    "# this identifies data for a particular cluster on the GFS volume\n",
    "cluster_subpath = \"cluster-data/cluster-2020-12-21-runs-try1/\"\n",
    "# on svm I think gfs_mount is /scratch/sam/repl-vol/ or something like that\n",
    "gfs_mount = \"/scratch/sam/il-representations-gcp-volume/\"  # Google Filestore mount point (local)\n",
    "runs_directory = os.path.join(gfs_mount, cluster_subpath)\n",
    "path_translations = {\n",
    "    # when loading things like 'encoder_path' and 'policy_path' from configs,\n",
    "    # replace the thing on the left with the thing on the right\n",
    "    \"/data/il-representations/\": gfs_mount,\n",
    "    \"/root/il-rep/runs/\": os.path.join(gfs_mount, cluster_subpath),\n",
    "}\n",
    "# FIXME(sam): remove exp_name (I think we can rewrite interp code to not rely on it)\n",
    "exp_name = \"10\"\n",
    "exp_dir = os.path.join(runs_directory, 'chain_runs', exp_name)\n",
    "assert os.path.exists(exp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a table of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parent_relpath(sample_parent_file, local_root_dir):\n",
    "    \"\"\"Get root-relative path to a 'parent' directory, such as the directory\n",
    "    containing a saved encoder or policy. This is somewhat tricky because\n",
    "    we need to replace paths that might be different on svm/perceptron or on\n",
    "    a laptop compared to what they were on GCP. e.g. inside the Ray docker\n",
    "    container, '/root/il-rep/runs' maps to 'cluster-data/' in the GFS volume.\n",
    "    The `path_translations` variable handles all the necessary changes.\"\"\"\n",
    "    for prefix, replacement in path_translations.items():\n",
    "        if sample_parent_file.startswith(prefix):\n",
    "            sample_parent_file = replacement + sample_parent_file[len(prefix):]\n",
    "\n",
    "    full_path = os.path.abspath(sample_parent_file)\n",
    "    full_dir = os.path.dirname(full_path)\n",
    "    rel_dir = os.path.relpath(full_dir, local_root_dir)\n",
    "\n",
    "    return rel_dir\n",
    "\n",
    "class SubexperimentRun:\n",
    "    \"\"\"A SubexperimentRun associates all the information associated\n",
    "    with a run of a particular Sacred sub-experiment. That means a\n",
    "    run (single execution) of the 'repl', 'il_train', or 'il_test'\n",
    "    experiments.\"\"\"\n",
    "    def __init__(self, subexp_dir, experiment_dir_root):\n",
    "        # Subexperiment dir is used as a unique identifier.\n",
    "        # We strip out the leading 'experiment_dir_root' to shorten identifiers.\n",
    "        subexp_dir = os.path.abspath(subexp_dir)\n",
    "        experiment_dir_root = os.path.abspath(experiment_dir_root)\n",
    "        self.ident = os.path.relpath(subexp_dir, experiment_dir_root)\n",
    "        self.subexp_dir = subexp_dir\n",
    "        self.experiment_dir_root = experiment_dir_root\n",
    "\n",
    "        # usually paths are like, e.g., 'chain_runs/repl/42' or\n",
    "        # 'chain_runs/il_train/13'; if we take the second last component,\n",
    "        # we should get the mode\n",
    "        self.mode = os.path.split(os.path.split(subexp_dir)[0])[1]\n",
    "        assert self.mode in {'repl', 'il_train', 'il_test'}, (mode, subexp_dir)\n",
    "\n",
    "        # Load experiment config\n",
    "        config_path = os.path.join(subexp_dir, 'config.json')\n",
    "        with open(config_path, 'r') as fp:\n",
    "            self.config = json.load(fp)\n",
    "\n",
    "        # Store a path to relevant progress.csv file (only for il_train/repl)\n",
    "        progress_path = os.path.join(subexp_dir, \"progress.csv\")\n",
    "        if os.path.exists(progress_path):\n",
    "            self.progress_path = progress_path\n",
    "        else:\n",
    "            self.progress_path = None\n",
    "\n",
    "        # Store a path to relevant eval.json file\n",
    "        eval_json_path = os.path.join(subexp_dir, \"eval.json\")\n",
    "        if os.path.exists(eval_json_path):\n",
    "            self.eval_json_path = eval_json_path\n",
    "        else:\n",
    "            self.eval_json_path = None\n",
    "\n",
    "        # Infer the .ident attribute for the parent experiment\n",
    "        # (if it exists)\n",
    "        if self.mode == 'il_train' and self.config.get('encoder_path') is not None:\n",
    "            encoder_relpath = get_parent_relpath(\n",
    "                self.config['encoder_path'], experiment_dir_root)\n",
    "            # The relpath is going to be something like\n",
    "            # \"chain_runs/10/repl/5/checkpoints/representation_encoder\".\n",
    "            # We heuristically remove the last two parts.\n",
    "            # (this definitely breaks on Windows…)\n",
    "            encoder_relpath = '/'.join(encoder_relpath.split('/')[:-2])\n",
    "            self.parent_ident = encoder_relpath\n",
    "        elif self.mode == 'il_test':\n",
    "            policy_relpath = get_parent_relpath(\n",
    "                self.config['policy_path'], experiment_dir_root)\n",
    "            self.parent_ident = policy_relpath\n",
    "        else:\n",
    "            # \"repl\" runs and \"il_train\" runs without an encoder_path\n",
    "            # have no parents\n",
    "            assert self.mode == 'repl' \\\n",
    "              or (self.mode == 'il_train' and self.config.get('encoder_path') is None), \\\n",
    "               (self.mode, self.config.get('encoder_path'))\n",
    "            self.parent_ident = None\n",
    "            \n",
    "        # HACK: adding a use_repl key so that we can see whether il_train runs used repL\n",
    "        if self.mode == 'il_train':\n",
    "            self.config['use_repl'] = self.parent_ident is not None\n",
    "\n",
    "    def get_merged_config(self, index):\n",
    "        \"\"\"Get a 'merged' config dictionary for this subexperiment and\n",
    "        all of its parents. The dict will have a format like this:\n",
    "        \n",
    "        {\"benchmark\": {…}, \"il_train\": {…}, \"il_test\": {…}, \"repl\": {…}}\n",
    "        \n",
    "        Note that some keys might not be present (e.g. if this is a `repl` run,\n",
    "        it will not have the `il_train` key; if this is an `il_train` run with\n",
    "        no parent, then the `repl` key will be absent).\"\"\"\n",
    "        config = {self.mode: dict(self.config)}\n",
    "        extract_keys = ('env_cfg', 'venv_opts', 'env_data')\n",
    "        for extract_key in extract_keys:\n",
    "            if extract_key in config[self.mode]:\n",
    "                # move 'benchmark' key to the top because that ingredient name is\n",
    "                # shared between il_train, and il_test experiments\n",
    "                config[extract_key] = config[self.mode][extract_key]\n",
    "                del config[self.mode][extract_key]\n",
    "        parent = self.get_parent(index)\n",
    "        if parent is not None:\n",
    "            # TODO: merge this properly, erroring on incompatible duplicate\n",
    "            # keys. I think Cody has code for this.\n",
    "            config.update(parent.get_merged_config(index))\n",
    "        return config\n",
    "    \n",
    "    def get_parent(self, index):\n",
    "        if self.parent_ident is None:\n",
    "            return None\n",
    "        return index.get_subexp(self.parent_ident)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.ident)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, SubexperimentRun):\n",
    "            return NotImplemented\n",
    "        return self.ident == other.ident\n",
    "\n",
    "class SubexperimentIndex:\n",
    "    \"\"\"An index of subexperiments. For now this just supports\n",
    "    looking up experiments by identifier. Later it might support\n",
    "    lookup by attributes.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.subexp_by_ident = {}\n",
    "        \n",
    "    def add_subexp(self, subexp):\n",
    "        if subexp.ident in self.subexp_by_ident:\n",
    "            raise ValueError(\"duplicate subexperiment:\", subexp)\n",
    "        self.subexp_by_ident[subexp.ident] = subexp\n",
    "        \n",
    "    def get_subexp(self, ident):\n",
    "        return self.subexp_by_ident[ident]\n",
    "    \n",
    "    def search(self, **attrs):\n",
    "        \"\"\"Find a subexperiment with attributes matching the values\n",
    "        given in 'attrs'.\"\"\"\n",
    "        results = []\n",
    "        for subexp in self.subexp_by_ident.values():\n",
    "            for k, v in attrs.items():\n",
    "                if getattr(subexp, k) != v:\n",
    "                    break\n",
    "            else:\n",
    "                results.append(subexp)\n",
    "        return results\n",
    "\n",
    "def get_experiment_directories(root_dir, skip_skopt=True):\n",
    "    \"\"\"Look for directories that end in a sequence of numbers, and contain a\n",
    "    grid_search subdirectory.\"\"\"\n",
    "    expt_pat = re.compile(r'^.*/(il_test|il_train|repl)/\\d+$')\n",
    "    ignore_pat = re.compile(r'^.*/(grid_search|_sources)$')  # ignore the grid_search subdir\n",
    "    expt_dirs = set()\n",
    "    for root, dirs, files in os.walk(root_dir, followlinks=True, topdown=True):\n",
    "        if ignore_pat.match(root):\n",
    "            del dirs[:]\n",
    "            continue\n",
    "            \n",
    "        # check whether tihs is a skopt dir\n",
    "        if skip_skopt and 'grid_search' in dirs:\n",
    "            gs_files = os.listdir(os.path.join(root, 'grid_search'))\n",
    "            if any(s.startswith('search-alg-') for s in gs_files):\n",
    "                # this is a skopt dir, skip it\n",
    "                print(\"skipping skopt directory in\", root)\n",
    "                del dirs[:]\n",
    "                continue\n",
    "\n",
    "        found_match = False\n",
    "        for d in dirs:\n",
    "            d_path = os.path.abspath(os.path.join(root, d))\n",
    "            m = expt_pat.match(d_path)\n",
    "            if m is None:\n",
    "                continue  # no match\n",
    "            expt_dirs.add(d_path)\n",
    "            found_match = True\n",
    "\n",
    "        if found_match:\n",
    "            del dirs[:]  # don't recurse\n",
    "    return sorted(expt_dirs)\n",
    "\n",
    "# Find all experiment directories (i.e. directories containing a grid_search\n",
    "# subdir)\n",
    "def load_all_subexperiments(root_dir, skip_skopt=True):\n",
    "    \"\"\"Find all experiment run subdirectories, and create SubexperimentIndex objects for them.\"\"\"\n",
    "    print(\"Searching for experiment directories (might take a minute or two)\")\n",
    "    all_expt_directories = get_experiment_directories(root_dir, skip_skopt=skip_skopt)\n",
    "    print(\"Loading experiments (might take another minute or two)\")\n",
    "    index = SubexperimentIndex()\n",
    "    for expt_dir in all_expt_directories:\n",
    "        subexp = SubexperimentRun(expt_dir, root_dir)\n",
    "        index.add_subexp(subexp)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subexp_index = load_all_subexperiments(runs_directory, skip_skopt=True)\n",
    "print('Discovered', len(subexp_index.subexp_by_ident), 'subexperiments')\n",
    "\n",
    "test_expts = subexp_index.search(mode='il_test')\n",
    "\n",
    "test_expts[1].get_merged_config(subexp_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print a table of il_test results\n",
    "\n",
    "Shows a separate set of il_test results for each benchmark setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d):\n",
    "    \"\"\"Flatten a nested dict into a single-level dict with\n",
    "    'keys/separated/like/this'.\"\"\"\n",
    "    out_dict = {}\n",
    "    if isinstance(d, dict):\n",
    "        key_iter = d.items()\n",
    "    else:\n",
    "        assert isinstance(d, list), type(d)\n",
    "        # we flatten lists into dicts of the form {0: <first elem>, 1: <second elem>, …}\n",
    "        key_iter = ((str(idx), v) for idx, v in enumerate(d))\n",
    "    for key, value in key_iter:\n",
    "        if isinstance(value, (dict, list)):\n",
    "            value = flatten_dict(value)\n",
    "            for subkey, subvalue in value.items():\n",
    "                out_dict[f'{key}/{subkey}'] = subvalue\n",
    "        else:\n",
    "            out_dict[key] = value\n",
    "    return out_dict\n",
    "\n",
    "def combine_dicts_multiset(dicts):\n",
    "    \"\"\"Combine a series of dicts into a key-multiset mapping, where the\n",
    "    multiset measures how many times each observed value occurs for each\n",
    "    key.\"\"\"\n",
    "    count_dict = {}\n",
    "    for d in dicts:\n",
    "        for k, v in d.items():\n",
    "            if k not in count_dict:\n",
    "                count_dict[k] = collections.Counter()\n",
    "            count_dict[k][v] += 1\n",
    "    return count_dict\n",
    "\n",
    "def remove_inapplicable_keys(flat_dict):\n",
    "    \"\"\"Remove keys that do not make a difference from a flattened config dicts.\n",
    "    Totally heuristic, so might have to add more options to this later on.\"\"\"\n",
    "    remove_keys = set()\n",
    "    \n",
    "    # remove inapplicable benchmark keys\n",
    "    for benchmark_name in ['magical', 'dm_control']:\n",
    "        if flat_dict.get('env_cfg/benchmark_name') != benchmark_name:\n",
    "            for key in flat_dict:\n",
    "                # this will remove, e.g., dm_control keys from magical experiments\n",
    "                if key.startswith('env_cfg/' + benchmark_name) or key.startswith('env_data/' + benchmark_name):\n",
    "                    remove_keys.add(key)\n",
    "                    \n",
    "    # remove repl keys from things that don't use repL\n",
    "    if flat_dict.get('il_train/use_repl') is False:\n",
    "        for key in flat_dict:\n",
    "            if key.startswith('repl/'):\n",
    "                remove_keys.add(key)\n",
    "                    \n",
    "    return {k: v for k, v in flat_dict.items() if k not in remove_keys}\n",
    "\n",
    "def simplify_config_dicts(hierarchical_dicts,\n",
    "                          base_thresh=0.75,\n",
    "                          remove_seeds=True,\n",
    "                          prohibited_base_keys=('env_cfg/task_name', 'env_cfg/benchmark_name', ),\n",
    "                          force_remove_keys=('il_test/policy_path', 'il_train/encoder_path')):\n",
    "    \"\"\"Simplify flattened config dicts so that:\n",
    "    \n",
    "    0. They are totally flat.\n",
    "    1. They only contain keys for which values actually differ between\n",
    "       different dicts, and\n",
    "    2. If the value of some key is the same for at least a fraction\n",
    "       `base_thresh` of dicts, then that key is moved into a _base config_.\n",
    "       Returned dicts will only contain that key if they have a different\n",
    "       value from the base config one.\n",
    "    3. Optionally, remove all seed values from dicts.\n",
    "\n",
    "    This makes it more clear which values are actually changing.\"\"\"\n",
    "    # first flatten all dicts\n",
    "    dicts = [dict(flatten_dict(d)) for d in hierarchical_dicts]\n",
    "    \n",
    "    # remove seeds, if required\n",
    "    if remove_seeds:\n",
    "        for d in dicts:\n",
    "            for key in list(d.keys()):\n",
    "                if key.split('/')[-1] == 'seed':\n",
    "                    del d[key]\n",
    "                    \n",
    "    # make sure that every dict has every key\n",
    "    all_keys = set()\n",
    "    for d in dicts:\n",
    "        all_keys |= d.keys()\n",
    "    for d in dicts:\n",
    "        for new_key in all_keys - d.keys():\n",
    "            d[new_key] = None\n",
    "                    \n",
    "    # remove inapplicable keys\n",
    "    dicts = [remove_inapplicable_keys(d) for d in dicts]\n",
    "\n",
    "    # now figure out which keys we wish to remove or move to the base config\n",
    "    base_config = {}\n",
    "    remove_keys = set()\n",
    "    base_thresh_abs = len(dicts) * base_thresh\n",
    "    count_dict = combine_dicts_multiset(dicts)\n",
    "    for key, counter in count_dict.items():\n",
    "        if len(counter) == 1 or key in force_remove_keys:\n",
    "            # if all dicts have the same value for this key, we will\n",
    "            # remove it from output dicts\n",
    "            remove_keys.add(key)\n",
    "        elif key not in prohibited_base_keys:\n",
    "            # if most dicts have the same value for this key, then\n",
    "            # we add it to the base config\n",
    "            (max_count_item, max_count), = counter.most_common(1)\n",
    "            if max_count > base_thresh_abs:\n",
    "                base_config[key] = max_count_item\n",
    "\n",
    "    # remove keys that we are ignoring, or for which the corresponding value\n",
    "    # already exists in the base config\n",
    "    new_dicts = []\n",
    "    for old_dict in dicts:\n",
    "        new_dict = {}\n",
    "        for key, value in old_dict.items():\n",
    "            if key in remove_keys \\\n",
    "              or (key in base_config and base_config[key] == value):\n",
    "                continue  # skip this key\n",
    "            new_dict[key] = value\n",
    "        new_dicts.append(new_dict)\n",
    "    \n",
    "    return base_config, new_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_configs = [subexp.get_merged_config(subexp_index) for subexp in test_expts]\n",
    "base_config, flat_configs = simplify_config_dicts(all_configs)\n",
    "flat_config_tups = [tuple(sorted(d.items())) for d in flat_configs]\n",
    "subexp_by_benchmark = {}\n",
    "for flat_cfg, subexp in zip(flat_config_tups, test_expts):\n",
    "    bench_key = tuple((k, v) for k, v in flat_cfg if k.startswith('env_') or k.startswith('venv_'))\n",
    "    subexp_by_benchmark.setdefault(bench_key, []).append((flat_cfg, subexp))\n",
    "\n",
    "display(HTML('<p><strong>Base config</strong></p>'))\n",
    "display(HTML('<p>Unless specified otherwise, all config dicts include these keys:</p>'))\n",
    "print(base_config)\n",
    "    \n",
    "for idx, (bench_key, cfgs_subexps) in enumerate(subexp_by_benchmark.items(), start=1):\n",
    "    # print out benchmark details\n",
    "    display(HTML(f'<p><strong>Results for benchmark config &#35;{idx}</strong></p>'))\n",
    "    display(HTML(f'<p>Config:</p>'))\n",
    "    rows = [f'<tr><th>{html.escape(key)}</th><td>{html.escape(value)}</td></tr>' for key, value in bench_key]\n",
    "    display(HTML(f'<table>{\"\".join(rows)}</table>'))\n",
    "    display(HTML(f'<p>Runs:</p>'))\n",
    "    \n",
    "    # cluster subexperiments by config\n",
    "    by_cfg = {}\n",
    "    for tup_cfg, subexp in cfgs_subexps:\n",
    "        tup_cfg = tuple(k for k in tup_cfg if k not in bench_key)\n",
    "        by_cfg.setdefault(tup_cfg, []).append(subexp)\n",
    "        \n",
    "    # is this a magical run?\n",
    "    is_magical = True  # ('env_cfg/benchmark_name', 'magical') in bench_key\n",
    "\n",
    "    # load all eval.json files and figure out what columns we need\n",
    "    stats_dicts = {}\n",
    "    columns = set()\n",
    "    for _, subexp in cfgs_subexps:\n",
    "        if subexp.eval_json_path:\n",
    "            with open(subexp.eval_json_path, 'r') as fp:\n",
    "                eval_dict = json.load(fp)\n",
    "            if is_magical:\n",
    "                stats_dict = {\n",
    "                    '-'.join(env_dict['test_env'].split('-')[:2]): env_dict['mean_score']\n",
    "                    for env_dict in eval_dict['full_data']\n",
    "                }\n",
    "                stats_dict['Average on all envs'] = eval_dict['return_mean']\n",
    "            else:\n",
    "                stats_dict = {'return_mean': eval_dict['return_mean']}\n",
    "            stats_dicts[subexp] = stats_dict\n",
    "            columns |= stats_dict.keys()\n",
    "        else:\n",
    "            stats_dicts[subexp] = {}\n",
    "    columns = sorted(columns)\n",
    "    \n",
    "    # now produce a table with one row per config\n",
    "    table_parts = ['<table>']                                         # begin table\n",
    "    table_parts.append('<tr>')                                        # begin header row\n",
    "    table_parts.append('<th style=\"border-collapse: collapse;\">Config</th>')\n",
    "    table_parts.extend(f'<th style=\"border-collapse: collapse;\">{html.escape(col_name)}</th>' for col_name in columns)\n",
    "    table_parts.append('</tr>')                                       # end header row\n",
    "\n",
    "    for cfg, subexps in sorted(by_cfg.items(), key=lambda cfg_se: dict(cfg_se[0])['il_test/exp_ident']):\n",
    "        table_parts.append('<tr>')                                    # begin row\n",
    "\n",
    "        # cell containing config\n",
    "        if True:  # remove to show full config\n",
    "            d = dict(cfg)\n",
    "            exp_ident = d['il_test/exp_ident']\n",
    "            # bench_name = d['env_cfg/benchmark_name']\n",
    "            # task_name = d['env_cfg/task_name']\n",
    "            # desc_str = f'{exp_ident} ({bench_name}/{task_name})'\n",
    "            table_parts.append(f'<td style=\"border-collapse: collapse;\">{html.escape(exp_ident)}</td>')\n",
    "        else:\n",
    "            kv_cfg = ', '.join(f'{key}={value!r}' for key, value in cfg)\n",
    "            table_parts.append(f'<td style=\"max-width: 600px; border-collapse: collapse;\">{html.escape(kv_cfg)}</td>')\n",
    "            \n",
    "        # cells containing data\n",
    "        for column in columns:\n",
    "            column_values = [stats_dicts[subexp][column] for subexp in subexps\n",
    "                             if column in stats_dicts[subexp]]\n",
    "            if not column_values:\n",
    "                table_parts.append('<td style=\"border-collapse: collapse;\">-</td>')\n",
    "            else:\n",
    "                mean = np.mean(column_values)\n",
    "                std = np.std(column_values)\n",
    "                n = len(column_values)\n",
    "                table_parts.append(f'<td style=\"border-collapse: collapse;\">{mean:.3g}±{std:.1g} ({n})</td>')\n",
    "        \n",
    "        # cells containing values\n",
    "\n",
    "        table_parts.append('</tr>')                                   # end row\n",
    "    table_parts.append('</table>')                                    # end table\n",
    "    display(HTML(''.join(table_parts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print a table of il_train Area Under Loss Curve results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy import trapz\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# How many splits?\n",
    "num_split = 6\n",
    "\n",
    "# Exclude first n values in the loss list?\n",
    "start_count = 3\n",
    "\n",
    "def calculate_auc(y, dx=1):\n",
    "    return trapz(y, dx=dx)\n",
    "    \n",
    "train_expts = subexp_index.search(mode='il_train')\n",
    "all_configs = [subexp.get_merged_config(subexp_index) for subexp in train_expts]\n",
    "base_config, flat_configs = simplify_config_dicts(all_configs)\n",
    "flat_config_tups = [tuple(sorted(d.items())) for d in flat_configs]\n",
    "subexp_by_benchmark = {}\n",
    "for flat_cfg, subexp in zip(flat_config_tups, train_expts):\n",
    "    bench_key = tuple((k, v) for k, v in flat_cfg if k.startswith('env_') or k.startswith('venv_'))\n",
    "    subexp_by_benchmark.setdefault(bench_key, []).append((flat_cfg, subexp))\n",
    "\n",
    "display(HTML('<p><strong>Base config</strong></p>'))\n",
    "display(HTML('<p>Unless specified otherwise, all config dicts include these keys:</p>'))\n",
    "print(base_config)\n",
    "    \n",
    "for idx, (bench_key, cfgs_subexps) in enumerate(subexp_by_benchmark.items(), start=1):\n",
    "    # print out benchmark details\n",
    "    display(HTML(f'<p><strong>Results for benchmark config &#35;{idx}</strong></p>'))\n",
    "    display(HTML(f'<p>Config:</p>'))\n",
    "    rows = [f'<tr><th>{html.escape(key)}</th><td>{html.escape(value)}</td></tr>' for key, value in bench_key]\n",
    "    display(HTML(f'<table>{\"\".join(rows)}</table>'))\n",
    "    display(HTML(f'<p>Runs:</p>'))\n",
    "    \n",
    "    # cluster subexperiments by config\n",
    "    by_cfg = {}\n",
    "    for tup_cfg, subexp in cfgs_subexps:\n",
    "        tup_cfg = tuple(k for k in tup_cfg if k not in bench_key)\n",
    "        by_cfg.setdefault(tup_cfg, []).append(subexp)\n",
    "\n",
    "    # load all progress files and figure out what columns we need\n",
    "    stats_dicts = {}\n",
    "    columns = set()\n",
    "    for _, subexp in cfgs_subexps:\n",
    "        if subexp.progress_path:\n",
    "            df = pd.read_csv(subexp.progress_path)\n",
    "            step_length = len(df['loss']) // num_split\n",
    "            stats_dict = {}\n",
    "            for step in range(step_length, len(df['loss']), step_length):\n",
    "                label = f\"step {step:02d}\"\n",
    "                stats_dict[label] = calculate_auc(df['loss'][start_count:step])\n",
    "            stats_dict[f\"step {len(df['loss'])}\"] = calculate_auc(df['loss'][start_count:len(df['loss'])])\n",
    "            stats_dicts[subexp] = stats_dict\n",
    "            columns |= stats_dict.keys()\n",
    "        else:\n",
    "            stats_dicts[subexp] = {}\n",
    "    columns = sorted(columns)\n",
    "    \n",
    "    # now produce a table with one row per config\n",
    "    table_parts = ['<table>']                                         # begin table\n",
    "    table_parts.append('<tr>')                                        # begin header row\n",
    "    table_parts.append('<th style=\"border-collapse: collapse;\">Config</th>')\n",
    "    table_parts.extend(f'<th style=\"border-collapse: collapse;\">{html.escape(col_name)}</th>' for col_name in columns)\n",
    "    table_parts.append('</tr>')                                       # end header row\n",
    "\n",
    "    for cfg, subexps in sorted(by_cfg.items(), key=lambda cfg_se: dict(cfg_se[0])['il_train/exp_ident']):\n",
    "        table_parts.append('<tr>')                                    # begin row\n",
    "\n",
    "        # cell containing config\n",
    "        if True:  # remove to show full config\n",
    "            d = dict(cfg)\n",
    "            exp_ident = d['il_train/exp_ident']\n",
    "            # bench_name = d['env_cfg/benchmark_name']\n",
    "            # task_name = d['env_cfg/task_name']\n",
    "            # desc_str = f'{exp_ident} ({bench_name}/{task_name})'\n",
    "            table_parts.append(f'<td style=\"border-collapse: collapse;\">{html.escape(exp_ident)}</td>')\n",
    "        else:\n",
    "            kv_cfg = ', '.join(f'{key}={value!r}' for key, value in cfg)\n",
    "            table_parts.append(f'<td style=\"max-width: 600px; border-collapse: collapse;\">{html.escape(kv_cfg)}</td>')\n",
    "            \n",
    "        # cells containing data\n",
    "        for column in columns:\n",
    "            column_values = [stats_dicts[subexp][column] for subexp in subexps\n",
    "                             if column in stats_dicts[subexp]]\n",
    "            if not column_values:\n",
    "                table_parts.append('<td style=\"border-collapse: collapse;\">-</td>')\n",
    "            else:\n",
    "                mean = np.mean(column_values)\n",
    "                std = np.std(column_values)\n",
    "                n = len(column_values)\n",
    "                table_parts.append(f'<td style=\"border-collapse: collapse;\">{mean:.3g}±{std:.1g} ({n})</td>')\n",
    "        \n",
    "        # cells containing values\n",
    "\n",
    "        table_parts.append('</tr>')                                   # end row\n",
    "    table_parts.append('</table>')                                    # end table\n",
    "    display(HTML(''.join(table_parts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Prepare files for analyzing with Viskit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see a trial object's information, you can do print(trial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_files(experiments, exp_index, out_dir):\n",
    "    \"\"\"\n",
    "    Create a folder named `out_dir`. This really just copies over files from il_train or il_test, as appropriate.\n",
    "    For instance, if il_train looks like this:\n",
    "    \n",
    "    il_train\n",
    "    │   ├── 1\n",
    "    │   │   ├── ...\n",
    "    │   │   ├── config.json\n",
    "    │   │   └── progress.csv\n",
    "    │   └── _sources\n",
    "    …\n",
    "    \n",
    "    Then the ouptut will look like this:\n",
    "    ├── progress\n",
    "    │   └── 1\n",
    "    │       ├── params.json   (same as config.json)\n",
    "    │       └── progress.csv\n",
    "    …\n",
    "\n",
    "    After you run this, you can execute viskit with: python viskit/frontend.py path/to/out_dir/\n",
    "    \"\"\"\n",
    "    # compute merged configs (nested/hierarchical dicts), and\n",
    "    # also throw out experiments with no progress.csv\n",
    "    hierarchical_dicts = []\n",
    "    new_experiments = []\n",
    "    for experiment in experiments:\n",
    "        if not experiment.progress_path:\n",
    "            print(\"Skipping experiment\", experiment.ident, \"because it has no progress.csv\")\n",
    "            continue\n",
    "        merged_config = experiment.get_merged_config(exp_index)\n",
    "        hierarchical_dicts.append(merged_config)\n",
    "        new_experiments.append(experiment)\n",
    "    experiments = new_experiments\n",
    "\n",
    "    # first flatten all dicts\n",
    "    dicts = [dict(flatten_dict(d)) for d in hierarchical_dicts]\n",
    "    \n",
    "    # make sure that every dict has every key\n",
    "    all_keys = set()\n",
    "    for d in dicts:\n",
    "        all_keys |= d.keys()\n",
    "    for d in dicts:\n",
    "        for new_key in all_keys - d.keys():\n",
    "            d[new_key] = None\n",
    "    \n",
    "    # now generate outputs for experiments\n",
    "    for flat_config, experiment in zip(dicts, experiments):\n",
    "        exp_out_dir = os.path.join(out_dir, experiment.ident.replace('/', '-'))\n",
    "        os.makedirs(exp_out_dir, exist_ok=True)\n",
    "\n",
    "        params_json_path = os.path.join(exp_out_dir, 'params.json')\n",
    "        with open(params_json_path, 'w') as fp:\n",
    "            json.dump(flat_config, fp)\n",
    "\n",
    "        progress_out_path = os.path.join(exp_out_dir, 'progress.csv')\n",
    "        shutil.copyfile(experiment.progress_path, progress_out_path)\n",
    "\n",
    "prepare_files(subexp_index.search(mode='repl'), subexp_index, 'viskit-repl')\n",
    "prepare_files(subexp_index.search(mode='il_train'), subexp_index, 'viskit-il-train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the encoder interpretation videos. Each sub_exp might take one or two minutes to save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "train_expts = subexp_index.search(mode='il_train')\n",
    "all_configs = [subexp.get_merged_config(subexp_index) for subexp in train_expts]\n",
    "base_config, flat_configs = simplify_config_dicts(all_configs)\n",
    "flat_config_tups = [tuple(sorted(d.items())) for d in flat_configs]\n",
    "subexp_by_benchmark = {}\n",
    "for flat_cfg, subexp in zip(flat_config_tups, train_expts):\n",
    "    bench_key = tuple((k, v) for k, v in flat_cfg if k.startswith('env_') or k.startswith('venv_'))\n",
    "    subexp_by_benchmark.setdefault(bench_key, []).append((flat_cfg, subexp))\n",
    "\n",
    "# Create a folder to save videos\n",
    "Path(f\"./runs/{cluster_subpath.split('/')[1]}\").mkdir(parents=True, exist_ok=True)\n",
    "interp_algo = 'saliency'\n",
    "\n",
    "for idx, (bench_key, cfgs_subexps) in enumerate(subexp_by_benchmark.items(), start=1):\n",
    "    # cluster subexperiments by config\n",
    "    by_cfg = {}\n",
    "    for tup_cfg, subexp in cfgs_subexps:\n",
    "        tup_cfg = tuple(k for k in tup_cfg if k not in bench_key)\n",
    "        by_cfg.setdefault(tup_cfg, []).append(subexp)\n",
    "\n",
    "    for tup_cfg, subexp in by_cfg.items():\n",
    "        exp = subexp[0]\n",
    "        encoder_path = exp.config['encoder_path']\n",
    "        if encoder_path:\n",
    "            for prefix, replacement in path_translations.items():\n",
    "                if encoder_path.startswith(prefix):\n",
    "                    encoder_path = replacement + encoder_path[len(prefix):]\n",
    "            command = \"python ../src/il_representations/scripts/interpret.py with \"\n",
    "            command += f\"log_dir=runs/{cluster_subpath.split('/')[1]} \"\n",
    "            command += f\"env_cfg.benchmark_name={exp.config['env_cfg']['benchmark_name']} \"\n",
    "            command += f\"env_cfg.task_name={exp.config['env_cfg']['task_name']} \"\n",
    "            command += f\"save_video=True \"\n",
    "            command += f\"chosen_algo={interp_algo} \"\n",
    "            command += f\"encoder_path={encoder_path} \"\n",
    "            command += f\"filename={exp.config['env_cfg']['task_name']}_{exp.config['exp_ident']} \"\n",
    "\n",
    "            print(f\"Generating videos for exp {exp.config['exp_ident']} on {exp.config['env_cfg']['task_name']}...\")\n",
    "            process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n",
    "            output, error = process.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}