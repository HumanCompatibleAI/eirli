{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "\n",
    "runs_directory = \"/scratch/sam/il-representations-gcp-volume/cluster-data/cluster-2020-09-27T03:04Z\"\n",
    "exp_name = \"plot\"\n",
    "exp_dir = os.path.join(runs_directory, 'chain_runs', exp_name)\n",
    "assert os.path.isdir(exp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Trial information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some config keys are not useful for understanding the experiments -> Filter these when printing\n",
    "repl_extra_config_key = [\"demo_timesteps\", \"device\", \"n_envs\", \"ppo_finetune\", \"ppo_timesteps\", \n",
    "                         \"seed\", \"torch_num_threads\", \"unit_test_max_train_steps\", \"use_random_rollouts\",\n",
    "                         \"benchmark\", \"algo_params/device\", \"algo_params/loss_calculator_kwargs\", \n",
    "                         \"algo_params/optimizer\", \"algo_params/preprocess_extra_context\", \n",
    "                         \"algo_params/preprocess_target\", \"algo_params/save_interval\", \"algo_params/scheduler\",\n",
    "                         \"algo_params/scheduler_kwargs\", \"algo_params/shuffle_batches\", \n",
    "                         \"algo_params/target_pair_constructor_kwargs\", \"algo_params/unit_test_max_train_steps\"]\n",
    "il_train_extra_config_key = [\"benchmark\", \"device_name\", \"final_pol_name\", \"gail\", \"seed\", \"torch_num_threads\", \n",
    "                             \"encoder_kwargs\"]\n",
    "il_test_extra_config_key = [\"benchmark\", \"device_name\", \"run_id\", \"seed\", \"torch_num_threads\", \"write_video\",\n",
    "                            \"video_file_name\", \"policy_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define Trial object\n",
    "class Trial:\n",
    "    \"\"\"\n",
    "        The Trial object contains its run info.\n",
    "        Each trial should have its repl_dir, il_train_dir, and il_test_dir. Depending on \n",
    "        the status of the trial, it might not have its il_train and il_test (yet).\n",
    "        In this case, self.il_test_dir and self.il_train_dir will be None.\n",
    "    \"\"\"\n",
    "    def __init__(self, last_run_dir):\n",
    "        self.il_test_dir = None\n",
    "        self.il_train_dir = None\n",
    "        self.repl_dir = None\n",
    "        self.set_dirs(last_run_dir)\n",
    "        self.return_mean = -999\n",
    "        self.get_return()\n",
    "        \n",
    "    def set_dirs(self, last_run_dir):\n",
    "        if 'il_test' in last_run_dir:\n",
    "            self.il_test_dir = last_run_dir\n",
    "            test_config = self.get_config('il_test')\n",
    "            last_run_dir = '/'.join(self.il_test_dir.split('/')[:-2] + \n",
    "                                    ['il_train', test_config['policy_path'].split('/')[-2]])\n",
    "        if 'il_train' in last_run_dir:\n",
    "            self.il_train_dir = last_run_dir\n",
    "            train_config = self.get_config('il_train')\n",
    "            last_run_dir = '/'.join(self.il_train_dir.split('/')[:-2] + \n",
    "                                    ['repl', train_config['encoder_path'].split('/')[-4]])\n",
    "        if 'repl' in last_run_dir:\n",
    "            self.repl_dir = last_run_dir\n",
    "    \n",
    "    def get_config(self, mode, key_to_remove=[]):\n",
    "        # Get the trial config specified by mode, with optional key_to_remove to hide unuseful info.\n",
    "        assert mode in ['repl', 'il_train', 'il_test']\n",
    "        with open(f'{self.__dict__[mode + \"_dir\"]}/config.json') as json_file:\n",
    "                config = json.load(json_file)\n",
    "                \n",
    "        def remove_dict_entry(dic, key_to_remove):\n",
    "            dic_copy = copy.deepcopy(dic)\n",
    "            for key in key_to_remove:\n",
    "                if '/' in key:\n",
    "                    del dic_copy[key.split('/')[0]][key.split('/')[1]]\n",
    "                else:\n",
    "                    del dic_copy[key]\n",
    "            return dic_copy\n",
    "\n",
    "        config = remove_dict_entry(config, key_to_remove)\n",
    "        return config\n",
    "    \n",
    "    def get_return(self):\n",
    "        if self.il_test_dir:\n",
    "            result_file = f'{self.il_test_dir}/eval.json'\n",
    "            if os.path.isfile(result_file):\n",
    "                with open(result_file) as json_file:\n",
    "                        result = json.load(json_file)\n",
    "                self.return_mean = result['return_mean']\n",
    "            else:\n",
    "                print(f'WARNING - {result_file} does not exist.')\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n'.join([f'{key}: {value}' for key, value in self.__dict__.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collect trial objects from specified dirs\n",
    "def get_trial_objects(root_dir, exp_type, trial_dirs):\n",
    "    \"\"\"\n",
    "    Return a list of trial objects by inspecting subdirs of run_dir, \n",
    "    which typically is either repl, il_train, or il_test.\n",
    "    \"\"\"\n",
    "    trial_list = []\n",
    "    for trial_dir in trial_dirs:\n",
    "        if trial_dir in ['_sources', 'progress']:\n",
    "            continue\n",
    "        dir_abspath = os.path.join(root_dir, exp_type, trial_dir)\n",
    "        trial_list.append(Trial(dir_abspath))\n",
    "    return trial_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tool to pretty print nested dictionaries\n",
    "def pretty_print(d, indent=0):\n",
    "   for key, value in d.items():\n",
    "      print('\\t' * indent + str(key))\n",
    "      if isinstance(value, dict):\n",
    "        for key_, value_ in value.items():\n",
    "             print('\\t' * (indent+1) + str(key_) + ': ' + str(value_))\n",
    "      else:\n",
    "        print('\\t' * (indent+1) + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify trial type and get trial objects accordingly\n",
    "trials = {\n",
    "    'full_exp': [],\n",
    "    'il_train_only': [],\n",
    "    'repl_only': []\n",
    "}\n",
    "\n",
    "il_test_dirs = os.listdir(os.path.join(exp_dir, 'il_test'))\n",
    "trials['full_exp'] = get_trial_objects(exp_dir, 'il_test', il_test_dirs)\n",
    "\n",
    "il_train_dirs = os.listdir(os.path.join(exp_dir, 'il_train'))\n",
    "recorded_train_dirs = [t.il_train_dir.split('/')[-1] for t in trials['full_exp']]\n",
    "il_train_only_dirs = [d for d in il_train_dirs if d not in recorded_train_dirs]\n",
    "trials['il_train_only'] = get_trial_objects(exp_dir, 'il_train', il_train_only_dirs)\n",
    "\n",
    "repl_dirs = os.listdir(os.path.join(exp_dir, 'repl'))\n",
    "recorded_repl_dirs = [t.repl_dir.split('/')[-1] for t in trials['full_exp'] + trials['il_train_only']]\n",
    "repl_only_dirs = [d for d in repl_dirs if d not in recorded_repl_dirs]\n",
    "trials['repl_only'] = get_trial_objects(exp_dir, 'repl', repl_only_dirs)\n",
    "\n",
    "print('\\nExperiment info: \\n')\n",
    "print('\\n'.join([f'{trial_type}: {len(trial_list)} runs' for trial_type, trial_list in trials.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print information of trials with the highest test score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if a trial's final return_mean = -999, then it means the trial's `il_test/eval.json` file could not be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted_trials = sorted(trials['full_exp'], key=lambda x: x.return_mean, reverse=True)\n",
    "n = 5\n",
    "selected_trials = sorted_trials[:n] if n < len(sorted_trials) else sorted_trials\n",
    "for count, trial in enumerate(selected_trials):\n",
    "    print(f\"{'='*40} Trial {count}/{len(sorted_trials)} {'='*40}\")\n",
    "    print(f\"Final return_mean: {trial.return_mean}\")\n",
    "    print(f\"repl_config: \")\n",
    "    pretty_print(trial.get_config('repl', repl_extra_config_key))\n",
    "    print(f\"il_train_config: \")\n",
    "    pretty_print(trial.get_config('il_train', il_train_extra_config_key))\n",
    "    print(f\"il_test_config: \")\n",
    "    pretty_print(trial.get_config('il_test', il_test_extra_config_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare files for analyzing with Viskit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see a trial object's information, you can do print(trial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_files(mode):\n",
    "    \"\"\"\n",
    "    Create a folder named \"progress\" under either 'il_train' or 'repl' dir. The structure will be like:\n",
    "    il_train\n",
    "    │   ├── 1\n",
    "    │   │   ├── ...\n",
    "    │   │   ├── config.json\n",
    "    │   │   └── progress.csv\n",
    "    │   ├── progress\n",
    "    │   │   └── 1\n",
    "    │   │       ├── params.json   (same as config.json)\n",
    "    │   │       └── progress.csv\n",
    "    │   └── _sources\n",
    "\n",
    "    then the Viskit analysis can be called by: python viskit/frontend.py path/to/il_train/progress/*\n",
    "    \"\"\"\n",
    "    assert mode in ['il_train', 'repl']\n",
    "    \n",
    "    # Create progress and its subdirs \n",
    "    dir_name = os.path.join(exp_dir, mode)\n",
    "    trial_dirs = os.listdir(dir_name)\n",
    "    for trial_dir in trial_dirs:\n",
    "        os.system(f'mkdir -p {dir_name}/progress/{trial_dir}')\n",
    "        os.system(f'cp {dir_name}/{trial_dir}/config.json {dir_name}/progress/{trial_dir}/params.json')\n",
    "        os.system(f'cp {dir_name}/{trial_dir}/progress.csv {dir_name}/progress/{trial_dir}/')\n",
    "    \n",
    "    # Copy and rename files\n",
    "    os.system(f'cp {dir_name}/*/config.json ')\n",
    "\n",
    "prepare_files('repl')\n",
    "prepare_files('il_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter for runs whose performance exceeds vanilla IL baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Glob the test result files\n",
    "glob_query = \"chain_runs/[0-9]*/il_test/[0-9]*/run.json\"\n",
    "glob_arg = os.path.join(runs_directory, glob_query)\n",
    "evaluation_files = glob.glob(glob_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Helper function to parse mean return from .json file\n",
    "def get_evaluation_result(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        results = json.load(f)\n",
    "        if results['result'] is not None:\n",
    "            return results['result']['return_mean']['value']\n",
    "        else:\n",
    "            return float(\"-inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parse the test result files for mean return\n",
    "evaluation_results = []\n",
    "for filename in evaluation_files:\n",
    "    evaluation_result = get_evaluation_result(filename)\n",
    "    evaluation_results.append(evaluation_result)\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter for test result files that exceed IL baseline return\n",
    "baseline_filename = evaluation_files[0]  #TODO put the filename of the baseline IL run here\n",
    "baseline_return = get_evaluation_result(baseline_filename)\n",
    "runs_with_improvement = []\n",
    "for run_file, run_return in zip(evaluation_files, evaluation_results):\n",
    "    if run_return > baseline_return:\n",
    "        run_name = os.path.dirname(run_file)\n",
    "        runs_with_improvement.append(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(evaluation_files)\n",
    "print('\\n')\n",
    "print(evaluation_results)\n",
    "print('\\n')\n",
    "print(runs_with_improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make sure your cwd is the il-representations directory\n",
    "if os.getcwd().split('/')[-1] == 'analysis':\n",
    "    os.chdir(\"..\")\n",
    "print('Check cwd', os.getcwd())\n",
    "\n",
    "from il_representations.scripts.interpret import (prepare_network, process_data, save_img, saliency_, deep_lift_, \n",
    "integrated_gradient_, layer_conductance_, layer_gradcam_, layer_act_, choose_layer, interp_ex)\n",
    "from il_representations.envs.config import benchmark_ingredient\n",
    "import il_representations.envs.auto as auto_env\n",
    "\n",
    "import sacred\n",
    "import numpy\n",
    "from sacred.observers import FileStorageObserver\n",
    "from sacred import Experiment\n",
    "from stable_baselines3.common.utils import get_device\n",
    "from captum.attr import LayerActivation, LayerGradientXActivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "render_interp_ex = Experiment('render_interp', ingredients=[benchmark_ingredient, interp_ex], interactive=True)\n",
    "interp_ex.observers.append(FileStorageObserver('runs/interpret_runs'))\n",
    "\n",
    "##### These should be the only things you need to modify in this code block #####\n",
    "policy_paths = [os.path.join(t.il_train_dir, 'policy_final.pt') for t in trials['full_exp']]\n",
    "configs = [t.get_config('repl', key_to_remove=repl_extra_config_key) for t in trials['full_exp']]\n",
    "\n",
    "@interp_ex.config\n",
    "def config():\n",
    "    encoder_paths = policy_paths\n",
    "    for path in encoder_paths:\n",
    "        assert os.path.isfile(path), f'Please double check if {path} exists.'\n",
    "    \n",
    "    \n",
    "    # Data settings\n",
    "    # The benchmark is set by detecting il_representations/envs/config's bench_defaults.benchmark_name\n",
    "    imgs = [8]  # index of the image to be inspected (int)\n",
    "    assert all(isinstance(im, int) for im in imgs), 'imgs list should contain integers only'\n",
    "\n",
    "    verbose = False\n",
    "    #################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "render_interp_ex = Experiment('render_interp', ingredients=[benchmark_ingredient, interp_ex], interactive=True)\n",
    "\n",
    "@render_interp_ex.main\n",
    "def run():\n",
    "    venv = auto_env.load_vec_env()\n",
    "    networks = prepare_network(venv)\n",
    "    images, labels = process_data()\n",
    "    return networks, images, labels\n",
    "\n",
    "r = render_interp_ex.run()\n",
    "networks = r.result[0]\n",
    "images = r.result[1]\n",
    "labels = r.result[2]\n",
    "verbose = True\n",
    "log_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def saliency():\n",
    "    for img, label in zip(images, labels):\n",
    "        for config, network in zip(configs, networks):\n",
    "            print('='*50)\n",
    "            pretty_print(config)\n",
    "            original_img = img[0].permute(1, 2, 0).detach().numpy()\n",
    "            saliency_(network, img, label, original_img, log_dir, False)\n",
    "\n",
    "saliency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def deep_lift():\n",
    "    for img, label in zip(images, labels):\n",
    "        for config, network in zip(configs, networks):\n",
    "            print('='*50)\n",
    "            original_img = img[0].permute(1, 2, 0).detach().numpy()\n",
    "            deep_lift_(network, img, label, original_img, log_dir, False)\n",
    "\n",
    "deep_lift()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
