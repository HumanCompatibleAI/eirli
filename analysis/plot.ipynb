{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "runs_directory = \"/scratch/sam/il-representations-gcp-volume/cluster-data/cluster-2020-09-27T03:04Z\"\n",
    "exp_name = \"10\"\n",
    "exp_dir = os.path.join(runs_directory, 'chain_runs', exp_name)\n",
    "assert os.path.isdir(exp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Trial information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Trial object\n",
    "class Trial:\n",
    "    \"\"\"\n",
    "        The Trial object contains its directory info.\n",
    "        Each trial should have its repl_dir, il_train_dir, and il_test_dir. Depending on \n",
    "        the status of the trial, it might not have its il_train and il_test started (yet).\n",
    "        In this case, self.il_test_dir and self.il_train_dir will be None.\n",
    "    \"\"\"\n",
    "    def __init__(self, last_run_dir):\n",
    "        self.il_test_dir = None\n",
    "        self.il_train_dir = None\n",
    "        self.repl_dir = None\n",
    "        self.set_dirs(last_run_dir)\n",
    "        \n",
    "    def set_dirs(self, last_run_dir):\n",
    "        if 'il_test' in last_run_dir:\n",
    "            self.il_test_dir = last_run_dir\n",
    "            with open(f'{self.il_test_dir}/config.json') as json_file:\n",
    "                test_config = json.load(json_file)\n",
    "            last_run_dir = '/'.join(self.il_test_dir.split('/')[:-2] + \n",
    "                                    ['il_train', test_config['policy_path'].split('/')[-2]])\n",
    "        if 'il_train' in last_run_dir:\n",
    "            self.il_train_dir = last_run_dir\n",
    "            with open(f'{self.il_train_dir}/config.json') as json_file:\n",
    "                train_config = json.load(json_file)\n",
    "            last_run_dir = '/'.join(self.il_train_dir.split('/')[:-2] + \n",
    "                                    ['repl', train_config['encoder_path'].split('/')[-4]])\n",
    "        if 'repl' in last_run_dir:\n",
    "            self.repl_dir = last_run_dir\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n'.join([f'{key}: {value}' for key, value in self.__dict__.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect trial objects from specified dirs\n",
    "def get_trial_objects(root_dir, exp_type, trial_dirs):\n",
    "    \"\"\"\n",
    "    Return a list of trial objects by inspecting subdirs of run_dir, \n",
    "    which typically is either repl, il_train, or il_test.\n",
    "    \"\"\"\n",
    "    trial_list = []\n",
    "    for trial_dir in trial_dirs:\n",
    "        if trial_dir == '_sources':\n",
    "            continue\n",
    "        dir_abspath = os.path.join(root_dir, exp_type, trial_dir)\n",
    "        trial_list.append(Trial(dir_abspath))\n",
    "    return trial_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify trial type and get trial objects accordingly\n",
    "trials = {\n",
    "    'full_exp': [],\n",
    "    'il_train_only': [],\n",
    "    'repl_only': []\n",
    "}\n",
    "\n",
    "il_test_dirs = os.listdir(os.path.join(exp_dir, 'il_test'))\n",
    "trials['full_exp'] = get_trial_objects(exp_dir, 'il_test', il_test_dirs)\n",
    "\n",
    "il_train_dirs = os.listdir(os.path.join(exp_dir, 'il_train'))\n",
    "recorded_train_dirs = [t.il_train_dir.split('/')[-1] for t in trials['full_exp']]\n",
    "il_train_only_dirs = [d for d in il_train_dirs if d not in recorded_train_dirs]\n",
    "trials['il_train_only'] = get_trial_objects(exp_dir, 'il_train', il_train_only_dirs)\n",
    "\n",
    "repl_dirs = os.listdir(os.path.join(exp_dir, 'repl'))\n",
    "recorded_repl_dirs = [t.repl_dir.split('/')[-1] for t in trials['full_exp'] + trials['il_train_only']]\n",
    "repl_only_dirs = [d for d in repl_dirs if d not in recorded_repl_dirs]\n",
    "trials['repl_only'] = get_trial_objects(exp_dir, 'repl', repl_only_dirs)\n",
    "\n",
    "print('Experiment info: \\n')\n",
    "print('\\n'.join([f'{trial_type}: {len(trial_list)} runs' for trial_type, trial_list in trials.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter for runs whose performance exceeds vanilla IL baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glob the test result files\n",
    "glob_query = \"chain_runs/[0-9]*/il_test/[0-9]*/run.json\"\n",
    "glob_arg = os.path.join(runs_directory, glob_query)\n",
    "evaluation_files = glob.glob(glob_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to parse mean return from .json file\n",
    "def get_evaluation_result(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        results = json.load(f)\n",
    "        if results['result'] is not None:\n",
    "            return results['result']['return_mean']['value']\n",
    "        else:\n",
    "            return float(\"-inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the test result files for mean return\n",
    "evaluation_results = []\n",
    "for filename in evaluation_files:\n",
    "    evaluation_result = get_evaluation_result(filename)\n",
    "    evaluation_results.append(evaluation_result)\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for test result files that exceed IL baseline return\n",
    "baseline_filename = evaluation_files[0]  #TODO put the filename of the baseline IL run here\n",
    "baseline_return = get_evaluation_result(baseline_filename)\n",
    "runs_with_improvement = []\n",
    "for run_file, run_return in zip(evaluation_files, evaluation_results):\n",
    "    if run_return > baseline_return:\n",
    "        run_name = os.path.dirname(run_file)\n",
    "        runs_with_improvement.append(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation_files)\n",
    "print('\\n')\n",
    "print(evaluation_results)\n",
    "print('\\n')\n",
    "print(runs_with_improvement)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
