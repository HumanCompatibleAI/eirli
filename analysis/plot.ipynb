{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import copy\n",
    "import html\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import json\n",
    "import glob\n",
    "from functools import partial\n",
    "from numpy import trapz\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "import seaborn as sns \n",
    "from math import sqrt \n",
    "import pandas as pd \n",
    "import math\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# FIXME(sam): make cluster subpath selectable with a dropdown. Also, automatically infer\n",
    "# gfs_mount from the hostname (have sensible default for svm/perceptron)\n",
    "\n",
    "# this identifies data for a particular cluster on the GFS volume\n",
    "# cluster_subpath = \"with-resnet-128-2020-01-27/\"\n",
    "# cluster_subpath = \"cluster-data/cluster-2021-01-29-set3-try4/\"\n",
    "# on svm I think gfs_mount is /scratch/sam/repl-vol/ or something like that\n",
    "# gfs_mount = \"/scratch/cynthiachen/\"\n",
    "# gfs_mount = \"/scratch/sam/il-representations-gcp-volume/cluster-data/cluster-2021-09-23-sam-new-vis-gail-v2/\"  # Google Filestore mount point (local)\n",
    "# gfs_mount = \"/scratch/sam/ilr-gail-procgen-runs-2021-09-29/\"\n",
    "gfs_mount = \"/scratch/sam/il-representations-gcp-volume/cluster-data/cluster-2021-09-29-sam-new-vis-repl-ablations/\"  # Google Filestore mount point (local)\n",
    "# gfs_mount = \"/home/sam/repos/il-representations/runs/\"\n",
    "# gfs_mount = \"/scratch/sam/il-rep-quals-runs-2021-04-17/\"\n",
    "cluster_subpath = \"./\"\n",
    "runs_directory = os.path.join(gfs_mount, cluster_subpath)\n",
    "path_translations = {\n",
    "    # when loading things like 'encoder_path' and 'policy_path' from configs,\n",
    "    # replace the thing on the left with the thing on the right\n",
    "    \"/data/il-representations/\": gfs_mount,\n",
    "    \"/root/il-rep/runs/\": os.path.join(gfs_mount, cluster_subpath),\n",
    "    # \"/home/sam/repos/il-representations/cloud/runs/\": os.path.join(gfs_mount, cluster_subpath),\n",
    "    \"/home/sam/repos/il-representations/runs/\": gfs_mount,\n",
    "}\n",
    "\n",
    "exp_dir = os.path.join(runs_directory, 'chain_runs')\n",
    "assert os.path.exists(exp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a table of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_parent_relpath(sample_parent_file, local_root_dir):\n",
    "    \"\"\"Get root-relative path to a 'parent' directory, such as the directory\n",
    "    containing a saved encoder or policy. This is somewhat tricky because\n",
    "    we need to replace paths that might be different on svm/perceptron or on\n",
    "    a laptop compared to what they were on GCP. e.g. inside the Ray docker\n",
    "    container, '/root/il-rep/runs' maps to 'cluster-data/' in the GFS volume.\n",
    "    The `path_translations` variable handles all the necessary changes.\"\"\"\n",
    "    for prefix, replacement in path_translations.items():\n",
    "        if sample_parent_file.startswith(prefix):\n",
    "            sample_parent_file = replacement + sample_parent_file[len(prefix):]\n",
    "\n",
    "    full_path = os.path.abspath(sample_parent_file)\n",
    "    full_dir = os.path.dirname(full_path)\n",
    "    rel_dir = os.path.relpath(full_dir, local_root_dir)\n",
    "\n",
    "    return rel_dir\n",
    "\n",
    "class SubexperimentRun:\n",
    "    \"\"\"A SubexperimentRun associates all the information associated\n",
    "    with a run of a particular Sacred sub-experiment. That means a\n",
    "    run (single execution) of the 'repl', 'il_train', or 'il_test'\n",
    "    experiments.\"\"\"\n",
    "    def __init__(self, subexp_dir, experiment_dir_root):\n",
    "        # Subexperiment dir is used as a unique identifier.\n",
    "        # We strip out the leading 'experiment_dir_root' to shorten identifiers.\n",
    "        subexp_dir = os.path.abspath(subexp_dir)\n",
    "        experiment_dir_root = os.path.abspath(experiment_dir_root)\n",
    "        self.ident = os.path.relpath(subexp_dir, experiment_dir_root)\n",
    "        self.subexp_dir = subexp_dir\n",
    "        self.experiment_dir_root = experiment_dir_root\n",
    "\n",
    "        # usually paths are like, e.g., 'chain_runs/repl/42' or\n",
    "        # 'chain_runs/il_train/13'; if we take the second last component,\n",
    "        # we should get the mode\n",
    "        self.mode = os.path.split(os.path.split(subexp_dir)[0])[1]\n",
    "        assert self.mode in {'repl', 'il_train', 'il_test'}, (mode, subexp_dir)\n",
    "\n",
    "        # Load experiment config\n",
    "        config_path = os.path.join(subexp_dir, 'config.json')\n",
    "        with open(config_path, 'r') as fp:\n",
    "            self.config = json.load(fp)\n",
    "\n",
    "        # Store a path to relevant progress.csv file (only for il_train/repl)\n",
    "        progress_path = os.path.join(subexp_dir, \"progress.csv\")\n",
    "        if os.path.exists(progress_path):\n",
    "            self.progress_path = progress_path\n",
    "        else:\n",
    "            self.progress_path = None\n",
    "\n",
    "        # Store a path to relevant eval.json file\n",
    "        eval_json_path = os.path.join(subexp_dir, \"eval.json\")\n",
    "        if os.path.exists(eval_json_path):\n",
    "            self.eval_json_path = eval_json_path\n",
    "        else:\n",
    "            self.eval_json_path = None\n",
    "\n",
    "        # Infer the .ident attribute for the parent experiment\n",
    "        # (if it exists)\n",
    "        if self.mode == 'il_train' and self.config.get('encoder_path') is not None:\n",
    "            encoder_relpath = get_parent_relpath(\n",
    "                self.config['encoder_path'], experiment_dir_root)\n",
    "            # The relpath is going to be something like\n",
    "            # \"chain_runs/10/repl/5/checkpoints/representation_encoder\".\n",
    "            # We heuristically remove the last two parts.\n",
    "            # (this definitely breaks on Windows…)\n",
    "            encoder_relpath = '/'.join(encoder_relpath.split('/')[:-2])\n",
    "            self.parent_ident = encoder_relpath\n",
    "        elif self.mode == 'il_test':\n",
    "            policy_relpath = get_parent_relpath(\n",
    "                self.config['policy_path'], experiment_dir_root)\n",
    "            if policy_relpath.endswith('/snapshots'):\n",
    "                policy_relpath = '/'.join(policy_relpath.split('/')[:-1])\n",
    "            self.parent_ident = policy_relpath\n",
    "        else:\n",
    "            # \"repl\" runs and \"il_train\" runs without an encoder_path\n",
    "            # have no parents\n",
    "            assert self.mode == 'repl' \\\n",
    "              or (self.mode == 'il_train' and self.config.get('encoder_path') is None), \\\n",
    "               (self.mode, self.config.get('encoder_path'))\n",
    "            self.parent_ident = None\n",
    "            \n",
    "        # HACK: adding a use_repl key so that we can see whether il_train runs used repL\n",
    "        if self.mode == 'il_train':\n",
    "            self.config['use_repl'] = self.parent_ident is not None\n",
    "\n",
    "    def get_merged_config(self, index):\n",
    "        \"\"\"Get a 'merged' config dictionary for this subexperiment and\n",
    "        all of its parents. The dict will have a format like this:\n",
    "        \n",
    "        {\"benchmark\": {…}, \"il_train\": {…}, \"il_test\": {…}, \"repl\": {…}}\n",
    "        \n",
    "        Note that some keys might not be present (e.g. if this is a `repl` run,\n",
    "        it will not have the `il_train` key; if this is an `il_train` run with\n",
    "        no parent, then the `repl` key will be absent).\"\"\"\n",
    "        config = {self.mode: dict(self.config)}\n",
    "        extract_keys = ('env_cfg', 'venv_opts', 'env_data')\n",
    "        for extract_key in extract_keys:\n",
    "            if extract_key in config[self.mode]:\n",
    "                # move 'benchmark' key to the top because that ingredient name is\n",
    "                # shared between il_train, and il_test experiments\n",
    "                config[extract_key] = config[self.mode][extract_key]\n",
    "                del config[self.mode][extract_key]\n",
    "        parent = self.get_parent(index)\n",
    "        if parent is not None:\n",
    "            # TODO: merge this properly, erroring on incompatible duplicate\n",
    "            # keys. I think Cody has code for this.\n",
    "            config.update(parent.get_merged_config(index))\n",
    "        return config\n",
    "    \n",
    "    def get_parent(self, index):\n",
    "        if self.parent_ident is None:\n",
    "            return None\n",
    "        return index.get_subexp(self.parent_ident)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.ident)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, SubexperimentRun):\n",
    "            return NotImplemented\n",
    "        return self.ident == other.ident\n",
    "\n",
    "class SubexperimentIndex:\n",
    "    \"\"\"An index of subexperiments. For now this just supports\n",
    "    looking up experiments by identifier. Later it might support\n",
    "    lookup by attributes.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.subexp_by_ident = {}\n",
    "        \n",
    "    def add_subexp(self, subexp):\n",
    "        if subexp.ident in self.subexp_by_ident:\n",
    "            raise ValueError(\"duplicate subexperiment:\", subexp)\n",
    "        self.subexp_by_ident[subexp.ident] = subexp\n",
    "        \n",
    "    def get_subexp(self, ident):\n",
    "        return self.subexp_by_ident[ident]\n",
    "    \n",
    "    def search(self, **attrs):\n",
    "        \"\"\"Find a subexperiment with attributes matching the values\n",
    "        given in 'attrs'.\"\"\"\n",
    "        results = []\n",
    "        for subexp in self.subexp_by_ident.values():\n",
    "            for k, v in attrs.items():\n",
    "                if getattr(subexp, k) != v:\n",
    "                    break\n",
    "            else:\n",
    "                results.append(subexp)\n",
    "        return results\n",
    "\n",
    "def get_experiment_directories(root_dir, skip_skopt=True):\n",
    "    \"\"\"Look for directories that end in a sequence of numbers, and contain a\n",
    "    grid_search subdirectory.\"\"\"\n",
    "    expt_pat = re.compile(r'^.*/(il_test|il_train|repl)/\\d+$')\n",
    "    ignore_pat = re.compile(r'^.*/(grid_search|_sources)$')  # ignore the grid_search subdir\n",
    "    expt_dirs = set()\n",
    "    for root, dirs, files in os.walk(root_dir, followlinks=True, topdown=True):\n",
    "        if ignore_pat.match(root):\n",
    "            del dirs[:]\n",
    "            continue\n",
    "            \n",
    "        # check whether tihs is a skopt dir\n",
    "        if skip_skopt and 'grid_search' in dirs:\n",
    "            gs_files = os.listdir(os.path.join(root, 'grid_search'))\n",
    "            if any(s.startswith('search-alg-') for s in gs_files):\n",
    "                # this is a skopt dir, skip it\n",
    "                print(\"skipping skopt directory in\", root)\n",
    "                del dirs[:]\n",
    "                continue\n",
    "\n",
    "        found_match = False\n",
    "        for d in dirs:\n",
    "            d_path = os.path.abspath(os.path.join(root, d))\n",
    "            m = expt_pat.match(d_path)\n",
    "            if m is None:\n",
    "                continue  # no match\n",
    "            expt_dirs.add(d_path)\n",
    "            found_match = True\n",
    "\n",
    "        if found_match:\n",
    "            del dirs[:]  # don't recurse\n",
    "    return sorted(expt_dirs)\n",
    "\n",
    "# Find all experiment directories (i.e. directories containing a grid_search\n",
    "# subdir)\n",
    "def load_all_subexperiments(root_dir, skip_skopt=True):\n",
    "    \"\"\"Find all experiment run subdirectories, and create SubexperimentIndex objects for them.\"\"\"\n",
    "    print(\"Searching for experiment directories (might take a minute or two)\")\n",
    "    all_expt_directories = get_experiment_directories(root_dir, skip_skopt=skip_skopt)\n",
    "    print(\"Loading experiments (might take another minute or two)\")\n",
    "    index = SubexperimentIndex()\n",
    "    for expt_dir in all_expt_directories:\n",
    "        subexp = SubexperimentRun(expt_dir, root_dir)\n",
    "        index.add_subexp(subexp)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subexp_index = load_all_subexperiments(runs_directory, skip_skopt=True)\n",
    "print('Discovered', len(subexp_index.subexp_by_ident), 'subexperiments')\n",
    "\n",
    "test_expts = subexp_index.search(mode='il_test')\n",
    "test_expts[1].get_merged_config(subexp_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print a table of il_test results\n",
    "\n",
    "Shows a separate set of il_test results for each benchmark setting, and also puts that data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def flatten_dict(d):\n",
    "    \"\"\"Flatten a nested dict into a single-level dict with\n",
    "    'keys/separated/like/this'.\"\"\"\n",
    "    out_dict = {}\n",
    "    if isinstance(d, dict):\n",
    "        key_iter = d.items()\n",
    "    else:\n",
    "        assert isinstance(d, list), type(d)\n",
    "        # we flatten lists into dicts of the form {0: <first elem>, 1: <second elem>, …}\n",
    "        key_iter = ((str(idx), v) for idx, v in enumerate(d))\n",
    "    for key, value in key_iter:\n",
    "        if isinstance(value, (dict, list)):\n",
    "            value = flatten_dict(value)\n",
    "            for subkey, subvalue in value.items():\n",
    "                out_dict[f'{key}/{subkey}'] = subvalue\n",
    "        else:\n",
    "            out_dict[key] = value\n",
    "    return out_dict\n",
    "\n",
    "def combine_dicts_multiset(dicts):\n",
    "    \"\"\"Combine a series of dicts into a key-multiset mapping, where the\n",
    "    multiset measures how many times each observed value occurs for each\n",
    "    key.\"\"\"\n",
    "    count_dict = {}\n",
    "    for d in dicts:\n",
    "        for k, v in d.items():\n",
    "            if k not in count_dict:\n",
    "                count_dict[k] = collections.Counter()\n",
    "            count_dict[k][v] += 1\n",
    "    return count_dict\n",
    "\n",
    "def remove_inapplicable_keys(flat_dict):\n",
    "    \"\"\"Remove keys that do not make a difference from a flattened config dicts.\n",
    "    Totally heuristic, so might have to add more options to this later on.\"\"\"\n",
    "    remove_keys = set()\n",
    "    \n",
    "    # remove inapplicable benchmark keys\n",
    "    for benchmark_name in ['magical', 'dm_control']:\n",
    "        if flat_dict.get('env_cfg/benchmark_name') != benchmark_name:\n",
    "            for key in flat_dict:\n",
    "                # this will remove, e.g., dm_control keys from magical experiments\n",
    "                if key.startswith('env_cfg/' + benchmark_name) or key.startswith('env_data/' + benchmark_name):\n",
    "                    remove_keys.add(key)\n",
    "                    \n",
    "    # remove repl keys from things that don't use repL\n",
    "    if flat_dict.get('il_train/use_repl') is False:\n",
    "        for key in flat_dict:\n",
    "            if key.startswith('repl/'):\n",
    "                remove_keys.add(key)\n",
    "                    \n",
    "    return {k: v for k, v in flat_dict.items() if k not in remove_keys}\n",
    "\n",
    "def simplify_config_dicts(hierarchical_dicts,\n",
    "                          base_thresh=0.75,\n",
    "                          remove_seeds=True,\n",
    "                          prohibited_base_keys=('env_cfg/task_name', 'env_cfg/benchmark_name', 'il_test/exp_ident'),\n",
    "                          force_remove_keys=('il_test/policy_path', 'il_train/encoder_path')):\n",
    "    \"\"\"Simplify flattened config dicts so that:\n",
    "    \n",
    "    0. They are totally flat.\n",
    "    1. They only contain keys for which values actually differ between\n",
    "       different dicts, and\n",
    "    2. If the value of some key is the same for at least a fraction\n",
    "       `base_thresh` of dicts, then that key is moved into a _base config_.\n",
    "       Returned dicts will only contain that key if they have a different\n",
    "       value from the base config one.\n",
    "    3. Optionally, remove all seed values from dicts.\n",
    "\n",
    "    This makes it more clear which values are actually changing.\"\"\"\n",
    "    # first flatten all dicts\n",
    "    dicts = [dict(flatten_dict(d)) for d in hierarchical_dicts]\n",
    "    \n",
    "    # remove seeds, if required\n",
    "    if remove_seeds:\n",
    "        for d in dicts:\n",
    "            for key in list(d.keys()):\n",
    "                if key.split('/')[-1] == 'seed':\n",
    "                    del d[key]\n",
    "                    \n",
    "    # make sure that every dict has every key\n",
    "    all_keys = set()\n",
    "    for d in dicts:\n",
    "        all_keys |= d.keys()\n",
    "    for d in dicts:\n",
    "        for new_key in all_keys - d.keys():\n",
    "            d[new_key] = None\n",
    "        \n",
    "    # remove inapplicable keys\n",
    "    dicts = [remove_inapplicable_keys(d) for d in dicts]\n",
    "\n",
    "    # now figure out which keys we wish to remove or move to the base config\n",
    "    base_config = {}\n",
    "    remove_keys = set()\n",
    "    base_thresh_abs = len(dicts) * base_thresh\n",
    "    count_dict = combine_dicts_multiset(dicts)\n",
    "    for key, counter in count_dict.items():\n",
    "        if len(counter) == 1 or key in force_remove_keys:\n",
    "            if key not in prohibited_base_keys:\n",
    "                # if all dicts have the same value for this key, we will\n",
    "                # remove it from output dicts\n",
    "                remove_keys.add(key)\n",
    "        elif key not in prohibited_base_keys:\n",
    "            # if most dicts have the same value for this key, then\n",
    "            # we add it to the base config\n",
    "            (max_count_item, max_count), = counter.most_common(1)\n",
    "            if max_count > base_thresh_abs:\n",
    "                base_config[key] = max_count_item\n",
    "\n",
    "    # remove keys that we are ignoring, or for which the corresponding value\n",
    "    # already exists in the base config\n",
    "    new_dicts = []\n",
    "    for old_dict in dicts:\n",
    "        new_dict = {}\n",
    "        for key, value in old_dict.items():\n",
    "            if key in remove_keys \\\n",
    "              or (key in base_config and base_config[key] == value):\n",
    "                continue  # skip this key\n",
    "            new_dict[key] = value\n",
    "        new_dicts.append(new_dict)\n",
    "\n",
    "    return base_config, new_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the below code can take 2-3m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape(obj):\n",
    "    return html.escape(str(obj))\n",
    "\n",
    "main_expts = subexp_index.search(mode='il_test')\n",
    "all_configs = [subexp.get_merged_config(subexp_index) for subexp in main_expts]\n",
    "base_config, flat_configs = simplify_config_dicts(all_configs)\n",
    "flat_config_tups = [tuple(sorted(d.items())) for d in flat_configs]\n",
    "subexp_by_benchmark = {}\n",
    "for flat_cfg, subexp in zip(flat_config_tups, main_expts):\n",
    "    bench_key = tuple((k, v) for k, v in flat_cfg if k.startswith('env_') or k.startswith('venv_'))\n",
    "    subexp_by_benchmark.setdefault(bench_key, []).append((flat_cfg, subexp))\n",
    "\n",
    "display(HTML('<p><strong>Base config</strong></p>'))\n",
    "display(HTML('<p>Unless specified otherwise, all config dicts include these keys:</p>'))\n",
    "print(base_config)\n",
    "\n",
    "raw_return_data = {}\n",
    "    \n",
    "for idx, (bench_key, cfgs_subexps) in enumerate(subexp_by_benchmark.items(), start=1):\n",
    "    # print out benchmark details\n",
    "    display(HTML(f'<p><strong>Results for benchmark config &#35;{idx}</strong></p>'))\n",
    "    display(HTML(f'<p>Config:</p>'))\n",
    "    rows = [f'<tr><th>{escape(key)}</th><td>{escape(value)}</td></tr>' for key, value in bench_key]\n",
    "    display(HTML(f'<table>{\"\".join(rows)}</table>'))\n",
    "    display(HTML(f'<p>Runs:</p>'))\n",
    "    \n",
    "    # cluster subexperiments by config\n",
    "    by_cfg = {}\n",
    "    for tup_cfg, subexp in cfgs_subexps:\n",
    "        tup_cfg = tuple(k for k in tup_cfg if k not in bench_key)\n",
    "        by_cfg.setdefault(tup_cfg, []).append(subexp)\n",
    "    \n",
    "    # load all eval.json files and figure out what columns we need\n",
    "    stats_dicts = {}\n",
    "    columns = set()\n",
    "    for _, subexp in cfgs_subexps:\n",
    "        if subexp.eval_json_path:\n",
    "            with open(subexp.eval_json_path, 'r') as fp:\n",
    "                eval_dict = json.load(fp)\n",
    "            # is this a magical run?\n",
    "            is_magical = 'full_data' in eval_dict.keys()\n",
    "            # is this a procgen run?\n",
    "            is_procgen = 'train_level' in eval_dict.keys()\n",
    "            assert not (is_magical and is_procgen)\n",
    "            if is_magical:\n",
    "                stats_dict = {\n",
    "                    '-'.join(env_dict['test_env'].split('-')[:2]): env_dict['mean_score']\n",
    "                    for env_dict in eval_dict['full_data']\n",
    "                }\n",
    "                stats_dict['Average on all envs'] = eval_dict['return_mean']\n",
    "            if is_procgen:\n",
    "                stats_dict = {\n",
    "                    level_type: eval_dict[level_type]['return_mean']\n",
    "                    for level_type in ['train_level', 'test_level']\n",
    "                }\n",
    "            else:\n",
    "                stats_dict = {'return_mean': eval_dict['return_mean']}\n",
    "            stats_dicts[subexp] = stats_dict\n",
    "            columns |= stats_dict.keys()\n",
    "        else:\n",
    "            stats_dicts[subexp] = {}\n",
    "    columns = sorted(columns)\n",
    "    \n",
    "    # now produce a table with one row per config\n",
    "    table_parts = ['<table>']                                         # begin table\n",
    "    table_parts.append('<tr>')                                        # begin header row\n",
    "    table_parts.append('<th style=\"border-collapse: collapse;\">Config</th>')\n",
    "    table_parts.extend(f'<th style=\"border-collapse: collapse;\">{html.escape(col_name)}</th>' for col_name in columns)\n",
    "    table_parts.append('</tr>')                                       # end header row\n",
    "\n",
    "    row_indexes = []\n",
    "    rows = []\n",
    "    \n",
    "    for cfg, subexps in sorted(by_cfg.items(), key=lambda cfg_se: dict(cfg_se[0])['il_test/exp_ident']):\n",
    "        table_parts.append('<tr>')                                    # begin row\n",
    "\n",
    "        # cell containing config\n",
    "        if True:  # remove to show full config\n",
    "            d = dict(cfg)\n",
    "            exp_ident = d['il_test/exp_ident']\n",
    "            # if 'mtest_st' in exp_ident:\n",
    "            #     exp_ident = exp_ident + '_' + d['repl/algo/py/type'].split('.')[-1].lower()\n",
    "            # bench_name = d['env_cfg/benchmark_name']\n",
    "            # task_name = d['env_cfg/task_name']\n",
    "            # desc_str = f'{exp_ident} ({bench_name}/{task_name})'\n",
    "            table_parts.append(f'<td style=\"border-collapse: collapse;\">{escape(exp_ident)}</td>')\n",
    "        else:\n",
    "            kv_cfg = ', '.join(f'{key}={value!r}' for key, value in cfg)\n",
    "            table_parts.append(f'<td style=\"max-width: 600px; border-collapse: collapse;\">{escape(kv_cfg)}</td>')\n",
    "        row_indexes.append(exp_ident)  \n",
    "        # cells containing data\n",
    "        row_values = []\n",
    "        for column in columns:\n",
    "            column_values = [stats_dicts[subexp][column] for subexp in subexps\n",
    "                             if column in stats_dicts[subexp]]\n",
    "            if not column_values:\n",
    "                table_parts.append('<td style=\"border-collapse: collapse;\">-</td>')\n",
    "                row_values.append({'mean': np.nan, 'std': np.nan, 'n': np.nan})\n",
    "            else:\n",
    "                mean = np.mean(column_values)\n",
    "                std = np.std(column_values)\n",
    "                n = len(column_values)\n",
    "                table_parts.append(f'<td style=\"border-collapse: collapse;\">{mean:.3g}±{std:.1g} ({n})</td>')\n",
    "                row_values.append({'mean': mean, 'std': std, 'n': n})\n",
    "        rows.append(row_values)\n",
    "        # cells containing values\n",
    "\n",
    "        table_parts.append('</tr>')                                   # end row\n",
    "    raw_return_data[dict(bench_key)['env_cfg/task_name']] = {'row_indexes': row_indexes, 'rows': rows, 'columns': columns}\n",
    "    table_parts.append('</table>')                                    # end table\n",
    "    display(HTML(''.join(table_parts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_monster_frame(main_expts, all_configs):\n",
    "    \"\"\"Make a huge dataframe containing results on all environments.\"\"\"\n",
    "    df_records = collections.defaultdict(list)\n",
    "    for expt, cfg in zip(main_expts, all_configs):\n",
    "        if not expt.eval_json_path:\n",
    "            continue\n",
    "        with open(expt.eval_json_path, 'r') as fp:\n",
    "            eval_dict = json.load(fp)\n",
    "        train_env = cfg['env_cfg']['task_name']\n",
    "        exp_ident = cfg['il_train']['exp_ident']\n",
    "        # is this a magical run?\n",
    "        is_magical = 'full_data' in eval_dict.keys()\n",
    "        is_procgen = not is_magical and ('train_level' in eval_dict.keys())\n",
    "        if is_magical:\n",
    "            test_envs_returns = []\n",
    "            for env_dict in eval_dict['full_data']:\n",
    "                short_test_env = '-' + env_dict['test_env'].split('-')[1]\n",
    "                test_envs_returns.append((short_test_env, env_dict['mean_score']))\n",
    "            test_envs_returns.append(('Average', eval_dict['return_mean']))\n",
    "        elif is_procgen:\n",
    "            test_envs_returns = []\n",
    "            for env_name in ['train_level', 'test_level']:\n",
    "                env_dict = eval_dict[env_name]\n",
    "                test_envs_returns.append((env_name, env_dict['return_mean']))\n",
    "        else:\n",
    "            test_envs_returns = [(train_env, eval_dict['return_mean'])]\n",
    "        for test_env, ret in test_envs_returns:\n",
    "            # here are all the columns we have in our data frame\n",
    "            df_records['eval_json_path'].append(expt.eval_json_path)\n",
    "            df_records['exp_ident'].append(exp_ident)\n",
    "            df_records['train_env'].append(train_env)\n",
    "            df_records['test_env'].append(test_env)\n",
    "            df_records['return'].append(ret)\n",
    "            df_records['is_magical'].append(is_magical)\n",
    "\n",
    "    # big dataframe containing all results\n",
    "    monster_frame = pd.DataFrame.from_dict(df_records)\n",
    "    return monster_frame\n",
    "\n",
    "monster_frame = make_monster_frame(main_expts, all_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot the results so we can insert them here: https://docs.google.com/spreadsheets/d/1bIRKNlLHOeZSsQTkP1oluL1KfJACb-WeTID7Ve-3R_U/edit#gid=0\n",
    "def return_mean(x): return np.mean(x)\n",
    "def return_std(x): return np.std(x)\n",
    "def count(x): return len(x)\n",
    "return_pivot = monster_frame.pivot_table(index=['exp_ident', 'train_env', 'test_env'], values='return', aggfunc=[return_mean, return_std, count])\n",
    "return_pivot.columns = [tup[0] for tup in return_pivot.columns.values]\n",
    "return_pivot.to_csv('./results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnrecognisedExpIdent(Exception):\n",
    "    \"\"\"Raised when factor_exp_ident fails because it cannot recognise exp_ident.\"\"\"\n",
    "\n",
    "def factor_exp_ident(row):\n",
    "    \"\"\"Parse an exp_ident into a dictionary of components.\"\"\"\n",
    "    exp_ident = row['exp_ident']\n",
    "    match = re.match(\n",
    "        r'^(?P<il_algo>bc|gail)(_icml)?_'\n",
    "        r'(?P<repl_algo>dynamics|vae|inv_dyn|identity_cpc|control|tcpc_8step)'\n",
    "        r'(_(?P<repl_data>.*))?$',\n",
    "        exp_ident)\n",
    "    if match is None:\n",
    "        raise UnrecognisedExpIdent(f\"Could not parse exp_ident='{exp_ident}'\")\n",
    "    human_names = {\n",
    "        'il_algo': {\n",
    "            'bc': 'BC',\n",
    "            'gail': 'GAIL',\n",
    "        },\n",
    "        'repl_algo': {\n",
    "            'dynamics': 'dynamics',\n",
    "            'vae': 'VAE',\n",
    "            'inv_dyn': 'inv. dyn.',\n",
    "            'identity_cpc': 'CPC',\n",
    "            'tcpc_8step': 'TCPC-8',\n",
    "            'control': 'control',\n",
    "        },\n",
    "        'repl_data': {\n",
    "            'rand_demos_magical_mt': 'MT rollouts + MT demos',\n",
    "            'rand_demos': 'rollouts + demos',\n",
    "            'rand_demos_test': 'test demos + rollouts',\n",
    "            None: 'n/a'\n",
    "        },\n",
    "    }\n",
    "    ret_dict = {}\n",
    "    for group_name, group_val in match.groupdict().items():\n",
    "        human_readable_value = human_names.get(group_name, {}).get(group_val, group_val)\n",
    "        ret_dict[group_name] = human_readable_value\n",
    "    return ret_dict\n",
    "\n",
    "def human_readable_name(row):\n",
    "    \"\"\"After succesfully applying factor_exp_ident, you can apply this function\n",
    "    to get a human-readable name for each exp_ident\"\"\"\n",
    "    if row['repl_data'] == 'n/a' and row['repl_algo'] == 'control':\n",
    "        return row['il_algo'] + ' control'\n",
    "    return f\"{row['il_algo']} + {row['repl_algo']} w/ {row['repl_data']}\"\n",
    "    \n",
    "allowed_test_variants = ['Average']\n",
    "allowed_mask = monster_frame['test_env'].isin(allowed_test_variants)\n",
    "allowed_monster = monster_frame[allowed_mask].sort_values(by='exp_ident')\n",
    "grouped_monster = allowed_monster.groupby(by=['train_env', 'test_env'])\n",
    "for (train_env, test_env), frame in grouped_monster:\n",
    "    try:\n",
    "        # this will only work for my quals runs from 2021-04-17\n",
    "        new_cols = frame.apply(factor_exp_ident, result_type='expand', axis=1)\n",
    "        frame = pd.concat([frame, new_cols], axis=1)\n",
    "        frame['human_name'] = frame.apply(human_readable_name, axis=1)\n",
    "    except UnrecognisedExpIdent:\n",
    "        raise\n",
    "    sns.set_context(\"talk\")\n",
    "    # using a facetgrid\n",
    "    g = sns.catplot(data=frame, row='il_algo', col='repl_algo', orient='v',\n",
    "                    x='repl_data', hue='repl_data', y='return', kind='box',\n",
    "                    sharex=False, sharey=True, margin_titles=True, height=5, dodge=False, aspect=0.7)\n",
    "    g.set_xticklabels(rotation=30)\n",
    "    plt.ylim([0, 1])\n",
    "    # g.legend.hide()\n",
    "    plt.suptitle(f\"{test_env} return for IL+repL approaches (trained on {train_env})\")\n",
    "    g.fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_return_dataframes = dict()                                                                                                                                                                                                                                                                                                                                                            \n",
    "\n",
    "for k in raw_return_data.keys():\n",
    "    print(f\"Processing task {k}\")\n",
    "    raw_return_dataframes[k] = dict()\n",
    "    for stat in ['mean', 'std', 'n']:\n",
    "        columns = raw_return_data[k]['columns']\n",
    "        index = raw_return_data[k]['row_indexes']\n",
    "        rows = []\n",
    "        for raw_row in raw_return_data[k]['rows']: \n",
    "            rows.append([el[stat] for el in raw_row])\n",
    "        try: \n",
    "            raw_return_dataframes[k][stat] = pd.DataFrame(rows, index=index, columns=columns)\n",
    "        except: \n",
    "            import pdb; pdb.set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print + DF-ize Raw AUC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How many splits?\n",
    "num_split = 6\n",
    "\n",
    "# Exclude first n values in the loss list?\n",
    "start_count = 3\n",
    "\n",
    "def calculate_auc(y, dx=1):\n",
    "    return trapz(y, dx=dx)\n",
    "    \n",
    "train_expts = subexp_index.search(mode='il_train')\n",
    "all_configs = [subexp.get_merged_config(subexp_index) for subexp in train_expts]\n",
    "base_config, flat_configs = simplify_config_dicts(all_configs)\n",
    "flat_config_tups = [tuple(sorted(d.items())) for d in flat_configs]\n",
    "subexp_by_benchmark = {}\n",
    "for flat_cfg, subexp in zip(flat_config_tups, train_expts):\n",
    "    bench_key = tuple((k, v) for k, v in flat_cfg if k.startswith('env_') or k.startswith('venv_'))\n",
    "    subexp_by_benchmark.setdefault(bench_key, []).append((flat_cfg, subexp))\n",
    "\n",
    "display(HTML('<p><strong>Base config</strong></p>'))\n",
    "display(HTML('<p>Unless specified otherwise, all config dicts include these keys:</p>'))\n",
    "print(base_config)\n",
    "    \n",
    "raw_auc_data = {}\n",
    "raw_loss_data = {}\n",
    "\n",
    "for idx, (bench_key, cfgs_subexps) in enumerate(subexp_by_benchmark.items(), start=1):\n",
    "    # print out benchmark details\n",
    "    task_name = dict(bench_key)['env_cfg/task_name']\n",
    "    \n",
    "    # cluster subexperiments by config\n",
    "    by_cfg = {}\n",
    "    for tup_cfg, subexp in cfgs_subexps:\n",
    "        tup_cfg = tuple(k for k in tup_cfg if k not in bench_key)\n",
    "        by_cfg.setdefault(tup_cfg, []).append(subexp)\n",
    "\n",
    "    # load all progress files and figure out what columns we need\n",
    "    stats_dicts = {}\n",
    "    columns = set()\n",
    "    raw_loss_data[task_name] = None\n",
    "    for cfg, subexp in cfgs_subexps:\n",
    "        d = dict(cfg)\n",
    "        exp_ident = d['il_train/exp_ident']\n",
    "        benchmark_name = d.get('env_cfg/benchmark_name', base_config.get('env_cfg/benchmark_name'))\n",
    "        full_length = 400 if benchmark_name == 'dm_control' else 40\n",
    "        \n",
    "        loss_df_cols = ['exp_ident', 'batches', 'seed', 'loss']\n",
    "        \n",
    "        if subexp.progress_path:\n",
    "            try:\n",
    "                df = pd.read_csv(subexp.progress_path)\n",
    "            except:\n",
    "                print(f'Read csv reported error for exp {exp_ident}, skipping...')\n",
    "                continue\n",
    "            if len(df['loss']) != full_length:\n",
    "                print(f'Experiment {exp_ident} only has len(loss) {len(df[\"loss\"])}, skipping... ')\n",
    "                continue\n",
    "                \n",
    "            step_length = len(df['loss']) // num_split\n",
    "            stats_dict = {}\n",
    "            if len(df['loss']) < 10: \n",
    "                stats_dicts[subexp] = {}\n",
    "                continue \n",
    "                \n",
    "            for step in range(step_length, len(df['loss']), step_length):\n",
    "                label = f\"step {step:02d}\"\n",
    "                stats_dict[label] = calculate_auc(df['loss'][start_count:step])\n",
    "                \n",
    "            stats_dict[f\"step {len(df['loss'])}\"] = calculate_auc(df['loss'][start_count:len(df['loss'])])\n",
    "            stats_dicts[subexp] = stats_dict\n",
    "            columns |= stats_dict.keys()\n",
    "            \n",
    "            # Add losses to raw_loss_data\n",
    "            sub_df_list = []\n",
    "            for count, loss in enumerate(df['loss']):\n",
    "                sub_df_list.append([exp_ident, count+1, subexp.config['seed'], loss])\n",
    "            sub_df = pd.DataFrame(sub_df_list, columns=loss_df_cols)\n",
    "\n",
    "            if raw_loss_data[task_name] is None:\n",
    "                raw_loss_data[task_name] = sub_df\n",
    "            else:\n",
    "                raw_loss_data[task_name] = raw_loss_data[task_name].append(sub_df, ignore_index=True)\n",
    "\n",
    "        else:\n",
    "            stats_dicts[subexp] = {}\n",
    "    columns = sorted(columns)\n",
    "    \n",
    "    raw_auc_data[task_name] = dict()\n",
    "    raw_auc_data[task_name]['columns'] = columns\n",
    "    \n",
    "    row_indexes = []\n",
    "    rows = []\n",
    "    for cfg, subexps in sorted(by_cfg.items(), key=lambda cfg_se: dict(cfg_se[0])['il_train/exp_ident']):\n",
    "        table_parts.append('<tr>')                                    # begin row\n",
    "\n",
    "        # cell containing config\n",
    "        if True:  # remove to show full config\n",
    "            d = dict(cfg)\n",
    "            exp_ident = d['il_train/exp_ident']\n",
    "            # bench_name = d['env_cfg/benchmark_name']\n",
    "            # task_name = d['env_cfg/task_name']\n",
    "            # desc_str = f'{exp_ident} ({bench_name}/{task_name})'\n",
    "            table_parts.append(f'<td style=\"border-collapse: collapse;\">{escape(exp_ident)}</td>')\n",
    "        else:\n",
    "            kv_cfg = ', '.join(f'{key}={value!r}' for key, value in cfg)\n",
    "            table_parts.append(f'<td style=\"max-width: 600px; border-collapse: collapse;\">{escape(kv_cfg)}</td>')\n",
    "            \n",
    "        # cells containing data\n",
    "        row_indexes.append(exp_ident)\n",
    "        row = []\n",
    "        for column in columns:\n",
    "            column_values = [stats_dicts[subexp][column] for subexp in subexps\n",
    "                             if subexp in stats_dicts.keys() and column in stats_dicts[subexp]]\n",
    "            if not column_values:\n",
    "                table_parts.append('<td style=\"border-collapse: collapse;\">-</td>')\n",
    "                row.append({'mean': np.nan, 'std': np.nan, 'n': np.nan})\n",
    "            else:\n",
    "                mean = np.mean(column_values)\n",
    "                std = np.std(column_values)\n",
    "                n = len(column_values)\n",
    "                table_parts.append(f'<td style=\"border-collapse: collapse;\">{mean:.3g}±{std:.1g} ({n})</td>')\n",
    "                row.append({'mean': mean, 'std': std, 'n': n})\n",
    "        rows.append(row)\n",
    "        # cells containing values\n",
    "\n",
    "        table_parts.append('</tr>')                                   # end row\n",
    "    raw_auc_data[task_name]['row_indexes'] = row_indexes\n",
    "    raw_auc_data[task_name]['rows'] = rows\n",
    "    table_parts.append('</table>')                                    # end table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_auc_dataframes = dict()\n",
    "\n",
    "for k in raw_auc_data.keys(): \n",
    "    print(k)\n",
    "    raw_auc_dataframes[k] = dict()\n",
    "    for stat in ['mean', 'std', 'n']:\n",
    "        columns = raw_auc_data[k]['columns']\n",
    "        index = raw_auc_data[k]['row_indexes']\n",
    "        rows = []\n",
    "        for raw_row in raw_auc_data[k]['rows']: \n",
    "            rows.append([el[stat] for el in raw_row])\n",
    "\n",
    "        raw_auc_dataframes[k][stat] = pd.DataFrame(rows, index=index, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface Check! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for task in raw_auc_dataframes.keys(): \n",
    "#     for stat in raw_auc_dataframes[task].keys(): \n",
    "#         raw_auc_dataframes[task][stat].to_csv(f\"cached_dfs/raw_auc_dataframes_{task}_{stat}.csv\")\n",
    "\n",
    "# for task in raw_return_dataframes.keys(): \n",
    "#     for stat in raw_return_dataframes[task].keys(): \n",
    "#         raw_return_dataframes[task][stat].to_csv(f\"cached_dfs/raw_return_dataframes_{task}_{stat}.csv\")\n",
    "\n",
    "# print(f\"These dataframes were last saved as a failsafe cache at {datetime.now()} PST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## RUN TO RELOAD DATA FROM DISK ## \n",
    "raw_return_dataframes = {}\n",
    "raw_auc_dataframes = {}\n",
    "for task in ['MatchRegions-Demo-v0', 'MoveToCorner-Demo-v0', 'MoveToRegion-Demo-v0', 'finger-spin', 'cheetah-run']: \n",
    "    raw_return_dataframes[task] = {}\n",
    "    raw_auc_dataframes[task] = {}\n",
    "    for stat in ['mean', 'std', 'n']: \n",
    "        raw_return_dataframes[task][stat] = pd.read_csv(f\"cached_dfs/raw_return_dataframes_{task}_{stat}.csv\", \n",
    "                                                            index_col=0)\n",
    "        raw_auc_dataframes[task][stat] = pd.read_csv(f\"cached_dfs/raw_auc_dataframes_{task}_{stat}.csv\", \n",
    "                                                         index_col=0)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_auc_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the raw data is stored in dataframes: raw_auc_dataframes and raw_return_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_index_val(index, check_algo=False, check_data=False): \n",
    "#     new_index = index \n",
    "#     for lookup_key, lookup_val in merge_lookups.items(): \n",
    "#         new_index = new_index.replace(lookup_key, lookup_val)\n",
    "#     return new_index \n",
    "\n",
    "def order_idx(indexes, algo_order):\n",
    "    for idx in indexes:\n",
    "        if idx in algo_order:\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"{idx} not arranged in algo_order, append to the list.\")\n",
    "            algo_order.append(idx)\n",
    "    return algo_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def control_not_key(col, control_key): \n",
    "#     if 'control' in col: \n",
    "#         if col == control_key: \n",
    "#             return True \n",
    "#         else: \n",
    "#             return False \n",
    "#     else: \n",
    "#         return True \n",
    "    \n",
    "# def should_keep_exp_ident(ident): \n",
    "#     if 'froco' in ident:\n",
    "#         return False \n",
    "#     if 'no_ortho' in ident and 'actual' not in ident: \n",
    "#         return False \n",
    "#     return True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def check_algo_or_data(index, lookup_dict, check_algo=False, check_data=False, verbose=True, blacklist=()):\n",
    "    new_index = index \n",
    "    assert sum([check_algo, check_data]) % 2 != 0, 'One of check_algo and check_data must be True'\n",
    "    inner_lookup_key = 'algo_lookups' if check_algo else 'data_lookups'\n",
    "    inner_lookup_dict = lookup_dict[inner_lookup_key]\n",
    "    check_type = 'data' if check_data else 'algo'\n",
    "    for term in blacklist: \n",
    "        if re.search(term, index): \n",
    "            if verbose: \n",
    "                print(f\"Run {index} skipped due to blacklist term {term}\")\n",
    "            return None \n",
    "    for lookup_key, lookup_val in inner_lookup_dict.items(): \n",
    "        if re.search(lookup_key, new_index):\n",
    "            new_index = lookup_val\n",
    "            return new_index\n",
    "    if verbose:\n",
    "        print(f\"Didn't find any {check_type} entries for {index}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_pivoted_dfs(base_df_dict, ret_col_lookup, blacklist_terms, \n",
    "                      whitelist_lookups, control_idx, algo_order, verbose=True): \n",
    "    data_dataframes = dict()\n",
    "    for task in base_df_dict.keys():\n",
    "        data_dataframes[task] = dict()\n",
    "        for stat in base_df_dict[task].keys():\n",
    "            dataset_list = set([check_algo_or_data(ind, whitelist_lookups, check_data=True, \n",
    "                                                   verbose=verbose, blacklist=blacklist_terms) \n",
    "                         for ind in base_df_dict[task][stat].index]) - set([None])\n",
    "            ret_col = ret_col_lookup[task]\n",
    "            algo_list = set([check_algo_or_data(algo, whitelist_lookups, check_algo=True, \n",
    "                                                verbose=verbose, blacklist=blacklist_terms) \n",
    "                         for algo in base_df_dict[task][stat][ret_col].index]) - set([None])\n",
    "            table = pd.DataFrame(np.zeros((len(algo_list), len(dataset_list))), \n",
    "                                 index=algo_list,\n",
    "                                 columns=dataset_list)\n",
    "            table[table == 0] = np.nan \n",
    "            \n",
    "            for exp_ident in base_df_dict[task][stat][ret_col].index:\n",
    "                data = check_algo_or_data(exp_ident, whitelist_lookups, check_data=True, \n",
    "                                          verbose=verbose, blacklist=blacklist_terms)\n",
    "                algo = check_algo_or_data(exp_ident, whitelist_lookups, check_algo=True, \n",
    "                                          verbose=verbose, blacklist=blacklist_terms)\n",
    "                if (data and algo):\n",
    "                    if verbose: \n",
    "                        print(f\"Using information from {exp_ident}, is that OK?\")\n",
    "                    if not math.isnan(float(base_df_dict[task][stat][ret_col][exp_ident])):\n",
    "                        try: \n",
    "                            table[data][algo] = float(base_df_dict[task][stat][ret_col][exp_ident])\n",
    "                        except Exception as e: \n",
    "                            print(e)\n",
    "                            import pdb; pdb.set_trace()\n",
    "                elif algo == control_idx: \n",
    "                    if not math.isnan(float(base_df_dict[task][stat][ret_col][exp_ident])):\n",
    "                        for dataset in dataset_list: \n",
    "                            table[dataset][algo] = float(base_df_dict[task][stat][ret_col][exp_ident])\n",
    "\n",
    "            # Move control to top of table\n",
    "            idx = order_idx(table.index, algo_order)\n",
    "            table = table.reindex(idx)\n",
    "            data_dataframes[task][stat] = table\n",
    "    return data_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_pooled_dfs(base_df_dict, control_idx): \n",
    "    pooled_dataframes = dict()\n",
    "\n",
    "    for task in base_df_dict.keys():\n",
    "\n",
    "        #create a copy of data_dataframes that I can modify \n",
    "        pooled_dataframes[task] = dict()\n",
    "        for stat in base_df_dict[task].keys(): \n",
    "            pooled_dataframes[task][stat] = base_df_dict[task][stat].copy(deep=True)\n",
    "\n",
    "\n",
    "        pooled_control_mean = pooled_dataframes[task]['mean'].loc[control_idx].mean()\n",
    "        pooled_control_std = pooled_dataframes[task]['std'].loc[control_idx].mean()\n",
    "        pooled_control_n = pooled_dataframes[task]['n'].loc[control_idx].sum()\n",
    "        pooled_dataframes[task]['mean'].loc[control_idx] = pooled_control_mean\n",
    "        pooled_dataframes[task]['std'].loc[control_idx] = pooled_control_std\n",
    "        pooled_dataframes[task]['n'].loc[control_idx] = pooled_control_n\n",
    "    return pooled_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_loss_dfs(base_df, blacklist_terms, whitelist_lookups, control_idx, verbose=False):\n",
    "\n",
    "    dataset_list = set([check_algo_or_data(row['exp_ident'], whitelist_lookups, check_data=True, \n",
    "                                            verbose=verbose, blacklist=blacklist_terms) \n",
    "                         for index, row in base_df.iterrows()]) - set([None])\n",
    "    dataset_list = [ds.replace('\\n','') for ds in dataset_list]\n",
    "    return_data = {data_source: [] for data_source in dataset_list}\n",
    "    for index, row in base_df.iterrows():\n",
    "        # Data source of this exp\n",
    "        data_source = check_algo_or_data(row['exp_ident'], whitelist_lookups, check_data=True, \n",
    "                                         verbose=verbose, blacklist=blacklist_terms)\n",
    "        new_exp_ident = check_algo_or_data(row['exp_ident'], whitelist_lookups, check_algo=True, \n",
    "                                         verbose=verbose, blacklist=blacklist_terms)\n",
    "        if data_source and new_exp_ident:\n",
    "            data_source = data_source.replace('\\n','')\n",
    "            return_data[data_source].append([new_exp_ident] + [row[key] for key in row.keys() if key != 'exp_ident'])\n",
    "        elif new_exp_ident == control_idx: \n",
    "            for dataset in dataset_list: \n",
    "                return_data[dataset].append([new_exp_ident] + [row[key] for key in row.keys() if key != 'exp_ident'])\n",
    "    \n",
    "    # Make dataframes\n",
    "    for data_source, value in return_data.items():\n",
    "        return_data[data_source] = pd.DataFrame(value, columns=base_df.columns)\n",
    "    return return_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def save_eps(sns_plot, env, plot_type):\n",
    "    save_dir = f\"./plots/{cluster_subpath.split('/')[1]}\"\n",
    "    save_path = os.path.join(save_dir, f\"{env}-{plot_type}.eps\")\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    fig = sns_plot.get_figure()\n",
    "    fig.savefig(save_path, bbox_inches=\"tight\", format='eps', dpi=1200)\n",
    "    print(f\"Plot saved at {os.path.abspath(save_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Plotting Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind_from_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_p_val_df(mean_df, std_df, n_df, control_idx, alternative):\n",
    "    test_means = mean_df[mean_df.index != control_idx]\n",
    "    control_means = mean_df.loc[control_idx]\n",
    "\n",
    "    test_std = std_df[std_df.index != control_idx]\n",
    "    control_std = std_df.loc[control_idx]\n",
    "\n",
    "    test_ns = n_df[n_df.index != control_idx]\n",
    "    control_ns = n_df.loc[control_idx]\n",
    "    \n",
    "    try: \n",
    "        t_stats, p_vals = ttest_ind_from_stats(test_means, test_std, test_ns, control_means, control_std, control_ns, \n",
    "                         equal_var=False, alternative=alternative)\n",
    "    except: \n",
    "        import pdb; pdb.set_trace()\n",
    "    \n",
    "    return p_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def task_return_heatmap(task, df_dict, control_key, fontsize=30, \n",
    "                        show_ylabel=True, font_scale=2, narrow=False, min_max_vals=None): \n",
    "    sns.set(font=\"Times New Roman\", font_scale=font_scale)\n",
    "    \n",
    "    \n",
    "    mean_df = df_dict[task]['mean']\n",
    "    str_format = \"{:.2f}\" if mean_df.loc[:].min()[0] < 1 else \"{:.0f}\"\n",
    "\n",
    "    \n",
    "    std_df = df_dict[task]['std']\n",
    "    \n",
    "    \n",
    "    n_df = df_dict[task]['n']\n",
    "    \n",
    "    p_vals = get_p_val_df(mean_df, std_df, n_df, control_key, alternative=\"greater\")\n",
    "    p_val_df = pd.DataFrame(p_vals, index=[el for el in mean_df.index if el != control_key],\n",
    "                            columns=mean_df.columns)\n",
    "    normed_df = mean_df - mean_df.loc[control_key]\n",
    "\n",
    "        \n",
    "    std_df = std_df.applymap(str_format.format)\n",
    "    mean_df = mean_df.applymap(str_format.format)\n",
    "    \n",
    "    text_df = pd.DataFrame()\n",
    "    for col in mean_df.columns:\n",
    "        text_df[col] = mean_df[col].astype(str) + ' (' + std_df[col].astype(str) + ')'\n",
    "    \n",
    "    for col in text_df.columns: \n",
    "        for ind in text_df.index: \n",
    "            if ind == control_key: \n",
    "                continue \n",
    "            elif p_val_df[col][ind] < 0.05: \n",
    "                text_df[col][ind] = text_df[col][ind] + '**'\n",
    "    \n",
    "    if narrow:\n",
    "        figsize = (4, 7)\n",
    "    else: \n",
    "        figsize = (13, 7)\n",
    "    if show_ylabel:\n",
    "        figsize = (figsize[0]+3, figsize[1])\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    plt.title(f\"Mean Return: {task}\", fontsize=30)\n",
    "    \n",
    "    if min_max_vals is not None: \n",
    "        vmin, vmax = min_max_vals\n",
    "    else: \n",
    "        vmin, vmax = None, None \n",
    "        \n",
    "\n",
    "    ax = sns.heatmap(normed_df, annot=text_df, annot_kws={\"fontsize\":fontsize}, cbar=False,\n",
    "                     cmap=sns.diverging_palette(250, 10, s=60, l=45, as_cmap=True), \n",
    "                     center=0, fmt='.20', yticklabels=show_ylabel, vmin=vmin, vmax=vmax)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def task_auc_heatmap(task, df_dict, control_key, fontsize=30,\n",
    "                     show_ylabel=True, font_scale=2, show_sig=False, narrow=False, min_max_vals=None): \n",
    "    sns.set(font=\"Times New Roman\", font_scale=font_scale)\n",
    "    \n",
    "    str_format = \"{:.2f}\"\n",
    "    subset_df = df_dict[task]['mean']\n",
    "    std_df = df_dict[task]['std']\n",
    "    n_df = df_dict[task]['n']\n",
    "    \n",
    "    p_vals = get_p_val_df(subset_df, std_df, n_df, control_key, alternative=\"less\")\n",
    "    p_val_df = pd.DataFrame(p_vals, \n",
    "                            index=[el for el in subset_df.index if el != control_key],\n",
    "                            columns=subset_df.columns)\n",
    "    std_sub_df = std_df.applymap(str_format.format)\n",
    "        \n",
    "    normed_df = subset_df - subset_df.loc[control_key]\n",
    "    \n",
    "    subset_df = subset_df.applymap(str_format.format)\n",
    "    text_df = pd.DataFrame()\n",
    "    for col in subset_df.columns:\n",
    "        text_df[col] = subset_df[col].astype(str) + ' (' + std_sub_df[col].astype(str) + ')'\n",
    "    \n",
    "    if show_sig: \n",
    "        for col in text_df.columns: \n",
    "            for ind in text_df.index: \n",
    "                if ind == control_key: \n",
    "                    continue \n",
    "                elif p_val_df[col][ind] < 0.05: \n",
    "                    text_df[col][ind] = text_df[col][ind] + '**'\n",
    "        \n",
    "    if narrow:\n",
    "        figsize = (4, 7)\n",
    "    else: \n",
    "        figsize = (13, 7)\n",
    "    if show_ylabel:\n",
    "        figsize = (figsize[0]+3, figsize[1])\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(f\"AUC: {task}\", fontsize=30)\n",
    "    \n",
    "    if min_max_vals is not None: \n",
    "        vmin, vmax = min_max_vals\n",
    "    else: \n",
    "        vmin, vmax = None, None \n",
    "    \n",
    "    \n",
    "    ax = sns.heatmap(normed_df, annot=text_df, annot_kws={\"fontsize\":fontsize}, cbar=False,\n",
    "                     cmap=sns.diverging_palette(10, 250, s=60, l=45, as_cmap=True), \n",
    "                     center=0, fmt='.20', yticklabels=show_ylabel, vmin=vmin, vmax=vmax)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(env, data_source, task_df, algo_order, show_ylabel=False, show_legend=False,\n",
    "                    ylim=None):\n",
    "    sns.set(font=\"Times New Roman\", font_scale=1.5)\n",
    "#     sns.set(font=\"Verdana\")  # Use default font?\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.title(f\"{data_source}\")\n",
    "    kw_legend = \"brief\" if show_legend else False\n",
    "    ax = sns.lineplot(data=task_df, x=\"batches\", y=\"loss\", hue=\"exp_ident\", hue_order=algo_order, \n",
    "                     legend=kw_legend)\n",
    "    \n",
    "    ylabel = f\"{env} Loss\" if show_ylabel else None\n",
    "    ax.set(xlabel=\"Epoch\", ylabel=ylabel)\n",
    "    if ylim:\n",
    "        ax.set(ylim=ylim)\n",
    "    if show_legend:\n",
    "        legend = ax.legend()\n",
    "        legend.texts[0].set_text(\"Algorithm\")\n",
    "        plt.setp(ax.get_legend().get_texts(), fontsize='12') # for legend text\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Baseline Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Return Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "control_idx = 'Control'\n",
    "\n",
    "envs = ['cheetah-run', 'finger-spin', 'MatchRegions-Demo-v0', 'MoveToRegion-Demo-v0']\n",
    "\n",
    "dmc_blacklist_terms = ['froco', 'ablation', 'newbcaugs']\n",
    "dmc_algo_lookups = {\n",
    "    '^icml_inv_dyn_': \"Inverse Dynamics\", \n",
    "    \"^icml_ac_tcpc_\": \"Action Conditioned TCPC\", \n",
    "    \"^icml_vae_\": \"VAE\", \n",
    "    \"^icml_dynamics_\": \"Dynamics Model\",\n",
    "    \"^control_ortho_init_\": \"Control\",\n",
    "    \"^icml_identity_cpc_\": \"Temporal CPC (TCPC)\", #due to error\n",
    "}\n",
    "\n",
    "\n",
    "magical_blacklist_terms = ['froco', 'ablation']\n",
    "magical_algo_lookups = {\n",
    "    '^icml_inv_dyn_': \"Inverse Dynamics\", \n",
    "    \"^icml_ac_tcpc_\": \"Action Conditioned TCPC\", \n",
    "    \"^icml_vae_\": \"VAE\", \n",
    "    \"^icml_dynamics_\": \"Dynamics Model\",\n",
    "    \"^control_ortho_init\": \"Control\",\n",
    "    \"^icml_identity_cpc_\": \"Temporal CPC (TCPC)\", \n",
    "}\n",
    "\n",
    "data_lookups = {\n",
    "    \"cfg_data_repl_random\": \"Random Rollouts\", \n",
    "    \"cfg_data_repl_demos_random\": \"Demos & \\nRandom Rollouts\", \n",
    "    \"cfg_data_repl_demos_magical_mt\": \"Multitask Demos\", \n",
    "    \"cfg_data_repl_rand_demos_magical_mt\": \"Multitask Demos & \\nRandom Rollouts\",\n",
    "}\n",
    "\n",
    "magical_whitelist_lookups = {'data_lookups': data_lookups, 'algo_lookups': magical_algo_lookups}\n",
    "dmc_whitelist_lookups = {'data_lookups': data_lookups, 'algo_lookups': dmc_algo_lookups}\n",
    "\n",
    "\n",
    "## ***CHANGE THESE NAMES FOR NEW PLOTS *** \n",
    "baseline_magical_configs = dict(control_idx=control_idx, \n",
    "                                blacklist_terms=magical_blacklist_terms, \n",
    "                                whitelist_lookups=magical_whitelist_lookups) \n",
    "baseline_dmc_configs = dict(control_idx=control_idx, \n",
    "                            blacklist_terms=dmc_blacklist_terms, \n",
    "                            whitelist_lookups=dmc_whitelist_lookups) \n",
    "\n",
    "baseline_plot_config_lookup = {'MoveToRegion-Demo-v0': baseline_magical_configs, \n",
    "                             'MatchRegions-Demo-v0': baseline_magical_configs, \n",
    "                             'cheetah-run': baseline_dmc_configs, \n",
    "                             'finger-spin': baseline_dmc_configs\n",
    "                               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Configs for baseline plots \n",
    "\n",
    "ret_col_lookup = {\n",
    "    'MatchRegions-Demo-v0': 'Average on all envs', \n",
    "    'MoveToRegion-Demo-v0': 'Average on all envs', \n",
    "    'MoveToCorner-Demo-v0': 'Average on all envs', \n",
    "    'finger-spin': 'return_mean', \n",
    "    'cheetah-run': 'return_mean'\n",
    "}\n",
    "\n",
    "# Set plot properties according to where we want to position it in paper\n",
    "right_envs = ['MatchRegions-Demo-v0', 'cheetah-run']\n",
    "left_envs = ['MoveToRegion-Demo-v0', 'finger-spin']\n",
    "\n",
    "algo_order = [\"Control\", \"Temporal CPC (TCPC)\", \"Action Conditioned TCPC\", \"VAE\", \"Dynamics Model\",\n",
    "              \"Inverse Dynamics\"]\n",
    "\n",
    "for env in envs:\n",
    "    print(f\"Creating plot for {env}\")\n",
    "    plot_config = baseline_plot_config_lookup[env]\n",
    "    pivoted_dfs = create_pivoted_dfs(raw_return_dataframes, ret_col_lookup, \n",
    "                                     plot_config['blacklist_terms'], plot_config['whitelist_lookups'], \n",
    "                                     plot_config['control_idx'], algo_order, verbose=False)\n",
    "    pooled_dfs = create_pooled_dfs(pivoted_dfs, control_idx=plot_config['control_idx'])\n",
    "    kwargs = {}\n",
    "    if env in left_envs:\n",
    "        kwargs = {'show_ylabel': False}\n",
    "    env_plot = task_return_heatmap(env, pooled_dfs, control_key=control_idx, min_max_vals=(-.1, .1), **kwargs)\n",
    "    display(env_plot)\n",
    "    save_eps(env_plot, env, 'return')\n",
    "\n",
    "# # to be paranoid\n",
    "del pivoted_dfs \n",
    "del pooled_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###  AUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ret_col_lookup = {\n",
    "    'MatchRegions-Demo-v0': 'step 40', \n",
    "    'MoveToRegion-Demo-v0': 'step 40', \n",
    "    'MoveToCorner-Demo-v0': 'step 40', \n",
    "    'finger-spin': 'step 400', \n",
    "    'cheetah-run': 'step 400'\n",
    "}\n",
    "\n",
    "for env in envs:\n",
    "    plot_config = baseline_plot_config_lookup[env]\n",
    "    pivoted_dfs = create_pivoted_dfs(raw_auc_dataframes, ret_col_lookup, \n",
    "                                     plot_config['blacklist_terms'], plot_config['whitelist_lookups'], \n",
    "                                     plot_config['control_idx'], algo_order, verbose=False)\n",
    "    pooled_dfs = create_pooled_dfs(pivoted_dfs, control_idx=plot_config['control_idx'])\n",
    "    kwargs = {}\n",
    "    if env in left_envs:\n",
    "        kwargs = {'show_ylabel': False}\n",
    "    env_plot = task_auc_heatmap(env, pooled_dfs, control_key=control_idx, **kwargs)\n",
    "    display(env_plot)\n",
    "    save_eps(env_plot, env, 'AUC')\n",
    "\n",
    "# to be paranoid\n",
    "del pivoted_dfs \n",
    "del pooled_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env_ylim = {\n",
    "    'MatchRegions-Demo-v0': (0.05, 0.2),\n",
    "    'MoveToRegion-Demo-v0': (0.05, 0.2),\n",
    "    'cheetah-run': (1, 4),\n",
    "    'finger-spin': (-0.7, 0)\n",
    "}\n",
    "\n",
    "for env in envs:\n",
    "    plot_config = baseline_plot_config_lookup[env]\n",
    "    filtered_dfs = filter_loss_dfs(raw_loss_data[env], \n",
    "                                   plot_config['blacklist_terms'], \n",
    "                                   plot_config['whitelist_lookups'], \n",
    "                                   plot_config['control_idx'],\n",
    "                                   verbose=False)\n",
    "    print(f\"Start plotting curves for env {env}...\")\n",
    "    for data_source, df in filtered_dfs.items():\n",
    "        print(set(df['exp_ident']))\n",
    "        kwargs = {}\n",
    "        if data_source == 'Demos & Random Rollouts':\n",
    "            kwargs['show_ylabel'] = True\n",
    "            if env == 'MatchRegions-Demo-v0':\n",
    "                kwargs['show_legend'] = True\n",
    "        if env in env_ylim.keys():\n",
    "            kwargs['ylim'] = env_ylim[env]\n",
    "        env_plot = plot_loss_curves(env, data_source, df, algo_order, **kwargs)\n",
    "        display(env_plot)\n",
    "        filename = f\"{env}-{data_source}\".replace(\" \", \"\").replace(\"&\", \"-\")\n",
    "        save_eps(env_plot, filename, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FROCO plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "control_idx = 'Control'\n",
    "\n",
    "envs = ['cheetah-run', 'finger-spin', 'MatchRegions-Demo-v0', 'MoveToRegion-Demo-v0']\n",
    "\n",
    "\n",
    "froco_blacklist_terms = ['ablation', 'newbcaugs']\n",
    "froco_algo_lookups = {\n",
    "    '^froco_icml_inv_dyn_': \"Inverse Dynamics\", \n",
    "    \"^froco_icml_ac_tcpc_\": \"Action Conditioned TCPC\", \n",
    "    \"^froco_icml_vae_\": \"VAE\", \n",
    "    \"^froco_icml_dynamics_\": \"Dynamics Model\",\n",
    "    \"^froco_control_ortho_init\": \"Control\",\n",
    "    \"^froco_icml_identity_cpc_\": \"Temporal CPC (TCPC)\", # Due to error \n",
    "}\n",
    "\n",
    "data_lookups = {\n",
    "    \"cfg_data_repl_random\": \"Random Rollouts\", \n",
    "    \"cfg_data_repl_demos_random\": \"Demos & \\nRandom Rollouts\", \n",
    "    \"cfg_data_repl_demos_magical_mt\": \"Multitask Demos\", \n",
    "    \"cfg_data_repl_rand_demos_magical_mt\": \"Multitask Demos & \\nRandom Rollouts\",\n",
    "}\n",
    "\n",
    "froco_whitelist_lookups = {'data_lookups': data_lookups, 'algo_lookups': froco_algo_lookups}\n",
    "\n",
    "\n",
    "## ***CHANGE THESE NAMES FOR NEW PLOTS *** \n",
    "froco_configs = dict(control_idx=control_idx, \n",
    "                                blacklist_terms=froco_blacklist_terms, \n",
    "                                whitelist_lookups=froco_whitelist_lookups) \n",
    "\n",
    "froco_plot_config_lookup = {'MoveToRegion-Demo-v0': froco_configs, \n",
    "                             'MatchRegions-Demo-v0': froco_configs, \n",
    "                             'cheetah-run': froco_configs, \n",
    "                             'finger-spin': froco_configs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Configs for baseline plots \n",
    "\n",
    "ret_col_lookup = {\n",
    "    'MatchRegions-Demo-v0': 'Average on all envs', \n",
    "    'MoveToRegion-Demo-v0': 'Average on all envs', \n",
    "    'MoveToCorner-Demo-v0': 'Average on all envs', \n",
    "    'finger-spin': 'return_mean', \n",
    "    'cheetah-run': 'return_mean'\n",
    "}\n",
    "\n",
    "# Set plot properties according to where we want to position it in paper\n",
    "right_envs = ['MatchRegions-Demo-v0', 'cheetah-run']\n",
    "left_envs = ['MoveToRegion-Demo-v0', 'finger-spin']\n",
    "\n",
    "algo_order = [\"Control\", \"Temporal CPC (TCPC)\", \"Action Conditioned TCPC \", \"VAE \", \"Dynamics Model \",\n",
    "              \"Inverse Dynamics \"]\n",
    "\n",
    "\n",
    "\n",
    "for env in envs:\n",
    "    print(f\"Creating FROCO plot for {env}\")\n",
    "    plot_config = froco_plot_config_lookup[env]\n",
    "    pivoted_dfs = create_pivoted_dfs(raw_return_dataframes, ret_col_lookup, \n",
    "                                     plot_config['blacklist_terms'], plot_config['whitelist_lookups'], \n",
    "                                     plot_config['control_idx'], algo_order, verbose=False)\n",
    "    pooled_dfs = create_pooled_dfs(pivoted_dfs, control_idx=plot_config['control_idx'])\n",
    "    kwargs = {}\n",
    "\n",
    "    if env in left_envs:\n",
    "        kwargs = {'show_ylabel': False}\n",
    "    \n",
    "    if env == 'MoveToRegion-Demo-v0': \n",
    "        kwargs['min_max_vals'] = (-.2, .2)\n",
    "    env_plot = task_return_heatmap(env, pooled_dfs, control_key=control_idx, **kwargs)\n",
    "    display(env_plot)\n",
    "    save_eps(env_plot, env, 'froco_return')\n",
    "\n",
    "# to be paranoid\n",
    "del pivoted_dfs \n",
    "del pooled_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Froco AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Configs for baseline plots \n",
    "\n",
    "ret_col_lookup = {\n",
    "    'MatchRegions-Demo-v0': 'step 40', \n",
    "    'MoveToRegion-Demo-v0': 'step 40', \n",
    "    'MoveToCorner-Demo-v0': 'step 40', \n",
    "    'finger-spin': 'step 400', \n",
    "    'cheetah-run': 'step 400'\n",
    "}\n",
    "# Set plot properties according to where we want to position it in paper\n",
    "right_envs = ['MatchRegions-Demo-v0', 'cheetah-run']\n",
    "left_envs = ['MoveToRegion-Demo-v0', 'finger-spin']\n",
    "\n",
    "algo_order = [\"Control\", \"Temporal CPC (TCPC)\", \"Action Conditioned TCPC \", \"VAE \", \"Dynamics Model \",\n",
    "              \"Inverse Dynamics \"]\n",
    "\n",
    "\n",
    "\n",
    "for env in envs:\n",
    "    print(f\"Creating FROCO plot for {env}\")\n",
    "    plot_config = froco_plot_config_lookup[env]\n",
    "    pivoted_dfs = create_pivoted_dfs(raw_auc_dataframes, ret_col_lookup, \n",
    "                                     plot_config['blacklist_terms'], plot_config['whitelist_lookups'], \n",
    "                                     plot_config['control_idx'], algo_order, verbose=False)\n",
    "    pooled_dfs = create_pooled_dfs(pivoted_dfs, control_idx=plot_config['control_idx'])\n",
    "    kwargs = {}\n",
    "\n",
    "    if env in left_envs:\n",
    "        kwargs = {'show_ylabel': False}\n",
    "    env_plot = task_auc_heatmap(env, pooled_dfs, control_key=control_idx, **kwargs)\n",
    "    display(env_plot)\n",
    "    save_eps(env_plot, env, 'froco_auc')\n",
    "\n",
    "# to be paranoid\n",
    "del pivoted_dfs \n",
    "del pooled_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_ylim = {\n",
    "    'MatchRegions-Demo-v0': (0, 4),\n",
    "    'MoveToRegion-Demo-v0': (0, 2),\n",
    "    'cheetah-run': (3.5, 7),\n",
    "    'finger-spin': (0, 2.5)\n",
    "}\n",
    "\n",
    "for env in envs:\n",
    "    plot_config = froco_plot_config_lookup[env]\n",
    "    filtered_dfs = filter_loss_dfs(raw_loss_data[env], \n",
    "                                   plot_config['blacklist_terms'], \n",
    "                                   plot_config['whitelist_lookups'], \n",
    "                                   plot_config['control_idx'],\n",
    "                                   verbose=False)\n",
    "    print(f\"Start plotting curves for env {env}...\")\n",
    "    for data_source, df in filtered_dfs.items():\n",
    "        print(set(df['exp_ident']))\n",
    "        kwargs = {}\n",
    "        if data_source == 'Demos & Random Rollouts':\n",
    "            kwargs['show_ylabel'] = True\n",
    "            if env == 'MatchRegions-Demo-v0':\n",
    "                kwargs['show_legend'] = True\n",
    "        if env in env_ylim.keys():\n",
    "            kwargs['ylim'] = env_ylim[env]\n",
    "        env_plot = plot_loss_curves(env, data_source, df, algo_order, **kwargs)\n",
    "        display(env_plot)\n",
    "        filename = f\"{env}-{data_source}\".replace(\" \", \"\").replace(\"&\", \"-\")\n",
    "        save_eps(env_plot, filename, 'froco_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To sync plots: '/home/cody/il-representations/analysis/plots/cluster-2021-01-29-set3-try4/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablations Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "control_idx = 'Temporal CPC'\n",
    "\n",
    "envs = ['cheetah-run', 'finger-spin', 'MatchRegions-Demo-v0', 'MoveToRegion-Demo-v0']\n",
    "\n",
    "\n",
    "ablation_blacklist_terms = ['froco', 'newbcaugs']\n",
    "ablation_algo_lookups = {\n",
    "    '^ablation_icml_tcpc_no_augs_': \"Temporal CPC - No Augmentations\",\n",
    "    '^ablation_icml_tcpc_momentum_': \"Momentum Temporal CPC\", \n",
    "    \"^icml_ac_tcpc_\": \"Action Conditioned TCPC\", \n",
    "    \"^ablation_icml_tceb_\": \"Temporal CEB\", \n",
    "    \"^ablation_icml_four_tcpc_\": \"Temporal CPC (t=4)\",\n",
    "    \"^icml_identity_cpc\": \"Temporal CPC\",\n",
    "    \"^ablation_icml_identity_cpc_\": \"Identity CPC (Not temporal)\", # TODO fix overlapping issue\n",
    "}\n",
    "\n",
    "ablation_data_lookups = {\n",
    "    \"cfg_data_repl_random\": \"Random Rollouts\", \n",
    "}\n",
    "\n",
    "ablation_whitelist_lookups = {'data_lookups': ablation_data_lookups, 'algo_lookups': ablation_algo_lookups}\n",
    "\n",
    "\n",
    "## ***CHANGE THESE NAMES FOR NEW PLOTS *** \n",
    "ablation_configs = dict(control_idx=control_idx, \n",
    "                                blacklist_terms=ablation_blacklist_terms, \n",
    "                                whitelist_lookups=ablation_whitelist_lookups) \n",
    "\n",
    "ablation_plot_config_lookup = {'MoveToRegion-Demo-v0': ablation_configs, \n",
    "                             'MatchRegions-Demo-v0': ablation_configs, \n",
    "                             'cheetah-run': ablation_configs, \n",
    "                             'finger-spin': ablation_configs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablation Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Configs for baseline plots \n",
    "\n",
    "ret_col_lookup = {\n",
    "    'MatchRegions-Demo-v0': 'Average on all envs', \n",
    "    'MoveToRegion-Demo-v0': 'Average on all envs', \n",
    "    'MoveToCorner-Demo-v0': 'Average on all envs', \n",
    "    'finger-spin': 'return_mean', \n",
    "    'cheetah-run': 'return_mean'\n",
    "}\n",
    "\n",
    "# Set plot properties according to where we want to position it in paper\n",
    " # NOTE these are inverted left/right atm \n",
    "right_envs = ['cheetah-run']\n",
    "left_envs = ['MoveToRegion-Demo-v0']\n",
    "center_envs = ['finger-spin', 'MatchRegions-Demo-v0']\n",
    "\n",
    "algo_order = [\"Temporal CPC\", \"Identity CPC (Not temporal)\", \"Temporal CPC (t=4)\", \"Temporal CEB\", \"Action Conditioned TCPC\",\n",
    "              \"Momentum Temporal CPC\", \"Temporal CPC - No Augmentations\"]\n",
    "\n",
    "\n",
    "\n",
    "for env in envs:\n",
    "    print(f\"Creating ablations plot for {env}\")\n",
    "    plot_config = ablation_plot_config_lookup[env]\n",
    "    pivoted_dfs = create_pivoted_dfs(raw_return_dataframes, ret_col_lookup, \n",
    "                                     plot_config['blacklist_terms'], plot_config['whitelist_lookups'], \n",
    "                                     plot_config['control_idx'], algo_order, verbose=False)\n",
    "    pooled_dfs = create_pooled_dfs(pivoted_dfs, control_idx=plot_config['control_idx'])\n",
    "    kwargs = {}\n",
    "\n",
    "    if env in left_envs:\n",
    "        kwargs = {'show_ylabel': False}\n",
    "    if env in center_envs: \n",
    "        kwargs = {'show_ylabel': False}\n",
    "    env_plot = task_return_heatmap(env, pooled_dfs, control_key=control_idx, narrow=True, **kwargs)\n",
    "    display(env_plot)\n",
    "    save_eps(env_plot, env, 'ablation_return')\n",
    "\n",
    "# to be paranoid\n",
    "del pivoted_dfs \n",
    "del pooled_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablation AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "ret_col_lookup = {\n",
    "    'MatchRegions-Demo-v0': 'step 40', \n",
    "    'MoveToRegion-Demo-v0': 'step 40', \n",
    "    'MoveToCorner-Demo-v0': 'step 40', \n",
    "    'finger-spin': 'step 400', \n",
    "    'cheetah-run': 'step 400'\n",
    "}\n",
    "\n",
    " # NOTE these are inverted left/right atm \n",
    "right_envs = ['cheetah-run']\n",
    "left_envs = ['MoveToRegion-Demo-v0']\n",
    "center_envs = ['finger-spin', 'MatchRegions-Demo-v0']\n",
    "\n",
    "algo_order = [\"Temporal CPC\", \"Identity CPC (Not temporal)\", \"Temporal CPC (t=4)\", \"Temporal CEB\", \"Action Conditioned TCPC\",\n",
    "              \"Momentum Temporal CPC\", \"Temporal CPC - No Augmentations\"]\n",
    "\n",
    "\n",
    "\n",
    "for env in envs:\n",
    "    plot_config = ablation_plot_config_lookup[env]\n",
    "    pivoted_dfs = create_pivoted_dfs(raw_auc_dataframes, ret_col_lookup, \n",
    "                                     plot_config['blacklist_terms'], plot_config['whitelist_lookups'], \n",
    "                                     plot_config['control_idx'], algo_order, verbose=False)\n",
    "    pooled_dfs = create_pooled_dfs(pivoted_dfs, control_idx=plot_config['control_idx'])\n",
    "    kwargs = {}\n",
    "\n",
    "    if env in left_envs:\n",
    "        kwargs = {'show_ylabel': False}\n",
    "    if env in center_envs: \n",
    "        kwargs = {'show_ylabel': False}\n",
    "    env_plot = task_auc_heatmap(env, pooled_dfs, control_key=control_idx, show_sig=True, narrow=True, **kwargs)\n",
    "    display(env_plot)\n",
    "    save_eps(env_plot, env, 'ablation_auc')\n",
    "\n",
    "# to be paranoid\n",
    "del pivoted_dfs \n",
    "del pooled_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hail Mary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Goal: For Magical, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "magical_envs = ['MatchRegions-Demo-v0', 'MoveToRegion-Demo-v0', 'MoveToCorner-Demo-v0']\n",
    "columns_to_not_avg = ['Average on all envs', 'MatchRegions-TestColour', \n",
    "                      'MoveToRegion-TestColour', 'MoveToCorner-TestColour']\n",
    "\n",
    "def create_non_colour_average_column(base_df_dict):\n",
    "    new_df_dict = dict()\n",
    "    for task in base_df_dict.keys():\n",
    "        if task not in magical_envs: \n",
    "            new_df_dict[task] = base_df_dict[task]\n",
    "        else:\n",
    "            new_df_dict[task] = dict()\n",
    "            old_mean_df = base_df_dict[task]['mean']\n",
    "            new_df_dict[task]['mean'] = old_mean_df.copy(deep=True)\n",
    "            new_df_dict[task]['mean']['Manual Average'] = old_mean_df[[col for col in old_mean_df.columns \n",
    "                                                                    if col not in columns_to_not_avg]].mean(axis=1)\n",
    "            old_std_df = base_df_dict[task]['std']\n",
    "            new_df_dict[task]['std'] = old_std_df.copy(deep=True)\n",
    "            new_df_dict[task]['std']['Manual Average'] = old_std_df[[col for col in old_std_df.columns \n",
    "                                                                    if col not in columns_to_not_avg]].mean(axis=1)  \n",
    "            \n",
    "            new_df_dict[task]['n'] = base_df_dict[task]['n'].copy(deep=True)\n",
    "            new_df_dict[task]['n']['Manual Average'] = new_df_dict[task]['n']['Average on all envs']\n",
    "            \n",
    "    return new_df_dict\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "control_idx = 'Control'\n",
    "\n",
    "envs = ['cheetah-run', 'finger-spin', 'MatchRegions-Demo-v0', 'MoveToRegion-Demo-v0']\n",
    "\n",
    "\n",
    "hail_mary_blacklist_terms = ['froco', 'ablation', 'newbcaugs']\n",
    "hail_mary_algo_lookups = {\n",
    "    '^icml_inv_dyn_': \"Inverse Dynamics \", \n",
    "    \"^icml_ac_tcpc_\": \"Action Conditioned TCPC \", \n",
    "    \"^icml_vae_\": \"VAE \", \n",
    "    \"^icml_dynamics_\": \"Dynamics Model \",\n",
    "    \"^control_ortho_init_\": \"Control\",\n",
    "    \"^icml_identity_cpc_\": \"Temporal CPC (TCPC)\", #due to error\n",
    "}\n",
    "\n",
    "data_lookups = {\n",
    "    \"cfg_data_repl_random\": \"Random Rollouts\", \n",
    "    \"cfg_data_repl_demos_random\": \"Demos & \\nRandom Rollouts\", \n",
    "    \"cfg_data_repl_demos_magical_mt\": \"Multitask Demos\", \n",
    "    \"cfg_data_repl_rand_demos_magical_mt\": \"Multitask Demos & \\nRandom Rollouts\",\n",
    "}\n",
    "\n",
    "hail_mary_whitelist_lookups = {'data_lookups': data_lookups, 'algo_lookups': hail_mary_algo_lookups}\n",
    "\n",
    "\n",
    "## ***CHANGE THESE NAMES FOR NEW PLOTS *** \n",
    "hail_mary_configs = dict(control_idx=control_idx, \n",
    "                         blacklist_terms=hail_mary_blacklist_terms, \n",
    "                         whitelist_lookups=hail_mary_whitelist_lookups) \n",
    "\n",
    "hail_mary_plot_config_lookup = {'MoveToRegion-Demo-v0': hail_mary_configs, \n",
    "                             'MatchRegions-Demo-v0': hail_mary_configs, \n",
    "                             'cheetah-run': hail_mary_configs, \n",
    "                             'finger-spin': hail_mary_configs\n",
    "                               }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hail Mary Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configs for baseline plots \n",
    "\n",
    "new_ret_col_lookup = {\n",
    "    'MatchRegions-Demo-v0': 'Manual Average', \n",
    "    'MoveToRegion-Demo-v0': 'Manual Average', \n",
    "    'MoveToCorner-Demo-v0': 'Manual Average', \n",
    "    'finger-spin': 'return_mean', \n",
    "    'cheetah-run': 'return_mean'\n",
    "}\n",
    "\n",
    "range_lookup = {\n",
    "    'MatchRegions-Demo-v0': (-.1, .1), \n",
    "    'MoveToRegion-Demo-v0': (-.1, .1), \n",
    "    'MoveToCorner-Demo-v0': (-.1, .1), \n",
    "    'finger-spin': (-100, 100), \n",
    "    'cheetah-run': (-100, 100)\n",
    "}\n",
    "\n",
    "# Set plot properties according to where we want to position it in paper\n",
    "right_envs = ['MatchRegions-Demo-v0', 'cheetah-run']\n",
    "left_envs = ['MoveToRegion-Demo-v0', 'finger-spin']\n",
    "\n",
    "algo_order = [\"Control\", \"Temporal CPC (TCPC)\", \"Action Conditioned TCPC \", \"VAE \", \"Dynamics Model \",\n",
    "              \"Inverse Dynamics \"]\n",
    "\n",
    "\n",
    "\n",
    "for env in envs:\n",
    "    print(f\"Creating plot for {env}\")\n",
    "    # CHANGE ME FOR EACH PLOT\n",
    "    plot_config = hail_mary_plot_config_lookup[env]\n",
    "    # CHANGE ME IF SWITCHING BETWEEN AUC AND RETURN \n",
    "    manually_averaged_dfs = create_non_colour_average_column(raw_return_dataframes)\n",
    "    pivoted_dfs = create_pivoted_dfs(manually_averaged_dfs, new_ret_col_lookup, \n",
    "                                     plot_config['blacklist_terms'], plot_config['whitelist_lookups'], \n",
    "                                     plot_config['control_idx'], algo_order, verbose=False)\n",
    "    pooled_dfs = create_pooled_dfs(pivoted_dfs, control_idx=plot_config['control_idx'])\n",
    "    kwargs = {}\n",
    "    if env in left_envs:\n",
    "        kwargs = {'show_ylabel': False}\n",
    "    min_max_vals = range_lookup[env]\n",
    "    env_plot = task_return_heatmap(env, pooled_dfs, control_key=control_idx,min_max_vals=min_max_vals , **kwargs)\n",
    "    display(env_plot)\n",
    "    # CHANGE ME TO SAVE PROPERLY\n",
    "    save_eps(env_plot, env, 'hail_mary_return')\n",
    "\n",
    "# # to be paranoid\n",
    "del pivoted_dfs \n",
    "del pooled_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hail Mary AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configs for baseline plots \n",
    "\n",
    "new_ret_col_lookup = {\n",
    "    'MatchRegions-Demo-v0': 'step 40', \n",
    "    'MoveToRegion-Demo-v0': 'step 40', \n",
    "    'MoveToCorner-Demo-v0': 'step 40', \n",
    "    'finger-spin': 'step 400', \n",
    "    'cheetah-run':'step 400'\n",
    "}\n",
    "\n",
    "\n",
    "# Set plot properties according to where we want to position it in paper\n",
    "right_envs = ['MatchRegions-Demo-v0', 'cheetah-run']\n",
    "left_envs = ['MoveToRegion-Demo-v0', 'finger-spin']\n",
    "\n",
    "algo_order = [\"Control\", \"Temporal CPC (TCPC)\", \"Action Conditioned TCPC \", \"VAE \", \"Dynamics Model \",\n",
    "              \"Inverse Dynamics \"]\n",
    "\n",
    "\n",
    "\n",
    "for env in envs:\n",
    "    print(f\"Creating plot for {env}\")\n",
    "    # CHANGE ME FOR EACH PLOT\n",
    "    plot_config = hail_mary_plot_config_lookup[env]\n",
    "    # CHANGE ME IF SWITCHING BETWEEN AUC AND RETURN \n",
    "    pivoted_dfs = create_pivoted_dfs(raw_auc_dataframes, new_ret_col_lookup, \n",
    "                                     plot_config['blacklist_terms'], plot_config['whitelist_lookups'], \n",
    "                                     plot_config['control_idx'], algo_order, verbose=False)\n",
    "    pooled_dfs = create_pooled_dfs(pivoted_dfs, control_idx=plot_config['control_idx'])\n",
    "    kwargs = {}\n",
    "    if env in left_envs:\n",
    "        kwargs = {'show_ylabel': False}\n",
    "    env_plot = task_auc_heatmap(env, pooled_dfs, control_key=control_idx, **kwargs)\n",
    "    display(env_plot)\n",
    "    # CHANGE ME TO SAVE PROPERLY\n",
    "    save_eps(env_plot, env, 'hail_mary_auc')\n",
    "\n",
    "# # to be paranoid\n",
    "del pivoted_dfs \n",
    "del pooled_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Older Code, Possibly Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import csv\n",
    "# def prepare_files(index, mode, exp_index, out_dir):\n",
    "#     \"\"\"\n",
    "#     Create a folder named `out_dir`. This really just copies over files from il_train or il_test, as appropriate.\n",
    "#     For instance, if il_train looks like this:\n",
    "    \n",
    "#     il_train\n",
    "#     │   ├── 1\n",
    "#     │   │   ├── ...\n",
    "#     │   │   ├── config.json\n",
    "#     │   │   └── progress.csv\n",
    "#     │   └── _sources\n",
    "#     …\n",
    "    \n",
    "#     Then the ouptut will look like this:\n",
    "#     ├── progress\n",
    "#     │   └── 1\n",
    "#     │       ├── params.json   (same as config.json)\n",
    "#     │       └── progress.csv\n",
    "#     …\n",
    "\n",
    "#     After you run this, you can execute viskit with: python viskit/frontend.py path/to/out_dir/\n",
    "#     \"\"\"\n",
    "#     experiments = index.search(mode=mode)\n",
    "#     # compute merged configs (nested/hierarchical dicts), and\n",
    "#     # also throw out experiments with no progress.csv\n",
    "#     hierarchical_dicts = []\n",
    "#     new_experiments = []\n",
    "#     for experiment in experiments:\n",
    "#         if not experiment.progress_path:\n",
    "#             print(\"Skipping experiment\", experiment.ident, \"because it has no progress.csv\")\n",
    "#             continue\n",
    "#         merged_config = experiment.get_merged_config(exp_index)\n",
    "#         hierarchical_dicts.append(merged_config)\n",
    "#         new_experiments.append(experiment)\n",
    "#     experiments = new_experiments\n",
    "\n",
    "#     # first flatten all dicts\n",
    "#     dicts = [dict(flatten_dict(d)) for d in hierarchical_dicts]\n",
    "    \n",
    "#     # make sure that every dict has every key\n",
    "#     all_keys = set()\n",
    "#     for d in dicts:\n",
    "#         all_keys |= d.keys()\n",
    "#     for d in dicts:\n",
    "#         for new_key in all_keys - d.keys():\n",
    "#             d[new_key] = None\n",
    "    \n",
    "#     # now generate outputs for experiments\n",
    "#     for flat_config, experiment in zip(dicts, experiments):\n",
    "#         exp_out_dir = os.path.join(out_dir, experiment.ident.replace('/', '-'))\n",
    "#         os.makedirs(exp_out_dir, exist_ok=True)\n",
    "\n",
    "#         params_json_path = os.path.join(exp_out_dir, 'params.json')\n",
    "#         with open(params_json_path, 'w') as fp:\n",
    "#             json.dump(flat_config, fp)\n",
    "\n",
    "#         progress_out_path = os.path.join(exp_out_dir, 'progress.csv')\n",
    "#         shutil.copyfile(experiment.progress_path, progress_out_path)\n",
    "        \n",
    "#         if mode == 'il_test':\n",
    "#             eval_json_path = experiment.eval_json_path\n",
    "#             with open(eval_json_path, 'r') as fp:\n",
    "#                 eval_dict = json.load(fp)\n",
    "                \n",
    "#             result_keys, result_vals = [], []\n",
    "#             for key, value in eval_dict.items():\n",
    "# #                 print(eval_dict.keys())\n",
    "#                 if key == 'return_mean' and value < 0.1:\n",
    "#                     print(eval_dict['policy_path'])\n",
    "#                 if isinstance(value, str) or isinstance(value, int) or isinstance(value, float):\n",
    "#                     result_keys.append(key)\n",
    "#                     result_vals.append(value)\n",
    "            \n",
    "#             with open(progress_out_path, mode='w') as result_file:\n",
    "#                 result_writer = csv.writer(result_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "#                 result_writer.writerow(result_keys)\n",
    "#                 result_writer.writerow(result_vals)\n",
    "\n",
    "                \n",
    "# prepare_files(subexp_index, 'repl', subexp_index, 'viskit-repl')\n",
    "# prepare_files(subexp_index, 'il_train', subexp_index, 'viskit-il-train')\n",
    "# prepare_files(subexp_index, 'il_test', subexp_index, 'viskit-il-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import matplotlib\n",
    "# # matplotlib.use(\"tkagg\")\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# from pathlib import Path\n",
    "\n",
    "# # include_ident_keywords = ['control', 'tcpc']\n",
    "# # exclude_ident_keywords = ['mt', 'rand_only']\n",
    "\n",
    "# include_ident_keywords = []\n",
    "# exclude_ident_keywords = []\n",
    "\n",
    "# def get_data(mode, data_type, include_ident_kw=None, exclude_ident_kw=None):\n",
    "#     assert mode in ['repl', 'il_train', 'il_test']\n",
    "#     assert data_type in ['loss', 'return']\n",
    "#     expts = subexp_index.search(mode=mode)\n",
    "#     all_configs = [subexp.get_merged_config(subexp_index) for subexp in expts]\n",
    "#     base_config, flat_configs = simplify_config_dicts(all_configs)\n",
    "#     flat_config_tups = [tuple(sorted(d.items())) for d in flat_configs]\n",
    "#     subexp_by_benchmark = {}\n",
    "#     for flat_cfg, subexp in zip(flat_config_tups, expts):\n",
    "#         bench_key = tuple((k, v) for k, v in flat_cfg if k.startswith('env_') or k.startswith('venv_'))\n",
    "#         subexp_by_benchmark.setdefault(bench_key, []).append((flat_cfg, subexp))\n",
    "    \n",
    "#     \"\"\"\n",
    "#         ret_dict has structure {'env_1': {'exp_ident_1': required_data, 'exp_ident_2': required_data, ...}, ...}\n",
    "#         required_data can be either a list (i.e. loss over time) or a number (int or float, like return_mean)\n",
    "#     \"\"\"\n",
    "#     ret_dict = {}\n",
    "#     for idx, (bench_key, cfgs_subexps) in enumerate(subexp_by_benchmark.items(), start=1):\n",
    "#         # cluster subexperiments by config\n",
    "#         by_cfg = {}\n",
    "#         for tup_cfg, subexp in cfgs_subexps:\n",
    "#             tup_cfg = tuple(k for k in tup_cfg if k not in bench_key)\n",
    "#             by_cfg.setdefault(tup_cfg, []).append(subexp)\n",
    "\n",
    "#         task_name = bench_key[1][1]\n",
    "            \n",
    "#         ret_dict[task_name] = {}\n",
    "#         for cfg, subexp in cfgs_subexps:\n",
    "#             d = dict(cfg)\n",
    "#             exp_ident = d['il_train/exp_ident']\n",
    "        \n",
    "#             if subexp.progress_path:\n",
    "#                 try:\n",
    "#                     df = pd.read_csv(subexp.progress_path)\n",
    "#                 except:\n",
    "#                     print(f'Read csv reported error for exp {exp_ident}, skipping...')\n",
    "#                     continue\n",
    "#                 full_length = 400 if d['env_cfg/benchmark_name'] == 'dm_control' else 40\n",
    "#                 if len(df['loss']) != full_length:\n",
    "#                     print(f'Experiment {exp_ident} only has len(loss) {len(df[\"loss\"])}, skipping... ')\n",
    "#                     continue\n",
    "                \n",
    "#                 ret_dict[task_name][exp_ident] = []\n",
    "#                 if data_type == 'loss':\n",
    "#                     ret_dict[task_name][exp_ident].append(df['loss'])\n",
    "                    \n",
    "#     print(ret_dict.keys())\n",
    "#     return ret_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def plot_curves(data_dict):\n",
    "#     sns.set(rc={'figure.figsize':(7, 6)})\n",
    "#     for task_key, exp_results in data_dict.items():\n",
    "#         df = None\n",
    "#         col_name = []\n",
    "#         plt.figure()\n",
    "#         for exp_ident, value, in exp_results.items():\n",
    "#             col_name = [f\"seed_{x}\" for x in range(len(value))]\n",
    "#             col_name += ['step', 'exp_ident']\n",
    "#             value.append([s for s in range(1, len(value[0])+1)])\n",
    "#             value.append([exp_ident for s in range(1, len(value[0])+1)])\n",
    "#             value = np.array(value).transpose(1, 0)\n",
    "#             sub_df = pd.DataFrame(data=value, columns=col_name)\n",
    "#             df = pd.concat([df, sub_df])\n",
    "#         df = pd.melt(df, id_vars=['step', 'exp_ident'])\n",
    "#         df['step'] = pd.to_numeric(df['step'])\n",
    "#         df['value'] = pd.to_numeric(df['value'])\n",
    "#         print(df.dtypes)\n",
    "        \n",
    "#         ax = sns.lineplot(x='step', y='value', hue='exp_ident', data=df)\n",
    "#         plt.setp(ax.get_legend().get_texts(), fontsize='12')\n",
    "#         plt.legend(bbox_to_anchor=(1.01, 1),borderaxespad=0)\n",
    "#         ax.set_title(bench_key)\n",
    "    \n",
    "# plot_curves(get_data('il_train', \n",
    "#                      'loss', \n",
    "#                      include_ident_kw=include_ident_keywords,\n",
    "#                      exclude_ident_kw=exclude_ident_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Interpret encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Interpret encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Save the encoder interpretation videos. Each sub_exp might take one or two minutes to save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import subprocess\n",
    "# from pathlib import Path\n",
    "\n",
    "# train_expts = subexp_index.search(mode='il_test')\n",
    "# all_configs = [subexp.get_merged_config(subexp_index) for subexp in train_expts]\n",
    "# base_config, flat_configs = simplify_config_dicts(all_configs)\n",
    "# flat_config_tups = [tuple(sorted(d.items())) for d in flat_configs]\n",
    "# subexp_by_benchmark = {}\n",
    "# for flat_cfg, subexp in zip(flat_config_tups, train_expts):\n",
    "#     bench_key = tuple((k, v) for k, v in flat_cfg if k.startswith('env_') or k.startswith('venv_'))\n",
    "#     subexp_by_benchmark.setdefault(bench_key, []).append((flat_cfg, subexp))\n",
    "\n",
    "# # Create a folder to save videos\n",
    "# Path(f\"./runs/{cluster_subpath.split('/')[1]}\").mkdir(parents=True, exist_ok=True)\n",
    "# interp_algo = 'saliency'\n",
    "                    \n",
    "# for idx, (bench_key, cfgs_subexps) in enumerate(subexp_by_benchmark.items(), start=1):\n",
    "#     # cluster subexperiments by config\n",
    "#     by_cfg = {}\n",
    "#     for tup_cfg, subexp in cfgs_subexps:\n",
    "#         tup_cfg = tuple(k for k in tup_cfg if k not in bench_key)\n",
    "#         by_cfg.setdefault(tup_cfg, []).append(subexp)\n",
    "\n",
    "#     for tup_cfg, subexp in by_cfg.items():\n",
    "#         exp = subexp[0]\n",
    "#         encoder_path = exp.config['encoder_path']\n",
    "#         if encoder_path:\n",
    "#             for prefix, replacement in path_translations.items():\n",
    "#                 if encoder_path.startswith(prefix):\n",
    "#                     encoder_path = replacement + encoder_path[len(prefix):]\n",
    "#             command = \"python ../src/il_representations/scripts/interpret.py with \"\n",
    "#             command += f\"log_dir=runs/{cluster_subpath.split('/')[1]} \"\n",
    "#             command += f\"env_cfg.benchmark_name={exp.config['env_cfg']['benchmark_name']} \"\n",
    "#             command += f\"env_cfg.task_name={exp.config['env_cfg']['task_name']} \"\n",
    "#             command += f\"save_video=True \"\n",
    "#             command += f\"chosen_algo={interp_algo} \"\n",
    "#             command += f\"encoder_path={encoder_path} \"\n",
    "#             command += f\"filename={exp.config['env_cfg']['task_name']}_{exp.config['exp_ident']} \"\n",
    "\n",
    "#             print(f\"Generating videos for exp {exp.config['exp_ident']} on {exp.config['env_cfg']['task_name']}...\")\n",
    "#             process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n",
    "#             output, error = process.communicate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
