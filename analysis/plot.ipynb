{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import copy\n",
    "import html\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import json\n",
    "import glob\n",
    "from functools import partial\n",
    "from numpy import trapz\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "import seaborn as sns \n",
    "from math import sqrt \n",
    "import pandas as pd \n",
    "import math\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "# FIXME(sam): make cluster subpath selectable with a dropdown. Also, automatically infer\n",
    "# gfs_mount from the hostname (have sensible default for svm/perceptron)\n",
    "\n",
    "# this identifies data for a particular cluster on the GFS volume\n",
    "# cluster_subpath = \"with-resnet-128-2020-01-27/\"\n",
    "cluster_subpath = \"cluster-data/cluster-2021-01-29-set3-try4/\"\n",
    "# on svm I think gfs_mount is /scratch/sam/repl-vol/ or something like that\n",
    "# gfs_mount = \"/scratch/cynthiachen/\"\n",
    "gfs_mount = \"/scratch/sam/il-representations-gcp-volume/\"  # Google Filestore mount point (local)\n",
    "runs_directory = os.path.join(gfs_mount, cluster_subpath)\n",
    "path_translations = {\n",
    "    # when loading things like 'encoder_path' and 'policy_path' from configs,\n",
    "    # replace the thing on the left with the thing on the right\n",
    "    \"/data/il-representations/\": gfs_mount,\n",
    "    \"/root/il-rep/runs/\": os.path.join(gfs_mount, cluster_subpath),\n",
    "    \"/home/sam/repos/il-representations/cloud/runs/\": os.path.join(gfs_mount, cluster_subpath)\n",
    "}\n",
    "\n",
    "exp_dir = os.path.join(runs_directory, 'chain_runs')\n",
    "assert os.path.exists(exp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a table of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_parent_relpath(sample_parent_file, local_root_dir):\n",
    "    \"\"\"Get root-relative path to a 'parent' directory, such as the directory\n",
    "    containing a saved encoder or policy. This is somewhat tricky because\n",
    "    we need to replace paths that might be different on svm/perceptron or on\n",
    "    a laptop compared to what they were on GCP. e.g. inside the Ray docker\n",
    "    container, '/root/il-rep/runs' maps to 'cluster-data/' in the GFS volume.\n",
    "    The `path_translations` variable handles all the necessary changes.\"\"\"\n",
    "    for prefix, replacement in path_translations.items():\n",
    "        if sample_parent_file.startswith(prefix):\n",
    "            sample_parent_file = replacement + sample_parent_file[len(prefix):]\n",
    "\n",
    "    full_path = os.path.abspath(sample_parent_file)\n",
    "    full_dir = os.path.dirname(full_path)\n",
    "    rel_dir = os.path.relpath(full_dir, local_root_dir)\n",
    "\n",
    "    return rel_dir\n",
    "\n",
    "class SubexperimentRun:\n",
    "    \"\"\"A SubexperimentRun associates all the information associated\n",
    "    with a run of a particular Sacred sub-experiment. That means a\n",
    "    run (single execution) of the 'repl', 'il_train', or 'il_test'\n",
    "    experiments.\"\"\"\n",
    "    def __init__(self, subexp_dir, experiment_dir_root):\n",
    "        # Subexperiment dir is used as a unique identifier.\n",
    "        # We strip out the leading 'experiment_dir_root' to shorten identifiers.\n",
    "        subexp_dir = os.path.abspath(subexp_dir)\n",
    "        experiment_dir_root = os.path.abspath(experiment_dir_root)\n",
    "        self.ident = os.path.relpath(subexp_dir, experiment_dir_root)\n",
    "        self.subexp_dir = subexp_dir\n",
    "        self.experiment_dir_root = experiment_dir_root\n",
    "\n",
    "        # usually paths are like, e.g., 'chain_runs/repl/42' or\n",
    "        # 'chain_runs/il_train/13'; if we take the second last component,\n",
    "        # we should get the mode\n",
    "        self.mode = os.path.split(os.path.split(subexp_dir)[0])[1]\n",
    "        assert self.mode in {'repl', 'il_train', 'il_test'}, (mode, subexp_dir)\n",
    "\n",
    "        # Load experiment config\n",
    "        config_path = os.path.join(subexp_dir, 'config.json')\n",
    "        with open(config_path, 'r') as fp:\n",
    "            self.config = json.load(fp)\n",
    "\n",
    "        # Store a path to relevant progress.csv file (only for il_train/repl)\n",
    "        progress_path = os.path.join(subexp_dir, \"progress.csv\")\n",
    "        if os.path.exists(progress_path):\n",
    "            self.progress_path = progress_path\n",
    "        else:\n",
    "            self.progress_path = None\n",
    "\n",
    "        # Store a path to relevant eval.json file\n",
    "        eval_json_path = os.path.join(subexp_dir, \"eval.json\")\n",
    "        if os.path.exists(eval_json_path):\n",
    "            self.eval_json_path = eval_json_path\n",
    "        else:\n",
    "            self.eval_json_path = None\n",
    "\n",
    "        # Infer the .ident attribute for the parent experiment\n",
    "        # (if it exists)\n",
    "        if self.mode == 'il_train' and self.config.get('encoder_path') is not None:\n",
    "            encoder_relpath = get_parent_relpath(\n",
    "                self.config['encoder_path'], experiment_dir_root)\n",
    "            # The relpath is going to be something like\n",
    "            # \"chain_runs/10/repl/5/checkpoints/representation_encoder\".\n",
    "            # We heuristically remove the last two parts.\n",
    "            # (this definitely breaks on Windows…)\n",
    "            encoder_relpath = '/'.join(encoder_relpath.split('/')[:-2])\n",
    "            self.parent_ident = encoder_relpath\n",
    "        elif self.mode == 'il_test':\n",
    "            policy_relpath = get_parent_relpath(\n",
    "                self.config['policy_path'], experiment_dir_root)\n",
    "            self.parent_ident = policy_relpath\n",
    "        else:\n",
    "            # \"repl\" runs and \"il_train\" runs without an encoder_path\n",
    "            # have no parents\n",
    "            assert self.mode == 'repl' \\\n",
    "              or (self.mode == 'il_train' and self.config.get('encoder_path') is None), \\\n",
    "               (self.mode, self.config.get('encoder_path'))\n",
    "            self.parent_ident = None\n",
    "            \n",
    "        # HACK: adding a use_repl key so that we can see whether il_train runs used repL\n",
    "        if self.mode == 'il_train':\n",
    "            self.config['use_repl'] = self.parent_ident is not None\n",
    "\n",
    "    def get_merged_config(self, index):\n",
    "        \"\"\"Get a 'merged' config dictionary for this subexperiment and\n",
    "        all of its parents. The dict will have a format like this:\n",
    "        \n",
    "        {\"benchmark\": {…}, \"il_train\": {…}, \"il_test\": {…}, \"repl\": {…}}\n",
    "        \n",
    "        Note that some keys might not be present (e.g. if this is a `repl` run,\n",
    "        it will not have the `il_train` key; if this is an `il_train` run with\n",
    "        no parent, then the `repl` key will be absent).\"\"\"\n",
    "        config = {self.mode: dict(self.config)}\n",
    "        extract_keys = ('env_cfg', 'venv_opts', 'env_data')\n",
    "        for extract_key in extract_keys:\n",
    "            if extract_key in config[self.mode]:\n",
    "                # move 'benchmark' key to the top because that ingredient name is\n",
    "                # shared between il_train, and il_test experiments\n",
    "                config[extract_key] = config[self.mode][extract_key]\n",
    "                del config[self.mode][extract_key]\n",
    "        parent = self.get_parent(index)\n",
    "        if parent is not None:\n",
    "            # TODO: merge this properly, erroring on incompatible duplicate\n",
    "            # keys. I think Cody has code for this.\n",
    "            config.update(parent.get_merged_config(index))\n",
    "        return config\n",
    "    \n",
    "    def get_parent(self, index):\n",
    "        if self.parent_ident is None:\n",
    "            return None\n",
    "        return index.get_subexp(self.parent_ident)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.ident)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, SubexperimentRun):\n",
    "            return NotImplemented\n",
    "        return self.ident == other.ident\n",
    "\n",
    "class SubexperimentIndex:\n",
    "    \"\"\"An index of subexperiments. For now this just supports\n",
    "    looking up experiments by identifier. Later it might support\n",
    "    lookup by attributes.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.subexp_by_ident = {}\n",
    "        \n",
    "    def add_subexp(self, subexp):\n",
    "        if subexp.ident in self.subexp_by_ident:\n",
    "            raise ValueError(\"duplicate subexperiment:\", subexp)\n",
    "        self.subexp_by_ident[subexp.ident] = subexp\n",
    "        \n",
    "    def get_subexp(self, ident):\n",
    "        return self.subexp_by_ident[ident]\n",
    "    \n",
    "    def search(self, **attrs):\n",
    "        \"\"\"Find a subexperiment with attributes matching the values\n",
    "        given in 'attrs'.\"\"\"\n",
    "        results = []\n",
    "        for subexp in self.subexp_by_ident.values():\n",
    "            for k, v in attrs.items():\n",
    "                if getattr(subexp, k) != v:\n",
    "                    break\n",
    "            else:\n",
    "                results.append(subexp)\n",
    "        return results\n",
    "\n",
    "def get_experiment_directories(root_dir, skip_skopt=True):\n",
    "    \"\"\"Look for directories that end in a sequence of numbers, and contain a\n",
    "    grid_search subdirectory.\"\"\"\n",
    "    expt_pat = re.compile(r'^.*/(il_test|il_train|repl)/\\d+$')\n",
    "    ignore_pat = re.compile(r'^.*/(grid_search|_sources)$')  # ignore the grid_search subdir\n",
    "    expt_dirs = set()\n",
    "    for root, dirs, files in os.walk(root_dir, followlinks=True, topdown=True):\n",
    "        if ignore_pat.match(root):\n",
    "            del dirs[:]\n",
    "            continue\n",
    "            \n",
    "        # check whether tihs is a skopt dir\n",
    "        if skip_skopt and 'grid_search' in dirs:\n",
    "            gs_files = os.listdir(os.path.join(root, 'grid_search'))\n",
    "            if any(s.startswith('search-alg-') for s in gs_files):\n",
    "                # this is a skopt dir, skip it\n",
    "                print(\"skipping skopt directory in\", root)\n",
    "                del dirs[:]\n",
    "                continue\n",
    "\n",
    "        found_match = False\n",
    "        for d in dirs:\n",
    "            d_path = os.path.abspath(os.path.join(root, d))\n",
    "            m = expt_pat.match(d_path)\n",
    "            if m is None:\n",
    "                continue  # no match\n",
    "            expt_dirs.add(d_path)\n",
    "            found_match = True\n",
    "\n",
    "        if found_match:\n",
    "            del dirs[:]  # don't recurse\n",
    "    return sorted(expt_dirs)\n",
    "\n",
    "# Find all experiment directories (i.e. directories containing a grid_search\n",
    "# subdir)\n",
    "def load_all_subexperiments(root_dir, skip_skopt=True):\n",
    "    \"\"\"Find all experiment run subdirectories, and create SubexperimentIndex objects for them.\"\"\"\n",
    "    print(\"Searching for experiment directories (might take a minute or two)\")\n",
    "    all_expt_directories = get_experiment_directories(root_dir, skip_skopt=skip_skopt)\n",
    "    print(\"Loading experiments (might take another minute or two)\")\n",
    "    index = SubexperimentIndex()\n",
    "    for expt_dir in all_expt_directories:\n",
    "        subexp = SubexperimentRun(expt_dir, root_dir)\n",
    "        index.add_subexp(subexp)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subexp_index = load_all_subexperiments(runs_directory, skip_skopt=True)\n",
    "print('Discovered', len(subexp_index.subexp_by_ident), 'subexperiments')\n",
    "\n",
    "test_expts = subexp_index.search(mode='il_test')\n",
    "\n",
    "test_expts[1].get_merged_config(subexp_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print a table of il_test results\n",
    "\n",
    "Shows a separate set of il_test results for each benchmark setting, and also puts that data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def flatten_dict(d):\n",
    "    \"\"\"Flatten a nested dict into a single-level dict with\n",
    "    'keys/separated/like/this'.\"\"\"\n",
    "    out_dict = {}\n",
    "    if isinstance(d, dict):\n",
    "        key_iter = d.items()\n",
    "    else:\n",
    "        assert isinstance(d, list), type(d)\n",
    "        # we flatten lists into dicts of the form {0: <first elem>, 1: <second elem>, …}\n",
    "        key_iter = ((str(idx), v) for idx, v in enumerate(d))\n",
    "    for key, value in key_iter:\n",
    "        if isinstance(value, (dict, list)):\n",
    "            value = flatten_dict(value)\n",
    "            for subkey, subvalue in value.items():\n",
    "                out_dict[f'{key}/{subkey}'] = subvalue\n",
    "        else:\n",
    "            out_dict[key] = value\n",
    "    return out_dict\n",
    "\n",
    "def combine_dicts_multiset(dicts):\n",
    "    \"\"\"Combine a series of dicts into a key-multiset mapping, where the\n",
    "    multiset measures how many times each observed value occurs for each\n",
    "    key.\"\"\"\n",
    "    count_dict = {}\n",
    "    for d in dicts:\n",
    "        for k, v in d.items():\n",
    "            if k not in count_dict:\n",
    "                count_dict[k] = collections.Counter()\n",
    "            count_dict[k][v] += 1\n",
    "    return count_dict\n",
    "\n",
    "def remove_inapplicable_keys(flat_dict):\n",
    "    \"\"\"Remove keys that do not make a difference from a flattened config dicts.\n",
    "    Totally heuristic, so might have to add more options to this later on.\"\"\"\n",
    "    remove_keys = set()\n",
    "    \n",
    "    # remove inapplicable benchmark keys\n",
    "    for benchmark_name in ['magical', 'dm_control']:\n",
    "        if flat_dict.get('env_cfg/benchmark_name') != benchmark_name:\n",
    "            for key in flat_dict:\n",
    "                # this will remove, e.g., dm_control keys from magical experiments\n",
    "                if key.startswith('env_cfg/' + benchmark_name) or key.startswith('env_data/' + benchmark_name):\n",
    "                    remove_keys.add(key)\n",
    "                    \n",
    "    # remove repl keys from things that don't use repL\n",
    "    if flat_dict.get('il_train/use_repl') is False:\n",
    "        for key in flat_dict:\n",
    "            if key.startswith('repl/'):\n",
    "                remove_keys.add(key)\n",
    "                    \n",
    "    return {k: v for k, v in flat_dict.items() if k not in remove_keys}\n",
    "\n",
    "def simplify_config_dicts(hierarchical_dicts,\n",
    "                          base_thresh=0.75,\n",
    "                          remove_seeds=True,\n",
    "                          prohibited_base_keys=('env_cfg/task_name', 'env_cfg/benchmark_name', ),\n",
    "                          force_remove_keys=('il_test/policy_path', 'il_train/encoder_path')):\n",
    "    \"\"\"Simplify flattened config dicts so that:\n",
    "    \n",
    "    0. They are totally flat.\n",
    "    1. They only contain keys for which values actually differ between\n",
    "       different dicts, and\n",
    "    2. If the value of some key is the same for at least a fraction\n",
    "       `base_thresh` of dicts, then that key is moved into a _base config_.\n",
    "       Returned dicts will only contain that key if they have a different\n",
    "       value from the base config one.\n",
    "    3. Optionally, remove all seed values from dicts.\n",
    "\n",
    "    This makes it more clear which values are actually changing.\"\"\"\n",
    "    # first flatten all dicts\n",
    "    dicts = [dict(flatten_dict(d)) for d in hierarchical_dicts]\n",
    "    \n",
    "    # remove seeds, if required\n",
    "    if remove_seeds:\n",
    "        for d in dicts:\n",
    "            for key in list(d.keys()):\n",
    "                if key.split('/')[-1] == 'seed':\n",
    "                    del d[key]\n",
    "                    \n",
    "    # make sure that every dict has every key\n",
    "    all_keys = set()\n",
    "    for d in dicts:\n",
    "        all_keys |= d.keys()\n",
    "    for d in dicts:\n",
    "        for new_key in all_keys - d.keys():\n",
    "            d[new_key] = None\n",
    "                    \n",
    "    # remove inapplicable keys\n",
    "    dicts = [remove_inapplicable_keys(d) for d in dicts]\n",
    "\n",
    "    # now figure out which keys we wish to remove or move to the base config\n",
    "    base_config = {}\n",
    "    remove_keys = set()\n",
    "    base_thresh_abs = len(dicts) * base_thresh\n",
    "    count_dict = combine_dicts_multiset(dicts)\n",
    "    for key, counter in count_dict.items():\n",
    "        if len(counter) == 1 or key in force_remove_keys:\n",
    "            # if all dicts have the same value for this key, we will\n",
    "            # remove it from output dicts\n",
    "            remove_keys.add(key)\n",
    "        elif key not in prohibited_base_keys:\n",
    "            # if most dicts have the same value for this key, then\n",
    "            # we add it to the base config\n",
    "            (max_count_item, max_count), = counter.most_common(1)\n",
    "            if max_count > base_thresh_abs:\n",
    "                base_config[key] = max_count_item\n",
    "\n",
    "    # remove keys that we are ignoring, or for which the corresponding value\n",
    "    # already exists in the base config\n",
    "    new_dicts = []\n",
    "    for old_dict in dicts:\n",
    "        new_dict = {}\n",
    "        for key, value in old_dict.items():\n",
    "            if key in remove_keys \\\n",
    "              or (key in base_config and base_config[key] == value):\n",
    "                continue  # skip this key\n",
    "            new_dict[key] = value\n",
    "        new_dicts.append(new_dict)\n",
    "    \n",
    "    return base_config, new_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the below code can take 2-3m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_configs = [subexp.get_merged_config(subexp_index) for subexp in test_expts]\n",
    "base_config, flat_configs = simplify_config_dicts(all_configs)\n",
    "flat_config_tups = [tuple(sorted(d.items())) for d in flat_configs]\n",
    "subexp_by_benchmark = {}\n",
    "for flat_cfg, subexp in zip(flat_config_tups, test_expts):\n",
    "    bench_key = tuple((k, v) for k, v in flat_cfg if k.startswith('env_') or k.startswith('venv_'))\n",
    "    subexp_by_benchmark.setdefault(bench_key, []).append((flat_cfg, subexp))\n",
    "\n",
    "display(HTML('<p><strong>Base config</strong></p>'))\n",
    "display(HTML('<p>Unless specified otherwise, all config dicts include these keys:</p>'))\n",
    "print(base_config)\n",
    "\n",
    "raw_return_data = {}\n",
    "    \n",
    "for idx, (bench_key, cfgs_subexps) in enumerate(subexp_by_benchmark.items(), start=1):\n",
    "    # print out benchmark details\n",
    "    display(HTML(f'<p><strong>Results for benchmark config &#35;{idx}</strong></p>'))\n",
    "    display(HTML(f'<p>Config:</p>'))\n",
    "    rows = [f'<tr><th>{html.escape(key)}</th><td>{html.escape(value)}</td></tr>' for key, value in bench_key]\n",
    "    display(HTML(f'<table>{\"\".join(rows)}</table>'))\n",
    "    display(HTML(f'<p>Runs:</p>'))\n",
    "    \n",
    "    # cluster subexperiments by config\n",
    "    by_cfg = {}\n",
    "    for tup_cfg, subexp in cfgs_subexps:\n",
    "        tup_cfg = tuple(k for k in tup_cfg if k not in bench_key)\n",
    "        by_cfg.setdefault(tup_cfg, []).append(subexp)\n",
    "        \n",
    "\n",
    "    # load all eval.json files and figure out what columns we need\n",
    "    stats_dicts = {}\n",
    "    columns = set()\n",
    "    for _, subexp in cfgs_subexps:\n",
    "        if subexp.eval_json_path:\n",
    "            with open(subexp.eval_json_path, 'r') as fp:\n",
    "                eval_dict = json.load(fp)\n",
    "            # is this a magical run?\n",
    "            is_magical = 'full_data' in eval_dict.keys()\n",
    "            if is_magical:\n",
    "                stats_dict = {\n",
    "                    '-'.join(env_dict['test_env'].split('-')[:2]): env_dict['mean_score']\n",
    "                    for env_dict in eval_dict['full_data']\n",
    "                }\n",
    "                stats_dict['Average on all envs'] = eval_dict['return_mean']\n",
    "            else:\n",
    "                stats_dict = {'return_mean': eval_dict['return_mean']}\n",
    "            stats_dicts[subexp] = stats_dict\n",
    "            columns |= stats_dict.keys()\n",
    "        else:\n",
    "            stats_dicts[subexp] = {}\n",
    "    columns = sorted(columns)\n",
    "    \n",
    "    # now produce a table with one row per config\n",
    "    table_parts = ['<table>']                                         # begin table\n",
    "    table_parts.append('<tr>')                                        # begin header row\n",
    "    table_parts.append('<th style=\"border-collapse: collapse;\">Config</th>')\n",
    "    table_parts.extend(f'<th style=\"border-collapse: collapse;\">{html.escape(col_name)}</th>' for col_name in columns)\n",
    "    table_parts.append('</tr>')                                       # end header row\n",
    "\n",
    "    row_indexes = []\n",
    "    rows = []\n",
    "    \n",
    "    for cfg, subexps in sorted(by_cfg.items(), key=lambda cfg_se: dict(cfg_se[0])['il_test/exp_ident']):\n",
    "        table_parts.append('<tr>')                                    # begin row\n",
    "\n",
    "        # cell containing config\n",
    "        if True:  # remove to show full config\n",
    "            d = dict(cfg)\n",
    "            exp_ident = d['il_test/exp_ident']\n",
    "            # bench_name = d['env_cfg/benchmark_name']\n",
    "            # task_name = d['env_cfg/task_name']\n",
    "            # desc_str = f'{exp_ident} ({bench_name}/{task_name})'\n",
    "            table_parts.append(f'<td style=\"border-collapse: collapse;\">{html.escape(exp_ident)}</td>')\n",
    "        else:\n",
    "            kv_cfg = ', '.join(f'{key}={value!r}' for key, value in cfg)\n",
    "            table_parts.append(f'<td style=\"max-width: 600px; border-collapse: collapse;\">{html.escape(kv_cfg)}</td>')\n",
    "        row_indexes.append(exp_ident)  \n",
    "        # cells containing data\n",
    "        row_values = []\n",
    "        for column in columns:\n",
    "            column_values = [stats_dicts[subexp][column] for subexp in subexps\n",
    "                             if column in stats_dicts[subexp]]\n",
    "            if not column_values:\n",
    "                table_parts.append('<td style=\"border-collapse: collapse;\">-</td>')\n",
    "                row_values.append({'mean': np.nan, 'std': np.nan, 'n': np.nan})\n",
    "            else:\n",
    "                mean = np.mean(column_values)\n",
    "                std = np.std(column_values)\n",
    "                n = len(column_values)\n",
    "                table_parts.append(f'<td style=\"border-collapse: collapse;\">{mean:.3g}±{std:.1g} ({n})</td>')\n",
    "                row_values.append({'mean': mean, 'std': std, 'n': n})\n",
    "        rows.append(row_values)\n",
    "        # cells containing values\n",
    "\n",
    "        table_parts.append('</tr>')                                   # end row\n",
    "    raw_return_data[bench_key[1][1]] = {'row_indexes': row_indexes, 'rows': rows, 'columns': columns}\n",
    "    table_parts.append('</table>')                                    # end table\n",
    "    display(HTML(''.join(table_parts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "raw_return_dataframes = dict()                                                                                                                                                                                                                                                                                                                                                            \n",
    "\n",
    "for k in raw_return_data.keys():\n",
    "    print(f\"Processing task {k}\")\n",
    "    raw_return_dataframes[k] = dict()\n",
    "    for stat in ['mean', 'std', 'n']:\n",
    "        columns = raw_return_data[k]['columns']\n",
    "        index = raw_return_data[k]['row_indexes']\n",
    "        rows = []\n",
    "        for raw_row in raw_return_data[k]['rows']: \n",
    "            rows.append([el[stat] for el in raw_row])\n",
    "        try: \n",
    "            raw_return_dataframes[k][stat] = pd.DataFrame(rows, index=index, columns=columns)\n",
    "        except: \n",
    "            import pdb; pdb.set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print + DF-ize Raw AUC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How many splits?\n",
    "num_split = 6\n",
    "\n",
    "# Exclude first n values in the loss list?\n",
    "start_count = 3\n",
    "\n",
    "def calculate_auc(y, dx=1):\n",
    "    return trapz(y, dx=dx)\n",
    "    \n",
    "train_expts = subexp_index.search(mode='il_train')\n",
    "all_configs = [subexp.get_merged_config(subexp_index) for subexp in train_expts]\n",
    "base_config, flat_configs = simplify_config_dicts(all_configs)\n",
    "flat_config_tups = [tuple(sorted(d.items())) for d in flat_configs]\n",
    "subexp_by_benchmark = {}\n",
    "for flat_cfg, subexp in zip(flat_config_tups, train_expts):\n",
    "    bench_key = tuple((k, v) for k, v in flat_cfg if k.startswith('env_') or k.startswith('venv_'))\n",
    "    subexp_by_benchmark.setdefault(bench_key, []).append((flat_cfg, subexp))\n",
    "\n",
    "display(HTML('<p><strong>Base config</strong></p>'))\n",
    "display(HTML('<p>Unless specified otherwise, all config dicts include these keys:</p>'))\n",
    "print(base_config)\n",
    "    \n",
    "raw_auc_data = {}\n",
    "\n",
    "for idx, (bench_key, cfgs_subexps) in enumerate(subexp_by_benchmark.items(), start=1):\n",
    "    # print out benchmark details\n",
    "    task_name = dict(bench_key)['env_cfg/task_name']\n",
    "    \n",
    "    # cluster subexperiments by config\n",
    "    by_cfg = {}\n",
    "    for tup_cfg, subexp in cfgs_subexps:\n",
    "        tup_cfg = tuple(k for k in tup_cfg if k not in bench_key)\n",
    "        by_cfg.setdefault(tup_cfg, []).append(subexp)\n",
    "\n",
    "    # load all progress files and figure out what columns we need\n",
    "    stats_dicts = {}\n",
    "    columns = set()\n",
    "    for cfg, subexp in cfgs_subexps:\n",
    "        d = dict(cfg)\n",
    "        exp_ident = d['il_train/exp_ident']\n",
    "        \n",
    "        if subexp.progress_path:\n",
    "            try:\n",
    "                df = pd.read_csv(subexp.progress_path)\n",
    "            except:\n",
    "                print(f'Read csv reported error for exp {exp_ident}, skipping...')\n",
    "                continue\n",
    "            full_length = 400 if d['env_cfg/benchmark_name'] == 'dm_control' else 40\n",
    "            if len(df['loss']) != full_length:\n",
    "                print(f'Experiment {exp_ident} only has len(loss) {len(df[\"loss\"])}, skipping... ')\n",
    "                continue\n",
    "                \n",
    "            step_length = len(df['loss']) // num_split\n",
    "            stats_dict = {}\n",
    "            if len(df['loss']) < 10: \n",
    "                stats_dicts[subexp] = {}\n",
    "                continue \n",
    "                \n",
    "            for step in range(step_length, len(df['loss']), step_length):\n",
    "                label = f\"step {step:02d}\"\n",
    "                stats_dict[label] = calculate_auc(df['loss'][start_count:step])\n",
    "                \n",
    "            stats_dict[f\"step {len(df['loss'])}\"] = calculate_auc(df['loss'][start_count:len(df['loss'])])\n",
    "            stats_dicts[subexp] = stats_dict\n",
    "            columns |= stats_dict.keys()\n",
    "        else:\n",
    "            stats_dicts[subexp] = {}\n",
    "    columns = sorted(columns)\n",
    "    \n",
    "    raw_auc_data[task_name] = dict()\n",
    "    raw_auc_data[task_name]['columns'] = columns\n",
    "    \n",
    "    row_indexes = []\n",
    "    rows = []\n",
    "    for cfg, subexps in sorted(by_cfg.items(), key=lambda cfg_se: dict(cfg_se[0])['il_train/exp_ident']):\n",
    "        table_parts.append('<tr>')                                    # begin row\n",
    "\n",
    "        # cell containing config\n",
    "        if True:  # remove to show full config\n",
    "            d = dict(cfg)\n",
    "            exp_ident = d['il_train/exp_ident']\n",
    "            # bench_name = d['env_cfg/benchmark_name']\n",
    "            # task_name = d['env_cfg/task_name']\n",
    "            # desc_str = f'{exp_ident} ({bench_name}/{task_name})'\n",
    "            table_parts.append(f'<td style=\"border-collapse: collapse;\">{html.escape(exp_ident)}</td>')\n",
    "        else:\n",
    "            kv_cfg = ', '.join(f'{key}={value!r}' for key, value in cfg)\n",
    "            table_parts.append(f'<td style=\"max-width: 600px; border-collapse: collapse;\">{html.escape(kv_cfg)}</td>')\n",
    "            \n",
    "        # cells containing data\n",
    "        row_indexes.append(exp_ident)\n",
    "        row = []\n",
    "        for column in columns:\n",
    "            column_values = [stats_dicts[subexp][column] for subexp in subexps\n",
    "                             if subexp in stats_dicts.keys() and column in stats_dicts[subexp]]\n",
    "            if not column_values:\n",
    "                table_parts.append('<td style=\"border-collapse: collapse;\">-</td>')\n",
    "                row.append({'mean': np.nan, 'std': np.nan, 'n': np.nan})\n",
    "            else:\n",
    "                mean = np.mean(column_values)\n",
    "                std = np.std(column_values)\n",
    "                n = len(column_values)\n",
    "                table_parts.append(f'<td style=\"border-collapse: collapse;\">{mean:.3g}±{std:.1g} ({n})</td>')\n",
    "                row.append({'mean': mean, 'std': std, 'n': n})\n",
    "        rows.append(row)\n",
    "        # cells containing values\n",
    "\n",
    "        table_parts.append('</tr>')                                   # end row\n",
    "    raw_auc_data[task_name]['row_indexes'] = row_indexes\n",
    "    raw_auc_data[task_name]['rows'] = rows\n",
    "    table_parts.append('</table>')                                    # end table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "raw_auc_dataframes = dict()\n",
    "\n",
    "for k in raw_auc_data.keys(): \n",
    "    print(k)\n",
    "    raw_auc_dataframes[k] = dict()\n",
    "    for stat in ['mean', 'std', 'n']:\n",
    "        columns = raw_auc_data[k]['columns']\n",
    "        index = raw_auc_data[k]['row_indexes']\n",
    "        rows = []\n",
    "        for raw_row in raw_auc_data[k]['rows']: \n",
    "            rows.append([el[stat] for el in raw_row])\n",
    "\n",
    "        raw_auc_dataframes[k][stat] = pd.DataFrame(rows, index=index, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface Check! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for task in raw_auc_dataframes.keys(): \n",
    "    for stat in raw_auc_dataframes[task].keys(): \n",
    "        raw_auc_dataframes[task][stat].to_csv(f\"cached_dfs/raw_auc_dataframes_{task}_{stat}.csv\")\n",
    "\n",
    "for task in raw_return_dataframes.keys(): \n",
    "    for stat in raw_return_dataframes[task].keys(): \n",
    "        raw_return_dataframes[task][stat].to_csv(f\"cached_dfs/raw_return_dataframes_{task}_{stat}.csv\")\n",
    "\n",
    "print(f\"These dataframes were last saved as a failsafe cache at {datetime.now()} PST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the raw data is stored in dataframes: raw_auc_dataframes and raw_return_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_index_val(index, check_algo=False, check_data=False): \n",
    "    new_index = index \n",
    "    for lookup_key, lookup_val in merge_lookups.items(): \n",
    "        new_index = new_index.replace(lookup_key, lookup_val)\n",
    "    return new_index \n",
    "\n",
    "def order_idx(indexes, algo_order):\n",
    "    for idx in indexes:\n",
    "        if idx in algo_order:\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"{idx} not arranged in algo_order, append to the list.\")\n",
    "            algo_order.append(idx)\n",
    "    return algo_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def control_not_key(col, control_key): \n",
    "    if 'control' in col: \n",
    "        if col == control_key: \n",
    "            return True \n",
    "        else: \n",
    "            return False \n",
    "    else: \n",
    "        return True \n",
    "    \n",
    "def should_keep_exp_ident(ident): \n",
    "    if 'froco' in ident:\n",
    "        return False \n",
    "    if 'no_ortho' in ident and 'actual' not in ident: \n",
    "        return False \n",
    "    return True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def check_algo_or_data(index, lookup_dict, check_algo=False, check_data=False, verbose=True, blacklist=()):\n",
    "    new_index = index \n",
    "    assert sum([check_algo, check_data]) % 2 != 0, 'One of check_algo and check_data must be True'\n",
    "    inner_lookup_key = 'algo_lookups' if check_algo else 'data_lookups'\n",
    "    inner_lookup_dict = lookup_dict[inner_lookup_key]\n",
    "    check_type = 'data' if check_data else 'algo'\n",
    "    for term in blacklist: \n",
    "        if term in index: \n",
    "            if verbose: \n",
    "                print(f\"Run {index} skipped due to blacklist term {term}\")\n",
    "            return None \n",
    "    for lookup_key, lookup_val in inner_lookup_dict.items(): \n",
    "        if lookup_key in new_index:\n",
    "            new_index = lookup_val\n",
    "            return new_index\n",
    "    if verbose:\n",
    "        print(f\"Didn't find any {check_type} entries for {index}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_pivoted_dfs(base_df_dict, ret_col_lookup, blacklist_terms, \n",
    "                      whitelist_lookups, control_idx, algo_order, verbose=True): \n",
    "    data_dataframes = dict()\n",
    "    for task in base_df_dict.keys():\n",
    "        data_dataframes[task] = dict()\n",
    "        for stat in base_df_dict[task].keys():\n",
    "            dataset_list = set([check_algo_or_data(ind, whitelist_lookups, check_data=True, \n",
    "                                                   verbose=verbose, blacklist=blacklist_terms) \n",
    "                         for ind in base_df_dict[task][stat].index]) - set([None])\n",
    "            ret_col = ret_col_lookup[task]\n",
    "            algo_list = set([check_algo_or_data(algo, whitelist_lookups, check_algo=True, \n",
    "                                                verbose=verbose, blacklist=blacklist_terms) \n",
    "                         for algo in base_df_dict[task][stat][ret_col].index]) - set([None])\n",
    "            \n",
    "            table = pd.DataFrame(np.zeros((len(algo_list), len(dataset_list))), \n",
    "                                 index=algo_list,\n",
    "                                 columns=dataset_list)\n",
    "\n",
    "            for exp_ident in base_df_dict[task][stat][ret_col].index:\n",
    "                data = check_algo_or_data(exp_ident, whitelist_lookups, check_data=True, \n",
    "                                          verbose=verbose, blacklist=blacklist_terms)\n",
    "                algo = check_algo_or_data(exp_ident, whitelist_lookups, check_algo=True, \n",
    "                                          verbose=verbose, blacklist=blacklist_terms)\n",
    "                if data and algo:\n",
    "                    if verbose: \n",
    "                        print(f\"Using information from {exp_ident}, is that OK?\")\n",
    "                    if not math.isnan(float(base_df_dict[task][stat][ret_col][exp_ident])):\n",
    "                        try: \n",
    "                            table[data][algo] = float(base_df_dict[task][stat][ret_col][exp_ident])\n",
    "                        except: \n",
    "                            import pdb; pdb.set_trace()\n",
    "\n",
    "            # Move control to top of table\n",
    "            idx = order_idx(table.index, algo_order)\n",
    "            table = table.reindex(idx)\n",
    "            data_dataframes[task][stat] = table\n",
    "    return data_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_pooled_dfs(base_df_dict, control_idx): \n",
    "    pooled_dataframes = dict()\n",
    "\n",
    "    for task in base_df_dict.keys():\n",
    "\n",
    "        #create a copy of data_dataframes that I can modify \n",
    "        pooled_dataframes[task] = dict()\n",
    "        for stat in base_df_dict[task].keys(): \n",
    "            pooled_dataframes[task][stat] = base_df_dict[task][stat].copy(deep=True)\n",
    "\n",
    "\n",
    "        pooled_control_mean = pooled_dataframes[task]['mean'].loc[control_idx].mean()\n",
    "        pooled_control_std = pooled_dataframes[task]['std'].loc[control_idx].mean()\n",
    "        pooled_control_n = pooled_dataframes[task]['n'].loc[control_idx].sum()\n",
    "        pooled_dataframes[task]['mean'].loc[control_idx] = pooled_control_mean\n",
    "        pooled_dataframes[task]['std'].loc[control_idx] = pooled_control_std\n",
    "        pooled_dataframes[task]['n'].loc[control_idx] = pooled_control_n\n",
    "    return pooled_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def save_pdf(sns_plot, env, plot_type):\n",
    "    save_dir = f\"./plots/{cluster_subpath.split('/')[1]}\"\n",
    "    save_path = os.path.join(save_dir, f\"{env}-{plot_type}.pdf\")\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    fig = sns_plot.get_figure()\n",
    "    fig.savefig(save_path, bbox_inches=\"tight\")\n",
    "    print(f\"Plot saved at {os.path.abspath(save_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Plotting Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=2)\n",
    "def task_return_heatmap(task, df_dict, control_key, show_cbar=True, fontsize=25, show_ylabel=True): \n",
    "    mean_df = df_dict[task]['mean']\n",
    "    str_format = \"{:.2f}\" if mean_df.loc[:].min()[0] < 1 else \"{:.0f}\"\n",
    "\n",
    "    \n",
    "    std_df = df_dict[task]['std']\n",
    "    std_df = std_df.applymap(str_format.format)\n",
    "\n",
    "    normed_df = mean_df - mean_df.loc[control_key]\n",
    "    normed_df.index = normed_df.index.map(clean_index_val)\n",
    "    \n",
    "    mean_df = mean_df.applymap(str_format.format)\n",
    "    text_df = pd.DataFrame()\n",
    "    for col in mean_df.columns:\n",
    "        text_df[col] = mean_df[col].astype(str) + ' (' + std_df[col].astype(str) + ')'\n",
    "    \n",
    "    figsize = (13, 7)\n",
    "    if show_ylabel:\n",
    "        figsize = (figsize[0]+1, figsize[1])\n",
    "    if show_cbar:\n",
    "        figsize = (figsize[0]+5, figsize[1])\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    plt.title(f\"Mean Return: {task}\", fontsize=30)\n",
    "    \n",
    "    ax = sns.heatmap(normed_df, annot=text_df, annot_kws={\"fontsize\":fontsize}, cbar=show_cbar,\n",
    "                     cmap=sns.diverging_palette(250, 10, s=60, l=45, as_cmap=True), \n",
    "                     center=0, fmt='.20', yticklabels=show_ylabel)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def task_auc_heatmap(task, df_dict, control_key, show_cbar=True, fontsize=25, show_ylabel=True): \n",
    "    str_format = \"{:.2f}\"\n",
    "    task_df = df_dict[task]['mean']\n",
    "    subset_df = task_df[task_df.index.map(lambda x: 'no_ortho' not in x and \n",
    "                                                    'froco' not in x and \n",
    "                                                     'actual' not in x)]\n",
    "    subset_df = subset_df[subset_df.index.map(partial(control_not_key, control_key=control_key))]\n",
    "    \n",
    "    std_df = df_dict[task]['std']\n",
    "    std_sub_df = std_df[std_df.index.map(lambda x: 'no_ortho' not in x and \n",
    "                                                    'froco' not in x and \n",
    "                                                     'actual' not in x)]\n",
    "    std_sub_df = std_sub_df[std_sub_df.index.map(partial(control_not_key, control_key=control_key))]\n",
    "    std_sub_df = std_sub_df.applymap(str_format.format)\n",
    "        \n",
    "    normed_df = subset_df - subset_df.loc[control_key]\n",
    "    normed_df.index = normed_df.index.map(clean_index_val)\n",
    "    \n",
    "    subset_df = subset_df.applymap(str_format.format)\n",
    "    text_df = pd.DataFrame()\n",
    "    for col in subset_df.columns:\n",
    "        text_df[col] = subset_df[col].astype(str) + ' (' + std_sub_df[col].astype(str) + ')'\n",
    "        \n",
    "    figsize = (15, 7)\n",
    "    if show_ylabel:\n",
    "        figsize = (figsize[0]+1, figsize[1])\n",
    "    if show_cbar:\n",
    "        figsize = (figsize[0]+5, figsize[1])\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(f\"AUC: {task}\", fontsize=30)\n",
    "    \n",
    "    ax = sns.heatmap(normed_df, annot=text_df, annot_kws={\"fontsize\":fontsize}, cbar=show_cbar,\n",
    "                     cmap=sns.diverging_palette(10, 250, s=60, l=45, as_cmap=True), \n",
    "                     center=0, fmt='.20', yticklabels=show_ylabel)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Baseline Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Configs for baseline plots \n",
    "control_idx = 'Control (Ortho Init) '\n",
    "blacklist_terms = ['froco', 'ablation', 'newbcaugs']\n",
    "envs = ['cheetah-run', 'finger-spin', 'MatchRegions', 'MoveToRegion']\n",
    "\n",
    "\n",
    "algo_lookups = {\n",
    "    'inv_dyn_': \"Inverse Dynamics \", \n",
    "    \"ac_tcpc_\": \"Action Conditioned TCPC \", \n",
    "    \"vae_\": \"VAE \", \n",
    "    \"dynamics_\": \"Dynamics Model \",\n",
    "    \"control_ortho_init_\": \"Control (Ortho Init) \",\n",
    "    \"identity_cpc_\": \"CPC \", \n",
    "}\n",
    "\n",
    "data_lookups = {\n",
    "    \"cfg_data_repl_random\": \"Random Rollouts\", \n",
    "    \"cfg_data_repl_demos_random\": \"Demos & \\nRandom Rollouts\", \n",
    "    \"cfg_data_repl_demos_magical_mt\": \"Multitask Demos\", \n",
    "    \"cfg_data_repl_rand_demos_magical_mt\": \"Multitask Demos & \\nRandom Rollouts\",\n",
    "}\n",
    "merge_lookups = {**algo_lookups, **data_lookups, 'icml_': '',}\n",
    "nested_lookups = {'data_lookups': data_lookups, 'algo_lookups': algo_lookups}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Return Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ret_col_lookup = {\n",
    "    'MatchRegions': 'Average on all envs', \n",
    "    'MoveToRegion': 'Average on all envs', \n",
    "    'MoveToCorner': 'Average on all envs', \n",
    "    'finger-spin': 'return_mean', \n",
    "    'cheetah-run': 'return_mean'\n",
    "}\n",
    "\n",
    "# Set plot properties according to where we want to position it in paper\n",
    "right_envs = ['MatchRegions', 'cheetah-run']\n",
    "left_envs = ['MoveToRegion', 'finger-spin']\n",
    "\n",
    "algo_order = [\"Control (Ortho Init) \", \"CPC \", \"Action Conditioned TCPC \", \"VAE \", \"Dynamics Model \",\n",
    "              \"Inverse Dynamics \"]\n",
    "\n",
    "pivoted_dfs = create_pivoted_dfs(raw_return_dataframes, ret_col_lookup, blacklist_terms, \n",
    "                               nested_lookups, control_idx, algo_order, verbose=False)\n",
    "pooled_dfs = create_pooled_dfs(pivoted_dfs, control_idx=control_idx)\n",
    "\n",
    "\n",
    "for env in envs:\n",
    "    kwargs = {}\n",
    "    if env in right_envs:\n",
    "        kwargs = {'show_cbar': False}\n",
    "    if env in left_envs:\n",
    "        kwargs = {'show_ylabel': False}\n",
    "    env_plot = task_return_heatmap(env, pooled_dfs, control_key=control_idx, **kwargs)\n",
    "    display(env_plot)\n",
    "    save_pdf(env_plot, env, 'return')\n",
    "\n",
    "# to be paranoid\n",
    "del pivoted_dfs \n",
    "del pooled_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###  AUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "control_idx = 'Control (Ortho Init) '\n",
    "blacklist_terms = ['froco', 'ablation', 'newbcaugs']\n",
    "envs = ['cheetah-run', 'finger-spin', 'MatchRegions', 'MoveToRegion']\n",
    "\n",
    "algo_lookups = {\n",
    "    'inv_dyn_': \"Inverse Dynamics \", \n",
    "    \"ac_tcpc_\": \"Action Conditioned TCPC \", \n",
    "    \"vae_\": \"VAE \", \n",
    "    \"dynamics_\": \"Dynamics Model \",\n",
    "    \"control_ortho_init_\": \"Control (Ortho Init) \",\n",
    "    \"identity_cpc_\": \"CPC \", \n",
    "}\n",
    "\n",
    "data_lookups = {\n",
    "    \"cfg_data_repl_random\": \"Random Rollouts\", \n",
    "    \"cfg_data_repl_demos_random\": \"Demos & \\nRandom Rollouts\", \n",
    "    \"cfg_data_repl_demos_magical_mt\": \"Multitask Demos\", \n",
    "    \"cfg_data_repl_rand_demos_magical_mt\": \"Multitask Demos & \\nRandom Rollouts\",\n",
    "}\n",
    "\n",
    "merge_lookups = {**algo_lookups, **data_lookups, 'icml_': '',}\n",
    "nested_lookups = {'data_lookups': data_lookups, 'algo_lookups': algo_lookups}\n",
    "\n",
    "ret_col_lookup = {\n",
    "    'MatchRegions': 'step 40', \n",
    "    'MoveToRegion': 'step 40', \n",
    "    'MoveToCorner': 'step 40', \n",
    "    'finger-spin': 'step 400', \n",
    "    'cheetah-run': 'step 400'\n",
    "}\n",
    "    \n",
    "pivoted_dfs = create_pivoted_dfs(raw_auc_dataframes, ret_col_lookup, blacklist_terms, \n",
    "                               nested_lookups, control_idx, algo_order, verbose=False)\n",
    "pooled_dfs = create_pooled_dfs(pivoted_dfs, control_idx=control_idx)\n",
    "\n",
    "\n",
    "for env in envs:\n",
    "    kwargs = {}\n",
    "    if env in right_envs:\n",
    "        kwargs = {'show_cbar': False}\n",
    "    if env in left_envs:\n",
    "        kwargs = {'show_ylabel': False}\n",
    "    env_plot = task_auc_heatmap(env, pooled_dfs, control_key=control_idx, **kwargs)\n",
    "    display(env_plot)\n",
    "    save_pdf(env_plot, env, 'AUC')\n",
    "\n",
    "# to be paranoid\n",
    "del pivoted_dfs \n",
    "del pooled_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Older Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def prepare_files(index, mode, exp_index, out_dir):\n",
    "    \"\"\"\n",
    "    Create a folder named `out_dir`. This really just copies over files from il_train or il_test, as appropriate.\n",
    "    For instance, if il_train looks like this:\n",
    "    \n",
    "    il_train\n",
    "    │   ├── 1\n",
    "    │   │   ├── ...\n",
    "    │   │   ├── config.json\n",
    "    │   │   └── progress.csv\n",
    "    │   └── _sources\n",
    "    …\n",
    "    \n",
    "    Then the ouptut will look like this:\n",
    "    ├── progress\n",
    "    │   └── 1\n",
    "    │       ├── params.json   (same as config.json)\n",
    "    │       └── progress.csv\n",
    "    …\n",
    "\n",
    "    After you run this, you can execute viskit with: python viskit/frontend.py path/to/out_dir/\n",
    "    \"\"\"\n",
    "    experiments = index.search(mode=mode)\n",
    "    # compute merged configs (nested/hierarchical dicts), and\n",
    "    # also throw out experiments with no progress.csv\n",
    "    hierarchical_dicts = []\n",
    "    new_experiments = []\n",
    "    for experiment in experiments:\n",
    "        if not experiment.progress_path:\n",
    "            print(\"Skipping experiment\", experiment.ident, \"because it has no progress.csv\")\n",
    "            continue\n",
    "        merged_config = experiment.get_merged_config(exp_index)\n",
    "        hierarchical_dicts.append(merged_config)\n",
    "        new_experiments.append(experiment)\n",
    "    experiments = new_experiments\n",
    "\n",
    "    # first flatten all dicts\n",
    "    dicts = [dict(flatten_dict(d)) for d in hierarchical_dicts]\n",
    "    \n",
    "    # make sure that every dict has every key\n",
    "    all_keys = set()\n",
    "    for d in dicts:\n",
    "        all_keys |= d.keys()\n",
    "    for d in dicts:\n",
    "        for new_key in all_keys - d.keys():\n",
    "            d[new_key] = None\n",
    "    \n",
    "    # now generate outputs for experiments\n",
    "    for flat_config, experiment in zip(dicts, experiments):\n",
    "        exp_out_dir = os.path.join(out_dir, experiment.ident.replace('/', '-'))\n",
    "        os.makedirs(exp_out_dir, exist_ok=True)\n",
    "\n",
    "        params_json_path = os.path.join(exp_out_dir, 'params.json')\n",
    "        with open(params_json_path, 'w') as fp:\n",
    "            json.dump(flat_config, fp)\n",
    "\n",
    "        progress_out_path = os.path.join(exp_out_dir, 'progress.csv')\n",
    "        shutil.copyfile(experiment.progress_path, progress_out_path)\n",
    "        \n",
    "        if mode == 'il_test':\n",
    "            eval_json_path = experiment.eval_json_path\n",
    "            with open(eval_json_path, 'r') as fp:\n",
    "                eval_dict = json.load(fp)\n",
    "                \n",
    "            result_keys, result_vals = [], []\n",
    "            for key, value in eval_dict.items():\n",
    "#                 print(eval_dict.keys())\n",
    "                if key == 'return_mean' and value < 0.1:\n",
    "                    print(eval_dict['policy_path'])\n",
    "                if isinstance(value, str) or isinstance(value, int) or isinstance(value, float):\n",
    "                    result_keys.append(key)\n",
    "                    result_vals.append(value)\n",
    "            \n",
    "            with open(progress_out_path, mode='w') as result_file:\n",
    "                result_writer = csv.writer(result_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                result_writer.writerow(result_keys)\n",
    "                result_writer.writerow(result_vals)\n",
    "\n",
    "                \n",
    "prepare_files(subexp_index, 'repl', subexp_index, 'viskit-repl')\n",
    "prepare_files(subexp_index, 'il_train', subexp_index, 'viskit-il-train')\n",
    "prepare_files(subexp_index, 'il_test', subexp_index, 'viskit-il-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# matplotlib.use(\"tkagg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# include_ident_keywords = ['control', 'tcpc']\n",
    "# exclude_ident_keywords = ['mt', 'rand_only']\n",
    "\n",
    "include_ident_keywords = []\n",
    "exclude_ident_keywords = []\n",
    "\n",
    "def get_data(mode, data_type, include_ident_kw=None, exclude_ident_kw=None):\n",
    "    assert mode in ['repl', 'il_train', 'il_test']\n",
    "    assert data_type in ['loss', 'return']\n",
    "    expts = subexp_index.search(mode=mode)\n",
    "    all_configs = [subexp.get_merged_config(subexp_index) for subexp in expts]\n",
    "    base_config, flat_configs = simplify_config_dicts(all_configs)\n",
    "    flat_config_tups = [tuple(sorted(d.items())) for d in flat_configs]\n",
    "    subexp_by_benchmark = {}\n",
    "    for flat_cfg, subexp in zip(flat_config_tups, expts):\n",
    "        bench_key = tuple((k, v) for k, v in flat_cfg if k.startswith('env_') or k.startswith('venv_'))\n",
    "        subexp_by_benchmark.setdefault(bench_key, []).append((flat_cfg, subexp))\n",
    "    \n",
    "    \"\"\"\n",
    "        ret_dict has structure {'env_1': {'exp_ident_1': required_data, 'exp_ident_2': required_data, ...}, ...}\n",
    "        required_data can be either a list (i.e. loss over time) or a number (int or float, like return_mean)\n",
    "    \"\"\"\n",
    "    ret_dict = {}\n",
    "    for idx, (bench_key, cfgs_subexps) in enumerate(subexp_by_benchmark.items(), start=1):\n",
    "        # cluster subexperiments by config\n",
    "        by_cfg = {}\n",
    "        for tup_cfg, subexp in cfgs_subexps:\n",
    "            tup_cfg = tuple(k for k in tup_cfg if k not in bench_key)\n",
    "            by_cfg.setdefault(tup_cfg, []).append(subexp)\n",
    "\n",
    "        task_name = bench_key[1][1]\n",
    "            \n",
    "        ret_dict[task_name] = {}\n",
    "        for cfg, subexp in cfgs_subexps:\n",
    "            d = dict(cfg)\n",
    "            exp_ident = d['il_train/exp_ident']\n",
    "        \n",
    "            if subexp.progress_path:\n",
    "                try:\n",
    "                    df = pd.read_csv(subexp.progress_path)\n",
    "                except:\n",
    "                    print(f'Read csv reported error for exp {exp_ident}, skipping...')\n",
    "                    continue\n",
    "                full_length = 400 if d['env_cfg/benchmark_name'] == 'dm_control' else 40\n",
    "                if len(df['loss']) != full_length:\n",
    "                    print(f'Experiment {exp_ident} only has len(loss) {len(df[\"loss\"])}, skipping... ')\n",
    "                    continue\n",
    "                \n",
    "                ret_dict[task_name][exp_ident] = []\n",
    "                if data_type == 'loss':\n",
    "                    ret_dict[task_name][exp_ident].append(df['loss'])\n",
    "                    \n",
    "    print(ret_dict.keys())\n",
    "    return ret_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_curves(data_dict):\n",
    "    sns.set(rc={'figure.figsize':(7, 6)})\n",
    "    for task_key, exp_results in data_dict.items():\n",
    "        df = None\n",
    "        col_name = []\n",
    "        plt.figure()\n",
    "        for exp_ident, value, in exp_results.items():\n",
    "            col_name = [f\"seed_{x}\" for x in range(len(value))]\n",
    "            col_name += ['step', 'exp_ident']\n",
    "            value.append([s for s in range(1, len(value[0])+1)])\n",
    "            value.append([exp_ident for s in range(1, len(value[0])+1)])\n",
    "            value = np.array(value).transpose(1, 0)\n",
    "            sub_df = pd.DataFrame(data=value, columns=col_name)\n",
    "            df = pd.concat([df, sub_df])\n",
    "        df = pd.melt(df, id_vars=['step', 'exp_ident'])\n",
    "        df['step'] = pd.to_numeric(df['step'])\n",
    "        df['value'] = pd.to_numeric(df['value'])\n",
    "        print(df.dtypes)\n",
    "        \n",
    "        ax = sns.lineplot(x='step', y='value', hue='exp_ident', data=df)\n",
    "        plt.setp(ax.get_legend().get_texts(), fontsize='12')\n",
    "        plt.legend(bbox_to_anchor=(1.01, 1),borderaxespad=0)\n",
    "        ax.set_title(bench_key)\n",
    "    \n",
    "plot_curves(get_data('il_train', \n",
    "                     'loss', \n",
    "                     include_ident_kw=include_ident_keywords,\n",
    "                     exclude_ident_kw=exclude_ident_keywords))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Interpret encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Save the encoder interpretation videos. Each sub_exp might take one or two minutes to save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "train_expts = subexp_index.search(mode='il_test')\n",
    "all_configs = [subexp.get_merged_config(subexp_index) for subexp in train_expts]\n",
    "base_config, flat_configs = simplify_config_dicts(all_configs)\n",
    "flat_config_tups = [tuple(sorted(d.items())) for d in flat_configs]\n",
    "subexp_by_benchmark = {}\n",
    "for flat_cfg, subexp in zip(flat_config_tups, train_expts):\n",
    "    bench_key = tuple((k, v) for k, v in flat_cfg if k.startswith('env_') or k.startswith('venv_'))\n",
    "    subexp_by_benchmark.setdefault(bench_key, []).append((flat_cfg, subexp))\n",
    "\n",
    "# Create a folder to save videos\n",
    "Path(f\"./runs/{cluster_subpath.split('/')[1]}\").mkdir(parents=True, exist_ok=True)\n",
    "interp_algo = 'saliency'\n",
    "                    \n",
    "for idx, (bench_key, cfgs_subexps) in enumerate(subexp_by_benchmark.items(), start=1):\n",
    "    # cluster subexperiments by config\n",
    "    by_cfg = {}\n",
    "    for tup_cfg, subexp in cfgs_subexps:\n",
    "        tup_cfg = tuple(k for k in tup_cfg if k not in bench_key)\n",
    "        by_cfg.setdefault(tup_cfg, []).append(subexp)\n",
    "\n",
    "    for tup_cfg, subexp in by_cfg.items():\n",
    "        exp = subexp[0]\n",
    "        encoder_path = exp.config['encoder_path']\n",
    "        if encoder_path:\n",
    "            for prefix, replacement in path_translations.items():\n",
    "                if encoder_path.startswith(prefix):\n",
    "                    encoder_path = replacement + encoder_path[len(prefix):]\n",
    "            command = \"python ../src/il_representations/scripts/interpret.py with \"\n",
    "            command += f\"log_dir=runs/{cluster_subpath.split('/')[1]} \"\n",
    "            command += f\"env_cfg.benchmark_name={exp.config['env_cfg']['benchmark_name']} \"\n",
    "            command += f\"env_cfg.task_name={exp.config['env_cfg']['task_name']} \"\n",
    "            command += f\"save_video=True \"\n",
    "            command += f\"chosen_algo={interp_algo} \"\n",
    "            command += f\"encoder_path={encoder_path} \"\n",
    "            command += f\"filename={exp.config['env_cfg']['task_name']}_{exp.config['exp_ident']} \"\n",
    "\n",
    "            print(f\"Generating videos for exp {exp.config['exp_ident']} on {exp.config['env_cfg']['task_name']}...\")\n",
    "            process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n",
    "            output, error = process.communicate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}