{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "\n",
    "runs_directory = \"/scratch/sam/il-representations-gcp-volume/cluster-data/cluster-2020-09-27T03:04Z\"\n",
    "exp_name = \"10\"\n",
    "exp_dir = os.path.join(runs_directory, 'downloads', exp_name)\n",
    "assert os.path.isdir(exp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Trial information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repl_extra_config_key = [\"demo_timesteps\", \"device\", \"n_envs\", \"ppo_finetune\", \"ppo_timesteps\", \n",
    "                         \"seed\", \"torch_num_threads\", \"unit_test_max_train_steps\", \"use_random_rollouts\",\n",
    "                         \"benchmark\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define Trial object\n",
    "class Trial:\n",
    "    \"\"\"\n",
    "        The Trial object contains its directory info.\n",
    "        Each trial should have its repl_dir, il_train_dir, and il_test_dir. Depending on \n",
    "        the status of the trial, it might not have its il_train and il_test (yet).\n",
    "        In this case, self.il_test_dir and self.il_train_dir will be None.\n",
    "    \"\"\"\n",
    "    def __init__(self, last_run_dir):\n",
    "        self.il_test_dir = None\n",
    "        self.il_train_dir = None\n",
    "        self.repl_dir = None\n",
    "        self.set_dirs(last_run_dir)\n",
    "        \n",
    "    def set_dirs(self, last_run_dir):\n",
    "        if 'il_test' in last_run_dir:\n",
    "            self.il_test_dir = last_run_dir\n",
    "            test_config = self.get_config('il_test')\n",
    "            last_run_dir = '/'.join(self.il_test_dir.split('/')[:-2] + \n",
    "                                    ['il_train', test_config['policy_path'].split('/')[-2]])\n",
    "        if 'il_train' in last_run_dir:\n",
    "            self.il_train_dir = last_run_dir\n",
    "            train_config = self.get_config('il_train')\n",
    "            last_run_dir = '/'.join(self.il_train_dir.split('/')[:-2] + \n",
    "                                    ['repl', train_config['encoder_path'].split('/')[-4]])\n",
    "        if 'repl' in last_run_dir:\n",
    "            self.repl_dir = last_run_dir\n",
    "    \n",
    "    def get_config(self, mode, key_to_remove=[]):\n",
    "        # Get the trial config specified by mode, with optional key_to_remove to hide unuseful info.\n",
    "        assert mode in ['repl', 'il_train', 'il_test']\n",
    "        with open(f'{self.__dict__[mode + \"_dir\"]}/config.json') as json_file:\n",
    "                config = json.load(json_file)\n",
    "                \n",
    "        def remove_dict_entry(dic, key_to_remove):\n",
    "            dic_copy = copy.deepcopy(dic)\n",
    "            for key in key_to_remove:\n",
    "                if key not in dic.keys():\n",
    "                    continue\n",
    "                if '/' in key:\n",
    "                    del dic_copy[key.split('/')[0]][key.split('/')[1]]\n",
    "                else:\n",
    "                    del dic_copy[key]\n",
    "            return dic_copy\n",
    "\n",
    "        config = remove_dict_entry(config, key_to_remove)\n",
    "        return config\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n'.join([f'{key}: {value}' for key, value in self.__dict__.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collect trial objects from specified dirs\n",
    "def get_trial_objects(root_dir, exp_type, trial_dirs):\n",
    "    \"\"\"\n",
    "    Return a list of trial objects by inspecting subdirs of run_dir, \n",
    "    which typically is either repl, il_train, or il_test.\n",
    "    \"\"\"\n",
    "    trial_list = []\n",
    "    for trial_dir in trial_dirs:\n",
    "        if trial_dir == '_sources':\n",
    "            continue\n",
    "        dir_abspath = os.path.join(root_dir, exp_type, trial_dir)\n",
    "        trial_list.append(Trial(dir_abspath))\n",
    "    return trial_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tool to pretty print nested dictionaries\n",
    "def pretty_print(d, indent=0):\n",
    "   for key, value in d.items():\n",
    "      print('\\t' * indent + str(key))\n",
    "      if isinstance(value, dict):\n",
    "        for key_, value_ in value.items():\n",
    "             print('\\t' * (indent+1) + str(key_) + ': ' + str(value_))\n",
    "      else:\n",
    "        print('\\t' * (indent+1) + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify trial type and get trial objects accordingly\n",
    "trials = {\n",
    "    'full_exp': [],\n",
    "    'il_train_only': [],\n",
    "    'repl_only': []\n",
    "}\n",
    "\n",
    "il_test_dirs = os.listdir(os.path.join(exp_dir, 'il_test'))\n",
    "trials['full_exp'] = get_trial_objects(exp_dir, 'il_test', il_test_dirs)\n",
    "\n",
    "il_train_dirs = os.listdir(os.path.join(exp_dir, 'il_train'))\n",
    "recorded_train_dirs = [t.il_train_dir.split('/')[-1] for t in trials['full_exp']]\n",
    "il_train_only_dirs = [d for d in il_train_dirs if d not in recorded_train_dirs]\n",
    "trials['il_train_only'] = get_trial_objects(exp_dir, 'il_train', il_train_only_dirs)\n",
    "\n",
    "repl_dirs = os.listdir(os.path.join(exp_dir, 'repl'))\n",
    "recorded_repl_dirs = [t.repl_dir.split('/')[-1] for t in trials['full_exp'] + trials['il_train_only']]\n",
    "repl_only_dirs = [d for d in repl_dirs if d not in recorded_repl_dirs]\n",
    "trials['repl_only'] = get_trial_objects(exp_dir, 'repl', repl_only_dirs)\n",
    "\n",
    "print('Experiment info: \\n')\n",
    "print('\\n'.join([f'{trial_type}: {len(trial_list)} runs' for trial_type, trial_list in trials.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter for runs whose performance exceeds vanilla IL baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Glob the test result files\n",
    "glob_query = \"chain_runs/[0-9]*/il_test/[0-9]*/run.json\"\n",
    "glob_arg = os.path.join(runs_directory, glob_query)\n",
    "evaluation_files = glob.glob(glob_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Helper function to parse mean return from .json file\n",
    "def get_evaluation_result(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        results = json.load(f)\n",
    "        if results['result'] is not None:\n",
    "            return results['result']['return_mean']['value']\n",
    "        else:\n",
    "            return float(\"-inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parse the test result files for mean return\n",
    "evaluation_results = []\n",
    "for filename in evaluation_files:\n",
    "    evaluation_result = get_evaluation_result(filename)\n",
    "    evaluation_results.append(evaluation_result)\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter for test result files that exceed IL baseline return\n",
    "baseline_filename = evaluation_files[0]  #TODO put the filename of the baseline IL run here\n",
    "baseline_return = get_evaluation_result(baseline_filename)\n",
    "runs_with_improvement = []\n",
    "for run_file, run_return in zip(evaluation_files, evaluation_results):\n",
    "    if run_return > baseline_return:\n",
    "        run_name = os.path.dirname(run_file)\n",
    "        runs_with_improvement.append(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(evaluation_files)\n",
    "print('\\n')\n",
    "print(evaluation_results)\n",
    "print('\\n')\n",
    "print(runs_with_improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make sure your cwd is the il-representations directory\n",
    "if os.getcwd().split('/')[-1] == 'analysis':\n",
    "    os.chdir(\"..\")\n",
    "print('Check cwd', os.getcwd())\n",
    "\n",
    "from il_representations.scripts.interpret import (prepare_network, process_data, save_img, saliency_, deep_lift_, \n",
    "integrated_gradient_, layer_conductance_, layer_gradcam_, layer_act_, choose_layer, interp_ex)\n",
    "from il_representations.envs.config import benchmark_ingredient\n",
    "import il_representations.envs.auto as auto_env\n",
    "\n",
    "import sacred\n",
    "from sacred.observers import FileStorageObserver\n",
    "from sacred import Experiment\n",
    "from stable_baselines3.common.utils import get_device\n",
    "from captum.attr import LayerActivation, LayerGradientXActivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "render_interp_ex = Experiment('render_interp', ingredients=[benchmark_ingredient, interp_ex], interactive=True)\n",
    "interp_ex.observers.append(FileStorageObserver('runs/interpret_runs'))\n",
    "\n",
    "##### These should be the only things you need to modify in this code block #####\n",
    "policy_paths = [os.path.join(t.il_train_dir, 'policy_final.pt') for t in trials['full_exp']]\n",
    "configs = [t.get_config('repl', key_to_remove=repl_extra_config_key) for t in trials['full_exp']]\n",
    "\n",
    "@interp_ex.config\n",
    "def config():\n",
    "    encoder_paths = policy_paths\n",
    "    for path in encoder_paths:\n",
    "        assert os.path.isfile(path), f'Please double check if {path} exists.'\n",
    "    \n",
    "    \n",
    "    # Data settings\n",
    "    # The benchmark is set by detecting il_representations/envs/config's bench_defaults.benchmark_name\n",
    "    imgs = [8]  # index of the image to be inspected (int)\n",
    "    assert all(isinstance(im, int) for im in imgs), 'imgs list should contain integers only'\n",
    "\n",
    "    verbose = False\n",
    "    #################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "render_interp_ex = Experiment('render_interp', ingredients=[benchmark_ingredient, interp_ex], interactive=True)\n",
    "\n",
    "@render_interp_ex.main\n",
    "def run():\n",
    "    venv = auto_env.load_vec_env()\n",
    "    networks = prepare_network(venv)\n",
    "    images, labels = process_data()\n",
    "    return networks, images, labels\n",
    "\n",
    "r = render_interp_ex.run()\n",
    "networks = r.result[0]\n",
    "images = r.result[1]\n",
    "labels = r.result[2]\n",
    "verbose = True\n",
    "log_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def saliency():\n",
    "    for img, label in zip(images, labels):\n",
    "        for config, network in zip(configs, networks):\n",
    "            print('='*50)\n",
    "            pretty_print(config)\n",
    "            original_img = img[0].permute(1, 2, 0).detach().numpy()\n",
    "            saliency_(network, img, label, original_img, log_dir, False)\n",
    "\n",
    "saliency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def deep_lift():\n",
    "    for img, label in zip(images, labels):\n",
    "        for config, network in zip(configs, networks):\n",
    "            print('='*50)\n",
    "            original_img = img[0].permute(1, 2, 0).detach().numpy()\n",
    "            deep_lift_(network, img, label, original_img, log_dir, False)\n",
    "\n",
    "deep_lift()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
