{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import copy\n",
    "import html\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import json\n",
    "import glob\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "# FIXME(sam): make cluster subpath selectable with a dropdown. Also, automatically infer\n",
    "# gfs_mount from the hostname (have sensible default for svm/perceptron)\n",
    "\n",
    "# this identifies data for a particular cluster on the GFS volume\n",
    "# cluster_subpath = \"with-resnet-128-2020-01-27/\"\n",
    "cluster_subpath = \"cluster-data/cluster-2021-01-29-set3-try4/\"\n",
    "# on svm I think gfs_mount is /scratch/sam/repl-vol/ or something like that\n",
    "# gfs_mount = \"/scratch/cynthiachen/\"\n",
    "gfs_mount = \"/scratch/sam/il-representations-gcp-volume/\"  # Google Filestore mount point (local)\n",
    "runs_directory = os.path.join(gfs_mount, cluster_subpath)\n",
    "path_translations = {\n",
    "    # when loading things like 'encoder_path' and 'policy_path' from configs,\n",
    "    # replace the thing on the left with the thing on the right\n",
    "    \"/data/il-representations/\": gfs_mount,\n",
    "    \"/root/il-rep/runs/\": os.path.join(gfs_mount, cluster_subpath),\n",
    "    \"/home/sam/repos/il-representations/cloud/runs/\": os.path.join(gfs_mount, cluster_subpath)\n",
    "}\n",
    "\n",
    "exp_dir = os.path.join(runs_directory, 'chain_runs')\n",
    "assert os.path.exists(exp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a table of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_parent_relpath(sample_parent_file, local_root_dir):\n",
    "    \"\"\"Get root-relative path to a 'parent' directory, such as the directory\n",
    "    containing a saved encoder or policy. This is somewhat tricky because\n",
    "    we need to replace paths that might be different on svm/perceptron or on\n",
    "    a laptop compared to what they were on GCP. e.g. inside the Ray docker\n",
    "    container, '/root/il-rep/runs' maps to 'cluster-data/' in the GFS volume.\n",
    "    The `path_translations` variable handles all the necessary changes.\"\"\"\n",
    "    for prefix, replacement in path_translations.items():\n",
    "        if sample_parent_file.startswith(prefix):\n",
    "            sample_parent_file = replacement + sample_parent_file[len(prefix):]\n",
    "\n",
    "    full_path = os.path.abspath(sample_parent_file)\n",
    "    full_dir = os.path.dirname(full_path)\n",
    "    rel_dir = os.path.relpath(full_dir, local_root_dir)\n",
    "\n",
    "    return rel_dir\n",
    "\n",
    "class SubexperimentRun:\n",
    "    \"\"\"A SubexperimentRun associates all the information associated\n",
    "    with a run of a particular Sacred sub-experiment. That means a\n",
    "    run (single execution) of the 'repl', 'il_train', or 'il_test'\n",
    "    experiments.\"\"\"\n",
    "    def __init__(self, subexp_dir, experiment_dir_root):\n",
    "        # Subexperiment dir is used as a unique identifier.\n",
    "        # We strip out the leading 'experiment_dir_root' to shorten identifiers.\n",
    "        subexp_dir = os.path.abspath(subexp_dir)\n",
    "        experiment_dir_root = os.path.abspath(experiment_dir_root)\n",
    "        self.ident = os.path.relpath(subexp_dir, experiment_dir_root)\n",
    "        self.subexp_dir = subexp_dir\n",
    "        self.experiment_dir_root = experiment_dir_root\n",
    "\n",
    "        # usually paths are like, e.g., 'chain_runs/repl/42' or\n",
    "        # 'chain_runs/il_train/13'; if we take the second last component,\n",
    "        # we should get the mode\n",
    "        self.mode = os.path.split(os.path.split(subexp_dir)[0])[1]\n",
    "        assert self.mode in {'repl', 'il_train', 'il_test'}, (mode, subexp_dir)\n",
    "\n",
    "        # Load experiment config\n",
    "        config_path = os.path.join(subexp_dir, 'config.json')\n",
    "        with open(config_path, 'r') as fp:\n",
    "            self.config = json.load(fp)\n",
    "\n",
    "        # Store a path to relevant progress.csv file (only for il_train/repl)\n",
    "        progress_path = os.path.join(subexp_dir, \"progress.csv\")\n",
    "        if os.path.exists(progress_path):\n",
    "            self.progress_path = progress_path\n",
    "        else:\n",
    "            self.progress_path = None\n",
    "\n",
    "        # Store a path to relevant eval.json file\n",
    "        eval_json_path = os.path.join(subexp_dir, \"eval.json\")\n",
    "        if os.path.exists(eval_json_path):\n",
    "            self.eval_json_path = eval_json_path\n",
    "        else:\n",
    "            self.eval_json_path = None\n",
    "\n",
    "        # Infer the .ident attribute for the parent experiment\n",
    "        # (if it exists)\n",
    "        if self.mode == 'il_train' and self.config.get('encoder_path') is not None:\n",
    "            encoder_relpath = get_parent_relpath(\n",
    "                self.config['encoder_path'], experiment_dir_root)\n",
    "            # The relpath is going to be something like\n",
    "            # \"chain_runs/10/repl/5/checkpoints/representation_encoder\".\n",
    "            # We heuristically remove the last two parts.\n",
    "            # (this definitely breaks on Windows…)\n",
    "            encoder_relpath = '/'.join(encoder_relpath.split('/')[:-2])\n",
    "            self.parent_ident = encoder_relpath\n",
    "        elif self.mode == 'il_test':\n",
    "            policy_relpath = get_parent_relpath(\n",
    "                self.config['policy_path'], experiment_dir_root)\n",
    "            self.parent_ident = policy_relpath\n",
    "        else:\n",
    "            # \"repl\" runs and \"il_train\" runs without an encoder_path\n",
    "            # have no parents\n",
    "            assert self.mode == 'repl' \\\n",
    "              or (self.mode == 'il_train' and self.config.get('encoder_path') is None), \\\n",
    "               (self.mode, self.config.get('encoder_path'))\n",
    "            self.parent_ident = None\n",
    "            \n",
    "        # HACK: adding a use_repl key so that we can see whether il_train runs used repL\n",
    "        if self.mode == 'il_train':\n",
    "            self.config['use_repl'] = self.parent_ident is not None\n",
    "\n",
    "    def get_merged_config(self, index):\n",
    "        \"\"\"Get a 'merged' config dictionary for this subexperiment and\n",
    "        all of its parents. The dict will have a format like this:\n",
    "        \n",
    "        {\"benchmark\": {…}, \"il_train\": {…}, \"il_test\": {…}, \"repl\": {…}}\n",
    "        \n",
    "        Note that some keys might not be present (e.g. if this is a `repl` run,\n",
    "        it will not have the `il_train` key; if this is an `il_train` run with\n",
    "        no parent, then the `repl` key will be absent).\"\"\"\n",
    "        config = {self.mode: dict(self.config)}\n",
    "        extract_keys = ('env_cfg', 'venv_opts', 'env_data')\n",
    "        for extract_key in extract_keys:\n",
    "            if extract_key in config[self.mode]:\n",
    "                # move 'benchmark' key to the top because that ingredient name is\n",
    "                # shared between il_train, and il_test experiments\n",
    "                config[extract_key] = config[self.mode][extract_key]\n",
    "                del config[self.mode][extract_key]\n",
    "        parent = self.get_parent(index)\n",
    "        if parent is not None:\n",
    "            # TODO: merge this properly, erroring on incompatible duplicate\n",
    "            # keys. I think Cody has code for this.\n",
    "            config.update(parent.get_merged_config(index))\n",
    "        return config\n",
    "    \n",
    "    def get_parent(self, index):\n",
    "        if self.parent_ident is None:\n",
    "            return None\n",
    "        return index.get_subexp(self.parent_ident)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.ident)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, SubexperimentRun):\n",
    "            return NotImplemented\n",
    "        return self.ident == other.ident\n",
    "\n",
    "class SubexperimentIndex:\n",
    "    \"\"\"An index of subexperiments. For now this just supports\n",
    "    looking up experiments by identifier. Later it might support\n",
    "    lookup by attributes.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.subexp_by_ident = {}\n",
    "        \n",
    "    def add_subexp(self, subexp):\n",
    "        if subexp.ident in self.subexp_by_ident:\n",
    "            raise ValueError(\"duplicate subexperiment:\", subexp)\n",
    "        self.subexp_by_ident[subexp.ident] = subexp\n",
    "        \n",
    "    def get_subexp(self, ident):\n",
    "        return self.subexp_by_ident[ident]\n",
    "    \n",
    "    def search(self, **attrs):\n",
    "        \"\"\"Find a subexperiment with attributes matching the values\n",
    "        given in 'attrs'.\"\"\"\n",
    "        results = []\n",
    "        for subexp in self.subexp_by_ident.values():\n",
    "            for k, v in attrs.items():\n",
    "                if getattr(subexp, k) != v:\n",
    "                    break\n",
    "            else:\n",
    "                results.append(subexp)\n",
    "        return results\n",
    "\n",
    "def get_experiment_directories(root_dir, skip_skopt=True):\n",
    "    \"\"\"Look for directories that end in a sequence of numbers, and contain a\n",
    "    grid_search subdirectory.\"\"\"\n",
    "    expt_pat = re.compile(r'^.*/(il_test|il_train|repl)/\\d+$')\n",
    "    ignore_pat = re.compile(r'^.*/(grid_search|_sources)$')  # ignore the grid_search subdir\n",
    "    expt_dirs = set()\n",
    "    for root, dirs, files in os.walk(root_dir, followlinks=True, topdown=True):\n",
    "        if ignore_pat.match(root):\n",
    "            del dirs[:]\n",
    "            continue\n",
    "            \n",
    "        # check whether tihs is a skopt dir\n",
    "        if skip_skopt and 'grid_search' in dirs:\n",
    "            gs_files = os.listdir(os.path.join(root, 'grid_search'))\n",
    "            if any(s.startswith('search-alg-') for s in gs_files):\n",
    "                # this is a skopt dir, skip it\n",
    "                print(\"skipping skopt directory in\", root)\n",
    "                del dirs[:]\n",
    "                continue\n",
    "\n",
    "        found_match = False\n",
    "        for d in dirs:\n",
    "            d_path = os.path.abspath(os.path.join(root, d))\n",
    "            m = expt_pat.match(d_path)\n",
    "            if m is None:\n",
    "                continue  # no match\n",
    "            expt_dirs.add(d_path)\n",
    "            found_match = True\n",
    "\n",
    "        if found_match:\n",
    "            del dirs[:]  # don't recurse\n",
    "    return sorted(expt_dirs)\n",
    "\n",
    "# Find all experiment directories (i.e. directories containing a grid_search\n",
    "# subdir)\n",
    "def load_all_subexperiments(root_dir, skip_skopt=True):\n",
    "    \"\"\"Find all experiment run subdirectories, and create SubexperimentIndex objects for them.\"\"\"\n",
    "    print(\"Searching for experiment directories (might take a minute or two)\")\n",
    "    all_expt_directories = get_experiment_directories(root_dir, skip_skopt=skip_skopt)\n",
    "    print(\"Loading experiments (might take another minute or two)\")\n",
    "    index = SubexperimentIndex()\n",
    "    for expt_dir in all_expt_directories:\n",
    "        subexp = SubexperimentRun(expt_dir, root_dir)\n",
    "        index.add_subexp(subexp)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for experiment directories (might take a minute or two)\n"
     ]
    }
   ],
   "source": [
    "subexp_index = load_all_subexperiments(runs_directory, skip_skopt=True)\n",
    "print('Discovered', len(subexp_index.subexp_by_ident), 'subexperiments')\n",
    "\n",
    "test_expts = subexp_index.search(mode='il_test')\n",
    "\n",
    "test_expts[1].get_merged_config(subexp_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print a table of il_test results\n",
    "\n",
    "Shows a separate set of il_test results for each benchmark setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def flatten_dict(d):\n",
    "    \"\"\"Flatten a nested dict into a single-level dict with\n",
    "    'keys/separated/like/this'.\"\"\"\n",
    "    out_dict = {}\n",
    "    if isinstance(d, dict):\n",
    "        key_iter = d.items()\n",
    "    else:\n",
    "        assert isinstance(d, list), type(d)\n",
    "        # we flatten lists into dicts of the form {0: <first elem>, 1: <second elem>, …}\n",
    "        key_iter = ((str(idx), v) for idx, v in enumerate(d))\n",
    "    for key, value in key_iter:\n",
    "        if isinstance(value, (dict, list)):\n",
    "            value = flatten_dict(value)\n",
    "            for subkey, subvalue in value.items():\n",
    "                out_dict[f'{key}/{subkey}'] = subvalue\n",
    "        else:\n",
    "            out_dict[key] = value\n",
    "    return out_dict\n",
    "\n",
    "def combine_dicts_multiset(dicts):\n",
    "    \"\"\"Combine a series of dicts into a key-multiset mapping, where the\n",
    "    multiset measures how many times each observed value occurs for each\n",
    "    key.\"\"\"\n",
    "    count_dict = {}\n",
    "    for d in dicts:\n",
    "        for k, v in d.items():\n",
    "            if k not in count_dict:\n",
    "                count_dict[k] = collections.Counter()\n",
    "            count_dict[k][v] += 1\n",
    "    return count_dict\n",
    "\n",
    "def remove_inapplicable_keys(flat_dict):\n",
    "    \"\"\"Remove keys that do not make a difference from a flattened config dicts.\n",
    "    Totally heuristic, so might have to add more options to this later on.\"\"\"\n",
    "    remove_keys = set()\n",
    "    \n",
    "    # remove inapplicable benchmark keys\n",
    "    for benchmark_name in ['magical', 'dm_control']:\n",
    "        if flat_dict.get('env_cfg/benchmark_name') != benchmark_name:\n",
    "            for key in flat_dict:\n",
    "                # this will remove, e.g., dm_control keys from magical experiments\n",
    "                if key.startswith('env_cfg/' + benchmark_name) or key.startswith('env_data/' + benchmark_name):\n",
    "                    remove_keys.add(key)\n",
    "                    \n",
    "    # remove repl keys from things that don't use repL\n",
    "    if flat_dict.get('il_train/use_repl') is False:\n",
    "        for key in flat_dict:\n",
    "            if key.startswith('repl/'):\n",
    "                remove_keys.add(key)\n",
    "                    \n",
    "    return {k: v for k, v in flat_dict.items() if k not in remove_keys}\n",
    "\n",
    "def simplify_config_dicts(hierarchical_dicts,\n",
    "                          base_thresh=0.75,\n",
    "                          remove_seeds=True,\n",
    "                          prohibited_base_keys=('env_cfg/task_name', 'env_cfg/benchmark_name', ),\n",
    "                          force_remove_keys=('il_test/policy_path', 'il_train/encoder_path')):\n",
    "    \"\"\"Simplify flattened config dicts so that:\n",
    "    \n",
    "    0. They are totally flat.\n",
    "    1. They only contain keys for which values actually differ between\n",
    "       different dicts, and\n",
    "    2. If the value of some key is the same for at least a fraction\n",
    "       `base_thresh` of dicts, then that key is moved into a _base config_.\n",
    "       Returned dicts will only contain that key if they have a different\n",
    "       value from the base config one.\n",
    "    3. Optionally, remove all seed values from dicts.\n",
    "\n",
    "    This makes it more clear which values are actually changing.\"\"\"\n",
    "    # first flatten all dicts\n",
    "    dicts = [dict(flatten_dict(d)) for d in hierarchical_dicts]\n",
    "    \n",
    "    # remove seeds, if required\n",
    "    if remove_seeds:\n",
    "        for d in dicts:\n",
    "            for key in list(d.keys()):\n",
    "                if key.split('/')[-1] == 'seed':\n",
    "                    del d[key]\n",
    "                    \n",
    "    # make sure that every dict has every key\n",
    "    all_keys = set()\n",
    "    for d in dicts:\n",
    "        all_keys |= d.keys()\n",
    "    for d in dicts:\n",
    "        for new_key in all_keys - d.keys():\n",
    "            d[new_key] = None\n",
    "                    \n",
    "    # remove inapplicable keys\n",
    "    dicts = [remove_inapplicable_keys(d) for d in dicts]\n",
    "\n",
    "    # now figure out which keys we wish to remove or move to the base config\n",
    "    base_config = {}\n",
    "    remove_keys = set()\n",
    "    base_thresh_abs = len(dicts) * base_thresh\n",
    "    count_dict = combine_dicts_multiset(dicts)\n",
    "    for key, counter in count_dict.items():\n",
    "        if len(counter) == 1 or key in force_remove_keys:\n",
    "            # if all dicts have the same value for this key, we will\n",
    "            # remove it from output dicts\n",
    "            remove_keys.add(key)\n",
    "        elif key not in prohibited_base_keys:\n",
    "            # if most dicts have the same value for this key, then\n",
    "            # we add it to the base config\n",
    "            (max_count_item, max_count), = counter.most_common(1)\n",
    "            if max_count > base_thresh_abs:\n",
    "                base_config[key] = max_count_item\n",
    "\n",
    "    # remove keys that we are ignoring, or for which the corresponding value\n",
    "    # already exists in the base config\n",
    "    new_dicts = []\n",
    "    for old_dict in dicts:\n",
    "        new_dict = {}\n",
    "        for key, value in old_dict.items():\n",
    "            if key in remove_keys \\\n",
    "              or (key in base_config and base_config[key] == value):\n",
    "                continue  # skip this key\n",
    "            new_dict[key] = value\n",
    "        new_dicts.append(new_dict)\n",
    "    \n",
    "    return base_config, new_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_configs = [subexp.get_merged_config(subexp_index) for subexp in test_expts]\n",
    "base_config, flat_configs = simplify_config_dicts(all_configs)\n",
    "flat_config_tups = [tuple(sorted(d.items())) for d in flat_configs]\n",
    "subexp_by_benchmark = {}\n",
    "for flat_cfg, subexp in zip(flat_config_tups, test_expts):\n",
    "    bench_key = tuple((k, v) for k, v in flat_cfg if k.startswith('env_') or k.startswith('venv_'))\n",
    "    subexp_by_benchmark.setdefault(bench_key, []).append((flat_cfg, subexp))\n",
    "\n",
    "display(HTML('<p><strong>Base config</strong></p>'))\n",
    "display(HTML('<p>Unless specified otherwise, all config dicts include these keys:</p>'))\n",
    "print(base_config)\n",
    "    \n",
    "for idx, (bench_key, cfgs_subexps) in enumerate(subexp_by_benchmark.items(), start=1):\n",
    "    # print out benchmark details\n",
    "    display(HTML(f'<p><strong>Results for benchmark config &#35;{idx}</strong></p>'))\n",
    "    display(HTML(f'<p>Config:</p>'))\n",
    "    rows = [f'<tr><th>{html.escape(key)}</th><td>{html.escape(value)}</td></tr>' for key, value in bench_key]\n",
    "    display(HTML(f'<table>{\"\".join(rows)}</table>'))\n",
    "    display(HTML(f'<p>Runs:</p>'))\n",
    "    \n",
    "    # cluster subexperiments by config\n",
    "    by_cfg = {}\n",
    "    for tup_cfg, subexp in cfgs_subexps:\n",
    "        tup_cfg = tuple(k for k in tup_cfg if k not in bench_key)\n",
    "        by_cfg.setdefault(tup_cfg, []).append(subexp)\n",
    "        \n",
    "\n",
    "    # load all eval.json files and figure out what columns we need\n",
    "    stats_dicts = {}\n",
    "    columns = set()\n",
    "    for _, subexp in cfgs_subexps:\n",
    "        if subexp.eval_json_path:\n",
    "            with open(subexp.eval_json_path, 'r') as fp:\n",
    "                eval_dict = json.load(fp)\n",
    "            # is this a magical run?\n",
    "            is_magical = 'full_data' in eval_dict.keys()\n",
    "            if is_magical:\n",
    "                stats_dict = {\n",
    "                    '-'.join(env_dict['test_env'].split('-')[:2]): env_dict['mean_score']\n",
    "                    for env_dict in eval_dict['full_data']\n",
    "                }\n",
    "                stats_dict['Average on all envs'] = eval_dict['return_mean']\n",
    "            else:\n",
    "                stats_dict = {'return_mean': eval_dict['return_mean']}\n",
    "            stats_dicts[subexp] = stats_dict\n",
    "            columns |= stats_dict.keys()\n",
    "        else:\n",
    "            stats_dicts[subexp] = {}\n",
    "    columns = sorted(columns)\n",
    "    \n",
    "    # now produce a table with one row per config\n",
    "    table_parts = ['<table>']                                         # begin table\n",
    "    table_parts.append('<tr>')                                        # begin header row\n",
    "    table_parts.append('<th style=\"border-collapse: collapse;\">Config</th>')\n",
    "    table_parts.extend(f'<th style=\"border-collapse: collapse;\">{html.escape(col_name)}</th>' for col_name in columns)\n",
    "    table_parts.append('</tr>')                                       # end header row\n",
    "\n",
    "    for cfg, subexps in sorted(by_cfg.items(), key=lambda cfg_se: dict(cfg_se[0])['il_test/exp_ident']):\n",
    "        table_parts.append('<tr>')                                    # begin row\n",
    "\n",
    "        # cell containing config\n",
    "        if True:  # remove to show full config\n",
    "            d = dict(cfg)\n",
    "            exp_ident = d['il_test/exp_ident']\n",
    "            # bench_name = d['env_cfg/benchmark_name']\n",
    "            # task_name = d['env_cfg/task_name']\n",
    "            # desc_str = f'{exp_ident} ({bench_name}/{task_name})'\n",
    "            table_parts.append(f'<td style=\"border-collapse: collapse;\">{html.escape(exp_ident)}</td>')\n",
    "        else:\n",
    "            kv_cfg = ', '.join(f'{key}={value!r}' for key, value in cfg)\n",
    "            table_parts.append(f'<td style=\"max-width: 600px; border-collapse: collapse;\">{html.escape(kv_cfg)}</td>')\n",
    "            \n",
    "        # cells containing data\n",
    "        for column in columns:\n",
    "            column_values = [stats_dicts[subexp][column] for subexp in subexps\n",
    "                             if column in stats_dicts[subexp]]\n",
    "            if not column_values:\n",
    "                table_parts.append('<td style=\"border-collapse: collapse;\">-</td>')\n",
    "            else:\n",
    "                mean = np.mean(column_values)\n",
    "                std = np.std(column_values)\n",
    "                n = len(column_values)\n",
    "                table_parts.append(f'<td style=\"border-collapse: collapse;\">{mean:.3g}±{std:.1g} ({n})</td>')\n",
    "        \n",
    "        # cells containing values\n",
    "\n",
    "        table_parts.append('</tr>')                                   # end row\n",
    "    table_parts.append('</table>')                                    # end table\n",
    "    display(HTML(''.join(table_parts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "import seaborn as sns \n",
    "from math import sqrt \n",
    "import pandas as pd \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_configs = [subexp.get_merged_config(subexp_index) for subexp in test_expts]\n",
    "base_config, flat_configs = simplify_config_dicts(all_configs)\n",
    "flat_config_tups = [tuple(sorted(d.items())) for d in flat_configs]\n",
    "subexp_by_benchmark = {}\n",
    "for flat_cfg, subexp in zip(flat_config_tups, test_expts):\n",
    "    bench_key = tuple((k, v) for k, v in flat_cfg if k.startswith('env_') or k.startswith('venv_'))\n",
    "    subexp_by_benchmark.setdefault(bench_key, []).append((flat_cfg, subexp))\n",
    "\n",
    "display(HTML('<p><strong>Base config</strong></p>'))\n",
    "display(HTML('<p>Unless specified otherwise, all config dicts include these keys:</p>'))\n",
    "print(base_config)\n",
    "\n",
    "raw_data = {}\n",
    "    \n",
    "for idx, (bench_key, cfgs_subexps) in enumerate(subexp_by_benchmark.items(), start=1):\n",
    "    # cluster subexperiments by config\n",
    "    by_cfg = {}\n",
    "    for tup_cfg, subexp in cfgs_subexps:\n",
    "        tup_cfg = tuple(k for k in tup_cfg if k not in bench_key)\n",
    "        by_cfg.setdefault(tup_cfg, []).append(subexp)\n",
    "        \n",
    "\n",
    "    # load all eval.json files and figure out what columns we need\n",
    "    stats_dicts = {}\n",
    "    columns = set()\n",
    "    for cfg, subexp in cfgs_subexps:\n",
    "        exp_ident = dict(cfg)['il_train/exp_ident']\n",
    "        \n",
    "        if subexp.eval_json_path:\n",
    "            with open(subexp.eval_json_path, 'r') as fp:\n",
    "                eval_dict = json.load(fp)\n",
    "            # is this a magical run?\n",
    "            is_magical = 'full_data' in eval_dict.keys()\n",
    "            if is_magical:\n",
    "                stats_dict = {\n",
    "                    '-'.join(env_dict['test_env'].split('-')[:2]): env_dict['mean_score']\n",
    "                    for env_dict in eval_dict['full_data']\n",
    "                }\n",
    "                stats_dict['Average on all envs'] = eval_dict['return_mean']\n",
    "            else:\n",
    "                stats_dict = {'return_mean': eval_dict['return_mean']}\n",
    "            stats_dicts[subexp] = stats_dict\n",
    "            columns |= stats_dict.keys()\n",
    "        else:\n",
    "            stats_dicts[subexp] = {}\n",
    "    columns = sorted(columns)\n",
    "    \n",
    "    # now produce a table with one row per config\n",
    "    table_parts = ['<table>']                                         # begin table\n",
    "    table_parts.append('<tr>')                                        # begin header row\n",
    "    table_parts.append('<th style=\"border-collapse: collapse;\">Config</th>')\n",
    "    table_parts.extend(f'<th style=\"border-collapse: collapse;\">{html.escape(col_name)}</th>' for col_name in columns)\n",
    "    table_parts.append('</tr>')                                       # end header row\n",
    "\n",
    "    \n",
    "    # iterate over rows \n",
    "    row_indexes = []\n",
    "    rows = []\n",
    "    for cfg, subexps in sorted(by_cfg.items(), key=lambda cfg_se: dict(cfg_se[0])['il_test/exp_ident']):\n",
    "        table_parts.append('<tr>')                                    # begin row\n",
    "\n",
    "        # cell containing config\n",
    "        if True:  # remove to show full config\n",
    "            d = dict(cfg)\n",
    "            exp_ident = d['il_test/exp_ident']\n",
    "            # bench_name = d['env_cfg/benchmark_name']\n",
    "            # task_name = d['env_cfg/task_name']\n",
    "            # desc_str = f'{exp_ident} ({bench_name}/{task_name})'\n",
    "            table_parts.append(f'<td style=\"border-collapse: collapse;\">{html.escape(exp_ident)}</td>')\n",
    "            row_indexes.append(exp_ident)\n",
    "        else:\n",
    "            kv_cfg = ', '.join(f'{key}={value!r}' for key, value in cfg)\n",
    "            table_parts.append(f'<td style=\"max-width: 600px; border-collapse: collapse;\">{html.escape(kv_cfg)}</td>')\n",
    "            \n",
    "        # cells containing data\\\n",
    "        row_values = []\n",
    "        for column in columns:\n",
    "            column_values = [stats_dicts[subexp][column] for subexp in subexps\n",
    "                             if subexp in stats_dicts.keys() and column in stats_dicts[subexp]]\n",
    "            if not column_values:\n",
    "                table_parts.append('<td style=\"border-collapse: collapse;\">-</td>')\n",
    "            else:\n",
    "                mean = np.mean(column_values)\n",
    "                std = np.std(column_values)\n",
    "                n = len(column_values)\n",
    "                row_values.append({'mean': mean, 'std': std, 'n': n})\n",
    "            \n",
    "        rows.append(row_values)\n",
    "\n",
    "        # cells containing values\n",
    "\n",
    "        table_parts.append('</tr>')                                   # end row\n",
    "    raw_data[bench_key[1][1]] = {'row_indexes': row_indexes, 'rows': rows, 'columns': columns}\n",
    "    table_parts.append('</table>')                                    # end table\n",
    "    #display(HTML(''.join(table_parts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def control_not_key(col, control_key): \n",
    "    if 'control' in col: \n",
    "        if col == control_key: \n",
    "            return True \n",
    "        else: \n",
    "            return False \n",
    "    else: \n",
    "        return True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "algo_lookups = {\n",
    "    'inv_dyn_': \"Inverse Dynamics \", \n",
    "    \"ac_tcpc_\": \"Action Conditioned TCPC \", \n",
    "    \"vae_\": \"VAE \", \n",
    "    \"dynamics_\": \"Dynamics Model \",\n",
    "    \"control_ortho_init_\": \"Control (Random Init) \",\n",
    "    \"identity_cpc_\": \"Augmentation CPC \", \n",
    "}\n",
    "\n",
    "data_lookups = {\n",
    "    \"cfg_data_repl_random\": \"Random Rollouts\", \n",
    "    \"cfg_data_repl_demos_random\": \"Demos & Random Rollouts\", \n",
    "    \"cfg_data_repl_demos_magical_mt\": \"Multitask Demos\", \n",
    "    \"cfg_data_repl_rand_demos_magical_mt\": \"Multitask Demos & Random Rollouts\",\n",
    "}\n",
    "\n",
    "merge_lookups = {**algo_lookups, **data_lookups, 'icml_': '',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_index_val(index, check_algo=False, check_data=False): \n",
    "    new_index = index \n",
    "    for lookup_key, lookup_val in merge_lookups.items(): \n",
    "        new_index = new_index.replace(lookup_key, lookup_val)\n",
    "    return new_index \n",
    "\n",
    "def check_algo_or_data(index, check_algo=False, check_data=False, verbose=True):\n",
    "    new_index = index \n",
    "    assert sum([check_algo, check_data]) % 2 != 0, 'One of check_algo and check_data must be True'\n",
    "    lookup_dic = algo_lookups if check_algo else data_lookups\n",
    "    for lookup_key, lookup_val in lookup_dic.items(): \n",
    "        if lookup_key in new_index:\n",
    "            new_index = lookup_val\n",
    "            return new_index\n",
    "    if verbose:\n",
    "        print(f\"Didn't find any entries for {index}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def task_heatmap(task, df_dict, control_key=\"control_ortho_init_cfg_data_repl_random\" ): \n",
    "    task_df = df_dict[task]['mean']\n",
    "    str_format = \"{:.2f}\" if task_df.loc[:].min()[0] < 1 else \"{:.0f}\"\n",
    "    subset_df = task_df[task_df.index.map(lambda x: 'no_ortho' not in x and 'froco' not in x)]\n",
    "    subset_df = subset_df[subset_df.index.map(partial(control_not_key, control_key=control_key))]\n",
    "    \n",
    "    std_sub_df = df_dict[task]['std']\n",
    "    std_sub_df = std_sub_df[std_sub_df.index.map(lambda x: 'no_ortho' not in x and 'froco' not in x)]\n",
    "    std_sub_df = std_sub_df[std_sub_df.index.map(partial(control_not_key, control_key=control_key))]\n",
    "    std_sub_df = std_sub_df.applymap(str_format.format)\n",
    "\n",
    "    normed_df = subset_df - subset_df.loc[control_key]\n",
    "    normed_df.index = normed_df.index.map(clean_index_val)\n",
    "    \n",
    "    subset_df = subset_df.applymap(str_format.format)\n",
    "    text_df = pd.DataFrame()\n",
    "    for col in subset_df.columns:\n",
    "        text_df[col] = subset_df[col].astype(str) + '(' + std_sub_df[col].astype(str) + ')'\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    sns.heatmap(normed_df, annot=text_df, annot_kws={\"fontsize\":13},\n",
    "                cmap=sns.diverging_palette(250, 10, s=60, l=45, as_cmap=True), center=0, fmt='.10')\n",
    "    plt.title(f\"Mean Return: {task}\")\n",
    "    \n",
    "task_dataframes = dict()\n",
    "\n",
    "for k in raw_data.keys(): \n",
    "    task_dataframes[k] = dict()\n",
    "    for stat in ['mean', 'std', 'n']:\n",
    "        columns = raw_data[k]['columns']\n",
    "        index = raw_data[k]['row_indexes']\n",
    "        rows = []\n",
    "        for raw_row in raw_data[k]['rows']: \n",
    "            rows.append([el[stat] for el in raw_row])\n",
    "\n",
    "        task_dataframes[k][stat] = pd.DataFrame(rows, index=index, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_dataframes = dict()\n",
    "\n",
    "\n",
    "for k in task_dataframes.keys():\n",
    "    data_dataframes[k] = dict()\n",
    "    for stat in task_dataframes[k].keys():\n",
    "        data_list = set([check_algo_or_data(ind, check_data=True) \n",
    "                     for ind in task_dataframes[k][stat].index]) - set([None])\n",
    "        ret_col = 'Average on all envs' if 'Average on all envs' \\\n",
    "                  in task_dataframes[k][stat].columns else 'return_mean'\n",
    "        algo_list = set([check_algo_or_data(algo, check_algo=True) \n",
    "                     for algo in task_dataframes[k][stat][ret_col].index.tolist()]) - set([None])\n",
    "        table = pd.DataFrame(np.zeros((len(algo_list), len(data_list))), \n",
    "                             index=algo_list,\n",
    "                             columns=data_list)\n",
    "\n",
    "        for exp_ident in task_dataframes[k][stat][ret_col].index:\n",
    "            data = check_algo_or_data(exp_ident, check_data=True)\n",
    "            algo = check_algo_or_data(exp_ident, check_algo=True)\n",
    "            if data and algo:\n",
    "                if not math.isnan(float(task_dataframes[k][stat][ret_col][exp_ident])):\n",
    "                    table[data][algo] = float(task_dataframes[k][stat][ret_col][exp_ident])\n",
    "        \n",
    "        # Move control to top of table\n",
    "        control_idx = 'Control (Random Init) '\n",
    "        idx = [control_idx] + [i for i in table.index if i != control_idx]\n",
    "        table = table.reindex(idx)\n",
    "        data_dataframes[k][stat] = table\n",
    "        \n",
    "\n",
    "data_dataframes['MatchRegions']['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task_heatmap('finger-spin', task_dataframes)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task_heatmap('cheetah-run', task_dataframes)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task_heatmap('MatchRegions', task_dataframes)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task_heatmap('MoveToRegion', task_dataframes)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task_heatmap('MatchRegions', data_dataframes, control_key=\"Control (Random Init) \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task_heatmap('MoveToRegion', data_dataframes, control_key=\"Control (Random Init) \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task_heatmap('cheetah-run', data_dataframes, control_key=\"Control (Random Init) \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task_heatmap('finger-spin', data_dataframes, control_key=\"Control (Random Init) \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print a table of il_train Area Under Loss Curve results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy import trapz\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# How many splits?\n",
    "num_split = 6\n",
    "\n",
    "# Exclude first n values in the loss list?\n",
    "start_count = 3\n",
    "\n",
    "def calculate_auc(y, dx=1):\n",
    "    return trapz(y, dx=dx)\n",
    "    \n",
    "train_expts = subexp_index.search(mode='il_train')\n",
    "all_configs = [subexp.get_merged_config(subexp_index) for subexp in train_expts]\n",
    "base_config, flat_configs = simplify_config_dicts(all_configs)\n",
    "flat_config_tups = [tuple(sorted(d.items())) for d in flat_configs]\n",
    "subexp_by_benchmark = {}\n",
    "for flat_cfg, subexp in zip(flat_config_tups, train_expts):\n",
    "    bench_key = tuple((k, v) for k, v in flat_cfg if k.startswith('env_') or k.startswith('venv_'))\n",
    "    subexp_by_benchmark.setdefault(bench_key, []).append((flat_cfg, subexp))\n",
    "\n",
    "display(HTML('<p><strong>Base config</strong></p>'))\n",
    "display(HTML('<p>Unless specified otherwise, all config dicts include these keys:</p>'))\n",
    "print(base_config)\n",
    "    \n",
    "raw_auc_data = {}\n",
    "\n",
    "for idx, (bench_key, cfgs_subexps) in enumerate(subexp_by_benchmark.items(), start=1):\n",
    "    # print out benchmark details\n",
    "    \n",
    "    # cluster subexperiments by config\n",
    "    by_cfg = {}\n",
    "    for tup_cfg, subexp in cfgs_subexps:\n",
    "        tup_cfg = tuple(k for k in tup_cfg if k not in bench_key)\n",
    "        by_cfg.setdefault(tup_cfg, []).append(subexp)\n",
    "\n",
    "    # load all progress files and figure out what columns we need\n",
    "    stats_dicts = {}\n",
    "    columns = set()\n",
    "    for cfg, subexp in cfgs_subexps:\n",
    "        d = dict(cfg)\n",
    "        exp_ident = d['il_train/exp_ident']\n",
    "        \n",
    "        if subexp.progress_path:\n",
    "            df = pd.read_csv(subexp.progress_path)\n",
    "            full_length = 400 if d['env_cfg/benchmark_name'] == 'dm_control' else 40\n",
    "            if len(df['loss']) != full_length:\n",
    "                print(f'Experiment {exp_ident} only has len(loss) {len(df[\"loss\"])}, skipping... ')\n",
    "#                 break\n",
    "            step_length = len(df['loss']) // num_split\n",
    "            stats_dict = {}\n",
    "            if len(df['loss']) < 10: \n",
    "                stats_dicts[subexp] = {}\n",
    "                continue \n",
    "            for step in range(step_length, len(df['loss']), step_length):\n",
    "                label = f\"step {step:02d}\"\n",
    "                stats_dict[label] = calculate_auc(df['loss'][start_count:step])\n",
    "            stats_dict[f\"step {len(df['loss'])}\"] = calculate_auc(df['loss'][start_count:len(df['loss'])])\n",
    "            stats_dicts[subexp] = stats_dict\n",
    "            columns |= stats_dict.keys()\n",
    "        else:\n",
    "            stats_dicts[subexp] = {}\n",
    "    columns = sorted(columns)\n",
    "    \n",
    "\n",
    "    raw_auc_data[bench_key[1][1]] = dict()\n",
    "    raw_auc_data[bench_key[1][1]]['columns'] = columns \n",
    "    \n",
    "    row_indexes = []\n",
    "    rows = []\n",
    "    for cfg, subexps in sorted(by_cfg.items(), key=lambda cfg_se: dict(cfg_se[0])['il_train/exp_ident']):\n",
    "        table_parts.append('<tr>')                                    # begin row\n",
    "\n",
    "        # cell containing config\n",
    "        if True:  # remove to show full config\n",
    "            d = dict(cfg)\n",
    "            exp_ident = d['il_train/exp_ident']\n",
    "            # bench_name = d['env_cfg/benchmark_name']\n",
    "            # task_name = d['env_cfg/task_name']\n",
    "            # desc_str = f'{exp_ident} ({bench_name}/{task_name})'\n",
    "            table_parts.append(f'<td style=\"border-collapse: collapse;\">{html.escape(exp_ident)}</td>')\n",
    "        else:\n",
    "            kv_cfg = ', '.join(f'{key}={value!r}' for key, value in cfg)\n",
    "            table_parts.append(f'<td style=\"max-width: 600px; border-collapse: collapse;\">{html.escape(kv_cfg)}</td>')\n",
    "            \n",
    "        # cells containing data\n",
    "        row_indexes.append(exp_ident)\n",
    "        row = []\n",
    "        for column in columns:\n",
    "            column_values = [stats_dicts[subexp][column] for subexp in subexps\n",
    "                             if subexp in stats_dicts.keys() and column in stats_dicts[subexp]]\n",
    "            if not column_values:\n",
    "                table_parts.append('<td style=\"border-collapse: collapse;\">-</td>')\n",
    "                row.append({'mean': np.nan, 'std': np.nan, 'n': np.nan})\n",
    "            else:\n",
    "                mean = np.mean(column_values)\n",
    "                std = np.std(column_values)\n",
    "                n = len(column_values)\n",
    "                table_parts.append(f'<td style=\"border-collapse: collapse;\">{mean:.3g}±{std:.1g} ({n})</td>')\n",
    "                row.append({'mean': mean, 'std': std, 'n': n})\n",
    "        rows.append(row)\n",
    "        # cells containing values\n",
    "\n",
    "        table_parts.append('</tr>')                                   # end row\n",
    "    raw_auc_data[bench_key[1][1]]['row_indexes'] = row_indexes\n",
    "    raw_auc_data[bench_key[1][1]]['rows'] = rows\n",
    "    table_parts.append('</table>')                                    # end table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task_auc_dataframes = dict()\n",
    "\n",
    "for k in raw_auc_data.keys(): \n",
    "    task_auc_dataframes[k] = dict()\n",
    "    for stat in ['mean', 'std', 'n']:\n",
    "        columns = raw_auc_data[k]['columns']\n",
    "        index = raw_auc_data[k]['row_indexes']\n",
    "        rows = []\n",
    "        for raw_row in raw_auc_data[k]['rows']: \n",
    "            rows.append([el[stat] for el in raw_row])\n",
    "\n",
    "        task_auc_dataframes[k][stat] = pd.DataFrame(rows, index=index, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def order_cols(cols): \n",
    "    integer_steps = [int(el.replace('step ', '')) for el in cols]\n",
    "    ordering = np.argsort(integer_steps)\n",
    "    col_ordering = cols[ordering]\n",
    "    return col_ordering \n",
    "\n",
    "def mostly_null(df, col): \n",
    "    return pd.isnull(df[col]).sum()/len(df) > .10\n",
    "\n",
    "def clean_df(df):\n",
    "    new_df = df.copy(deep=True)\n",
    "    for col in new_df.columns: \n",
    "        if mostly_null(new_df, col): \n",
    "            del new_df[col]\n",
    "    ordered_cols = order_cols(new_df.columns)\n",
    "    return new_df[ordered_cols]\n",
    "\n",
    "cleaned_auc_dataframes = dict()\n",
    "for task in task_auc_dataframes.keys(): \n",
    "    cleaned_auc_dataframes[task] = dict()\n",
    "    for stat in task_auc_dataframes[task].keys(): \n",
    "        cleaned_auc_dataframes[task][stat] = clean_df(task_auc_dataframes[task][stat])\n",
    "\n",
    "print(cleaned_auc_dataframes['finger-spin'])  # This is empty?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_auc_dataframes = dict()\n",
    "\n",
    "\n",
    "for k in cleaned_auc_dataframes.keys():\n",
    "    if k == 'finger-spin':\n",
    "        continue\n",
    "    data_auc_dataframes[k] = dict()\n",
    "    for stat in cleaned_auc_dataframes[k].keys():\n",
    "        data_list = set([check_algo_or_data(ind, check_data=True) \n",
    "                     for ind in cleaned_auc_dataframes[k][stat].index]) - set([None])\n",
    "        ret_col = 'step 40' if 'step 40' \\\n",
    "                  in cleaned_auc_dataframes[k][stat].columns else 'step 400'\n",
    "\n",
    "        algo_list = set([check_algo_or_data(algo, check_algo=True) \n",
    "                     for algo in cleaned_auc_dataframes[k][stat][ret_col].index]) - set([None])\n",
    "        table = pd.DataFrame(np.zeros((len(algo_list), len(data_list))), \n",
    "                             index=algo_list,\n",
    "                             columns=data_list)\n",
    "\n",
    "        for exp_ident in cleaned_auc_dataframes[k][stat][ret_col].index:\n",
    "            data = check_algo_or_data(exp_ident, check_data=True)\n",
    "            algo = check_algo_or_data(exp_ident, check_algo=True)\n",
    "            if data and algo:\n",
    "                if not math.isnan(float(cleaned_auc_dataframes[k][stat][ret_col][exp_ident])):\n",
    "                    table[data][algo] = float(cleaned_auc_dataframes[k][stat][ret_col][exp_ident])\n",
    "        \n",
    "        # Move control to top of table\n",
    "        control_idx = 'Control (Random Init) '\n",
    "        idx = [control_idx] + [i for i in table.index if i != control_idx]\n",
    "        table = table.reindex(idx)\n",
    "        data_auc_dataframes[k][stat] = table\n",
    "        \n",
    "\n",
    "data_auc_dataframes['MatchRegions']['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def task_auc_heatmap(task, df_dict, control_key=\"control_ortho_init_cfg_data_repl_random\" ): \n",
    "    str_format = \"{:.2f}\"\n",
    "    task_df = df_dict[task]['mean']\n",
    "    subset_df = task_df[task_df.index.map(lambda x: 'no_ortho' not in x and \n",
    "                                                    'froco' not in x and \n",
    "                                                     'actual' not in x)]\n",
    "    subset_df = subset_df[subset_df.index.map(partial(control_not_key, control_key=control_key))]\n",
    "    \n",
    "    std_df = df_dict[task]['std']\n",
    "    std_sub_df = std_df[std_df.index.map(lambda x: 'no_ortho' not in x and \n",
    "                                                    'froco' not in x and \n",
    "                                                     'actual' not in x)]\n",
    "    std_sub_df = std_sub_df[std_sub_df.index.map(partial(control_not_key, control_key=control_key))]\n",
    "    std_sub_df = std_sub_df.applymap(str_format.format)\n",
    "        \n",
    "    normed_df = subset_df - subset_df.loc[control_key]\n",
    "    normed_df.index = normed_df.index.map(clean_index_val)\n",
    "    \n",
    "    subset_df = subset_df.applymap(str_format.format)\n",
    "    text_df = pd.DataFrame()\n",
    "    for col in subset_df.columns:\n",
    "        text_df[col] = subset_df[col].astype(str) + '(' + std_sub_df[col].astype(str) + ')'\n",
    "        \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    sns.heatmap(normed_df, annot=text_df, annot_kws={\"fontsize\":13}, \n",
    "                cmap=sns.diverging_palette(10, 250, s=60, l=45, as_cmap=True), center=0, fmt='.20')\n",
    "    plt.title(f\"AUC: {task}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def prepare_files(index, mode, exp_index, out_dir):\n",
    "    \"\"\"\n",
    "    Create a folder named `out_dir`. This really just copies over files from il_train or il_test, as appropriate.\n",
    "    For instance, if il_train looks like this:\n",
    "    \n",
    "    il_train\n",
    "    │   ├── 1\n",
    "    │   │   ├── ...\n",
    "    │   │   ├── config.json\n",
    "    │   │   └── progress.csv\n",
    "    │   └── _sources\n",
    "    …\n",
    "    \n",
    "    Then the ouptut will look like this:\n",
    "    ├── progress\n",
    "    │   └── 1\n",
    "    │       ├── params.json   (same as config.json)\n",
    "    │       └── progress.csv\n",
    "    …\n",
    "\n",
    "    After you run this, you can execute viskit with: python viskit/frontend.py path/to/out_dir/\n",
    "    \"\"\"\n",
    "    experiments = index.search(mode=mode)\n",
    "    # compute merged configs (nested/hierarchical dicts), and\n",
    "    # also throw out experiments with no progress.csv\n",
    "    hierarchical_dicts = []\n",
    "    new_experiments = []\n",
    "    for experiment in experiments:\n",
    "        if not experiment.progress_path:\n",
    "            print(\"Skipping experiment\", experiment.ident, \"because it has no progress.csv\")\n",
    "            continue\n",
    "        merged_config = experiment.get_merged_config(exp_index)\n",
    "        hierarchical_dicts.append(merged_config)\n",
    "        new_experiments.append(experiment)\n",
    "    experiments = new_experiments\n",
    "\n",
    "    # first flatten all dicts\n",
    "    dicts = [dict(flatten_dict(d)) for d in hierarchical_dicts]\n",
    "    \n",
    "    # make sure that every dict has every key\n",
    "    all_keys = set()\n",
    "    for d in dicts:\n",
    "        all_keys |= d.keys()\n",
    "    for d in dicts:\n",
    "        for new_key in all_keys - d.keys():\n",
    "            d[new_key] = None\n",
    "    \n",
    "    # now generate outputs for experiments\n",
    "    for flat_config, experiment in zip(dicts, experiments):\n",
    "        exp_out_dir = os.path.join(out_dir, experiment.ident.replace('/', '-'))\n",
    "        os.makedirs(exp_out_dir, exist_ok=True)\n",
    "\n",
    "        params_json_path = os.path.join(exp_out_dir, 'params.json')\n",
    "        with open(params_json_path, 'w') as fp:\n",
    "            json.dump(flat_config, fp)\n",
    "\n",
    "        progress_out_path = os.path.join(exp_out_dir, 'progress.csv')\n",
    "        shutil.copyfile(experiment.progress_path, progress_out_path)\n",
    "        \n",
    "        if mode == 'il_test':\n",
    "            eval_json_path = experiment.eval_json_path\n",
    "            with open(eval_json_path, 'r') as fp:\n",
    "                eval_dict = json.load(fp)\n",
    "                \n",
    "            result_keys, result_vals = [], []\n",
    "            for key, value in eval_dict.items():\n",
    "#                 print(eval_dict.keys())\n",
    "                if key == 'return_mean' and value < 0.1:\n",
    "                    print(eval_dict['policy_path'])\n",
    "                if isinstance(value, str) or isinstance(value, int) or isinstance(value, float):\n",
    "                    result_keys.append(key)\n",
    "                    result_vals.append(value)\n",
    "            \n",
    "            with open(progress_out_path, mode='w') as result_file:\n",
    "                result_writer = csv.writer(result_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                result_writer.writerow(result_keys)\n",
    "                result_writer.writerow(result_vals)\n",
    "\n",
    "                \n",
    "prepare_files(subexp_index, 'repl', subexp_index, 'viskit-repl')\n",
    "prepare_files(subexp_index, 'il_train', subexp_index, 'viskit-il-train')\n",
    "prepare_files(subexp_index, 'il_test', subexp_index, 'viskit-il-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# matplotlib.use(\"tkagg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "output_path = f\"./graphs/{cluster_subpath.split('/')[1]}\"\n",
    "Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Images will be saved at {output_path}\")\n",
    "\n",
    "# include_ident_keywords = ['control', 'tcpc']\n",
    "# exclude_ident_keywords = ['mt', 'rand_only']\n",
    "\n",
    "include_ident_keywords = []\n",
    "exclude_ident_keywords = []\n",
    "\n",
    "def get_data(mode, data_type, include_ident_kw=None, exclude_ident_kw=None):\n",
    "    assert mode in ['repl', 'il_train', 'il_test']\n",
    "    assert data_type in ['loss', 'return']\n",
    "    expts = subexp_index.search(mode=mode)\n",
    "    all_configs = [subexp.get_merged_config(subexp_index) for subexp in expts]\n",
    "    base_config, flat_configs = simplify_config_dicts(all_configs)\n",
    "    flat_config_tups = [tuple(sorted(d.items())) for d in flat_configs]\n",
    "    subexp_by_benchmark = {}\n",
    "    for flat_cfg, subexp in zip(flat_config_tups, expts):\n",
    "        bench_key = tuple((k, v) for k, v in flat_cfg if k.startswith('env_') or k.startswith('venv_'))\n",
    "        subexp_by_benchmark.setdefault(bench_key, []).append((flat_cfg, subexp))\n",
    "    \n",
    "    \"\"\"\n",
    "        ret_dict has structure {'env_1': {'exp_ident_1': required_data, 'exp_ident_2': required_data, ...}, ...}\n",
    "        required_data can be either a list (i.e. loss over time) or a number (int or float, like return_mean)\n",
    "    \"\"\"\n",
    "    ret_dict = {}\n",
    "    for idx, (bench_key, cfgs_subexps) in enumerate(subexp_by_benchmark.items(), start=1):\n",
    "        # cluster subexperiments by config\n",
    "        by_cfg = {}\n",
    "        for tup_cfg, subexp in cfgs_subexps:\n",
    "            tup_cfg = tuple(k for k in tup_cfg if k not in bench_key)\n",
    "            by_cfg.setdefault(tup_cfg, []).append(subexp)\n",
    "\n",
    "        task_name = bench_key[1][1]\n",
    "        ret_dict[task_name] = {}\n",
    "        for tup_cfg, subexp in by_cfg.items():\n",
    "            exp_ident = tup_cfg[0][1]\n",
    "            \n",
    "            if include_ident_kw or exclude_ident_kw:\n",
    "                # Find out whether we want to include this set of exp in plots or not\n",
    "                include = [kw in exp_ident for kw in include_ident_kw]\n",
    "                exclude = [kw in exp_ident for kw in exclude_ident_kw]\n",
    "                if not True in include or True in exclude:\n",
    "                    continue\n",
    "                \n",
    "            ret_dict[task_name][exp_ident] = []\n",
    "            if data_type == 'loss':\n",
    "                for exp in subexp:\n",
    "                    ret_dict[task_name][exp_ident].append(pd.read_csv(subexp[0].progress_path)['loss'])\n",
    "                    \n",
    "            if data_type == 'return':  \n",
    "                assert mode == 'il_test', 'Only test experiments have return results.'\n",
    "                for exp in subexp:\n",
    "                    if exp.eval_json_path:\n",
    "                        eval_json_path = exp.eval_json_path\n",
    "                        with open(eval_json_path, 'r') as fp:\n",
    "                            eval_dict = json.load(fp)\n",
    "                        is_magical = 'full_data' in eval_dict.keys()\n",
    "                        if is_magical:\n",
    "                            stats_dict = {\n",
    "                                '-'.join(env_dict['test_env'].split('-')[:2]): env_dict['mean_score']\n",
    "                                for env_dict in eval_dict['full_data']\n",
    "                            }\n",
    "                            stats_dict['Average on all envs'] = eval_dict['return_mean']\n",
    "                        else:\n",
    "                            stats_dict = {'return_mean': eval_dict['return_mean']}\n",
    "\n",
    "                        ret_dict[task_name][exp_ident] = stats_dict\n",
    "    print(ret_dict.keys())\n",
    "    return ret_dict\n",
    "\n",
    "def plot_curves(data_dict):\n",
    "    sns.set(rc={'figure.figsize':(7, 6)})\n",
    "    for task_key, exp_results in data_dict.items():\n",
    "        df = None\n",
    "        col_name = []\n",
    "        plt.figure()\n",
    "        for exp_ident, value, in exp_results.items():\n",
    "            col_name = [f\"seed_{x}\" for x in range(len(value))]\n",
    "            col_name += ['step', 'exp_ident']\n",
    "            value.append([s for s in range(1, len(value[0])+1)])\n",
    "            value.append([exp_ident for s in range(1, len(value[0])+1)])\n",
    "            value = np.array(value).transpose(1, 0)\n",
    "            sub_df = pd.DataFrame(data=value, columns=col_name)\n",
    "            df = pd.concat([df, sub_df])\n",
    "        df = pd.melt(df, id_vars=['step', 'exp_ident'])\n",
    "        df['step'] = pd.to_numeric(df['step'])\n",
    "        df['value'] = pd.to_numeric(df['value'])\n",
    "        print(df.dtypes)\n",
    "        \n",
    "        ax = sns.lineplot(x='step', y='value', hue='exp_ident', data=df)\n",
    "        plt.setp(ax.get_legend().get_texts(), fontsize='12')\n",
    "        ax.set_title(bench_key)\n",
    "        ax.savefig(os.path.join(output_path, f'{task_key}_loss.png'))\n",
    "\n",
    "def plot_heatmap(data_dict):\n",
    "    print(\"Generating heatmap (might need to adjust figsize for better readability)\")\n",
    "    print(\"Red = performance > control (good)\")\n",
    "    sns.set(rc={'figure.figsize':(10, 15)})\n",
    "    for task_key, task_results in data_dict.items():\n",
    "        df = pd.DataFrame.from_dict(data=task_results)\n",
    "        control = df['control_no_ortho_init_cfg_data_repl_random']\n",
    "        df = df.sub(control, axis='rows').T\n",
    "        plt.figure()\n",
    "        ax = sns.heatmap(df, annot=True, annot_kws={\"fontsize\":13}, center=0, cmap=\"vlag\")\n",
    "        ax.set_title(task_key)\n",
    "        fig = ax.get_figure()\n",
    "        fig.savefig(os.path.join(output_path, f'{task_key}_heatmap.png'))\n",
    "        \n",
    "    \n",
    "# plot_curves(get_data('il_train', \n",
    "#                      'loss', \n",
    "#                      include_ident_kw=include_ident_keywords,\n",
    "#                      exclude_ident_kw=exclude_ident_keywords))\n",
    "\n",
    "plot_heatmap(get_data('il_test', \n",
    "                     'return', \n",
    "                     include_ident_kw=include_ident_keywords,\n",
    "                     exclude_ident_kw=exclude_ident_keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Interpret encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Save the encoder interpretation videos. Each sub_exp might take one or two minutes to save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "train_expts = subexp_index.search(mode='il_test')\n",
    "all_configs = [subexp.get_merged_config(subexp_index) for subexp in train_expts]\n",
    "base_config, flat_configs = simplify_config_dicts(all_configs)\n",
    "flat_config_tups = [tuple(sorted(d.items())) for d in flat_configs]\n",
    "subexp_by_benchmark = {}\n",
    "for flat_cfg, subexp in zip(flat_config_tups, train_expts):\n",
    "    bench_key = tuple((k, v) for k, v in flat_cfg if k.startswith('env_') or k.startswith('venv_'))\n",
    "    subexp_by_benchmark.setdefault(bench_key, []).append((flat_cfg, subexp))\n",
    "\n",
    "# Create a folder to save videos\n",
    "Path(f\"./runs/{cluster_subpath.split('/')[1]}\").mkdir(parents=True, exist_ok=True)\n",
    "interp_algo = 'saliency'\n",
    "                    \n",
    "for idx, (bench_key, cfgs_subexps) in enumerate(subexp_by_benchmark.items(), start=1):\n",
    "    # cluster subexperiments by config\n",
    "    by_cfg = {}\n",
    "    for tup_cfg, subexp in cfgs_subexps:\n",
    "        tup_cfg = tuple(k for k in tup_cfg if k not in bench_key)\n",
    "        by_cfg.setdefault(tup_cfg, []).append(subexp)\n",
    "\n",
    "    for tup_cfg, subexp in by_cfg.items():\n",
    "        exp = subexp[0]\n",
    "        encoder_path = exp.config['encoder_path']\n",
    "        if encoder_path:\n",
    "            for prefix, replacement in path_translations.items():\n",
    "                if encoder_path.startswith(prefix):\n",
    "                    encoder_path = replacement + encoder_path[len(prefix):]\n",
    "            command = \"python ../src/il_representations/scripts/interpret.py with \"\n",
    "            command += f\"log_dir=runs/{cluster_subpath.split('/')[1]} \"\n",
    "            command += f\"env_cfg.benchmark_name={exp.config['env_cfg']['benchmark_name']} \"\n",
    "            command += f\"env_cfg.task_name={exp.config['env_cfg']['task_name']} \"\n",
    "            command += f\"save_video=True \"\n",
    "            command += f\"chosen_algo={interp_algo} \"\n",
    "            command += f\"encoder_path={encoder_path} \"\n",
    "            command += f\"filename={exp.config['env_cfg']['task_name']}_{exp.config['exp_ident']} \"\n",
    "\n",
    "            print(f\"Generating videos for exp {exp.config['exp_ident']} on {exp.config['env_cfg']['task_name']}...\")\n",
    "            process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n",
    "            output, error = process.communicate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}