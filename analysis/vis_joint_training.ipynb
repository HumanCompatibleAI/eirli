{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising joint training runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import html\n",
    "import os\n",
    "import json\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "search_dirs = ['/scratch/sam/il-rep-jt-2021-05-18/joint_train_runs/']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a table of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_eval_json_files(search_dirs):\n",
    "    \"\"\"Look for eval.json and config.json files in the given search\n",
    "    directories. Yield pairs of (config path, eval.json path).\"\"\"\n",
    "    config_name = 'config.json'\n",
    "    eval_name = 'eval.json'\n",
    "    for search_dir in search_dirs:\n",
    "        for dirpath, _, filenames in os.walk(search_dir):\n",
    "            fn_set = set(filenames)\n",
    "            if config_name in fn_set and eval_name in fn_set:\n",
    "                dirpath = os.path.abspath(dirpath)\n",
    "                conf_path = os.path.join(dirpath, config_name)\n",
    "                eval_path = os.path.join(dirpath, eval_name)\n",
    "                yield conf_path, eval_path\n",
    "\n",
    "def read_and_combine_configs(conf_eval_path_iter):\n",
    "    \"\"\"Given an iterator that yields pairs of (config path, eval path), this\n",
    "    function reads the corresponding (JSON) files and merges them into the\n",
    "    same dict (specifically, it yields the config dict augmented with an\n",
    "    extra 'eval' key for eval.json results).\"\"\"\n",
    "    for conf_path, eval_path in conf_eval_path_iter:\n",
    "        with open(conf_path, 'r') as conf_fp, open(eval_path, 'r') as eval_fp:\n",
    "            conf_dict = json.load(conf_fp)\n",
    "            eval_dict = json.load(eval_fp)\n",
    "            yield {\n",
    "                'conf_path': conf_path,\n",
    "                'eval': eval_dict,\n",
    "                **conf_dict,\n",
    "            }\n",
    "    \n",
    "def make_pandas_table(search_dirs):\n",
    "    \"\"\"Look for joint_training.py runs underneath each of the given\n",
    "    search_dirs, then return results for all runs as as a big Pandas table\n",
    "    with the following columns:\n",
    "    \n",
    "    - exp_ident (human-readable name)\n",
    "    - conf_path (path to config.json, uniquely identifies run)\n",
    "    - train_env (train env for method)\n",
    "    - test_env (evaluation env for this particular row; there may be multiple\n",
    "      eval envs for each run)\n",
    "    - return (mean return on test_env)\n",
    "    \"\"\"\n",
    "    path_pair_iter = find_eval_json_files(search_dirs=search_dirs)\n",
    "    dict_iter = read_and_combine_configs(path_pair_iter)\n",
    "    frame_dict = collections.defaultdict(lambda: [])\n",
    "    for data_dict in dict_iter:\n",
    "        env_cfg = data_dict['env_cfg']\n",
    "        train_env = env_cfg['task_name']\n",
    "        eval_dict = data_dict['eval']\n",
    "        envs_returns = []\n",
    "        if env_cfg['benchmark_name'] == 'magical':\n",
    "            for env_eval_dict in eval_dict['full_data']:\n",
    "                # look for test_env, mean_score\n",
    "                test_env = env_eval_dict['test_env']\n",
    "                # shorten long names like \"MoveToCorner-Demo-v0\" to just \"-Demo\"\n",
    "                short_test_env = '-' + test_env.split('-')[1]\n",
    "                envs_returns.append((short_test_env, env_eval_dict['mean_score']))\n",
    "            envs_returns.append(('Average', eval_dict['return_mean']))\n",
    "        else:\n",
    "            envs_returns.append((train_env, eval_dict['return_mean']))\n",
    "        for test_env_name, test_env_return in envs_returns:\n",
    "            frame_dict['exp_ident'].append(data_dict['exp_ident'])\n",
    "            frame_dict['conf_path'].append(data_dict['conf_path'])\n",
    "            frame_dict['train_env'].append(train_env)\n",
    "            frame_dict['test_env'].append(test_env_name)\n",
    "            frame_dict['return'].append(test_env_return)\n",
    "    return pd.DataFrame.from_dict(frame_dict)\n",
    "\n",
    "def mean_std(arr):\n",
    "    \"\"\"Aggregation func for pd.pivot_table that displays mean and standard\n",
    "    deviation of array as a single string.\"\"\"\n",
    "    mean = np.mean(arr)\n",
    "    std = np.std(arr)\n",
    "    return f'{mean:.2f}±{std:.2f} ({len(arr)})'\n",
    "\n",
    "pandas_table = make_pandas_table(search_dirs=search_dirs)\n",
    "for train_env_name, sub_table in pandas_table.groupby('train_env'):\n",
    "    display(HTML(\n",
    "        f'<p><strong>Results for {html.escape(train_env_name)}</strong>'\n",
    "        '<br/>(numbers are mean return ± stddev and seed count)</p>'))\n",
    "    pivoted_table = pd.pivot_table(sub_table, columns=['test_env'], values='return', index='exp_ident', aggfunc=mean_std)\n",
    "    display(pivoted_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
