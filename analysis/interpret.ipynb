{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook offers interpretability tools from Captum to help visualize & understand what a model has learned. \n",
    "The tools include:\n",
    "- Primary Attribution: Evaluates contribution of each input feature to the output of a model.\n",
    "- Layer Attribution: Evaluates contribution of each neuron in a given layer to the output of the model.  \n",
    "\n",
    "For code blocks containing Layer Attribution methods, you may indicate which layer you want to inspect with that method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "# Make sure your cwd is the il-representations directory\n",
    "if os.getcwd().split('/')[-1] == 'analysis':\n",
    "    os.chdir(\"..\")\n",
    "print('Check cwd', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from il_representations.scripts.interpret import (prepare_network, process_data, save_img, saliency_, integrated_gradient_, \n",
    "                                                  deep_lift_, layer_conductance_, layer_gradcam_, layer_act_, \n",
    "                                                  choose_layer, interp_ex, get_venv)\n",
    "from il_representations.envs.config import benchmark_ingredient\n",
    "import il_representations.envs.auto as auto_env\n",
    "\n",
    "import sacred\n",
    "from sacred.observers import FileStorageObserver\n",
    "from sacred import Experiment\n",
    "from stable_baselines3.common.utils import get_device\n",
    "from captum.attr import LayerActivation, LayerGradientXActivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_interp_ex = Experiment('render_interp', ingredients=[benchmark_ingredient, interp_ex], interactive=True)\n",
    "interp_ex.observers.append(FileStorageObserver('runs/interpret_runs'))\n",
    "now = datetime.now().strftime(\"%m-%d-%Y-%H-%M-%S\")\n",
    "\n",
    "@interp_ex.config\n",
    "def config():\n",
    "    ##### These should be the only things you need to modify in this code block #####\n",
    "    encoder_path = os.path.join(os.getcwd(), 'runs/downloads/ActionConditionedTemporalCPC/249_epochs.ckpt')\n",
    "    assert os.path.isfile(encoder_path), f'Please double check if {encoder_path} exists.'\n",
    "    \n",
    "    \n",
    "    # Data settings\n",
    "    # The benchmark is set by detecting il_representations/envs/config's bench_defaults.benchmark_name\n",
    "    imgs = [30]  # index of the image to be inspected (int)\n",
    "    assert all(isinstance(im, int) for im in imgs), 'imgs list should contain integers only'\n",
    "\n",
    "    verbose = False\n",
    "    \n",
    "# When log_dir = None, the images will not be saved\n",
    "log_dir = os.path.join(os.getcwd(), f'runs/interpret_runs/interpret-{now}')\n",
    "os.system(f'mkdir -p {log_dir}')\n",
    "    #################################################################################\n",
    "\n",
    "print('log dir:', log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_interp_ex = Experiment('render_interp', ingredients=[benchmark_ingredient, interp_ex], interactive=True)\n",
    "\n",
    "@render_interp_ex.main\n",
    "def run():\n",
    "    venv = get_venv()\n",
    "    network = prepare_network(venv)\n",
    "    images, labels = process_data(venv)\n",
    "    return network, images, labels\n",
    "\n",
    "r = render_interp_ex.run()\n",
    "network = r.result[0]\n",
    "images = r.result[1]\n",
    "labels = r.result[2]\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliency\n",
    "\n",
    "Saliency is a simple approach for computing input attribution, returning the gradient of the output with respect to the input. This approach can be understood as taking a first-order Taylor expansion of the network at the input, and the gradients are simply the coefficients of each feature in the linear representation of the model. The absolute value of these coefficients can be taken to represent feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saliency():\n",
    "    for img, label in zip(images, labels):\n",
    "        original_img = img[0].permute(1, 2, 0).detach().numpy()\n",
    "        saliency_(network, img, label, original_img, log_dir, False)\n",
    "\n",
    "saliency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Gradients\n",
    "Integrated gradients represents the integral of gradients with respect to inputs along the path from a given baseline to input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrated_gradients():\n",
    "    for img, label in zip(images, labels):\n",
    "        original_img = img[0].permute(1, 2, 0).detach().numpy()\n",
    "        integrated_gradient_(network, img.contiguous(), label, original_img, log_dir, False)\n",
    "\n",
    "integrated_gradients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLift\n",
    "DeepLIFT is a back-propagation based approach that attributes a change to inputs based on the differences between the inputs and corresponding references (or baselines) for non-linear activations. As such, DeepLIFT seeks to explain the difference in the output from reference in terms of the difference in inputs from reference. DeepLIFT uses the concept of multipliers to \"blame\" specific neurons for the difference in output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_lift():\n",
    "    for img, label in zip(images, labels):\n",
    "        original_img = img[0].permute(1, 2, 0).detach().numpy()\n",
    "        deep_lift_(network, img, label, original_img, log_dir, False)\n",
    "\n",
    "deep_lift()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer GradCAM\n",
    "GradCAM is a layer attribution method designed for convolutional neural networks, and is usually applied to the last convolutional layer. GradCAM computes the gradients of the target output with respect to the given layer, averages for each output channel (dimension 2 of output), and multiplies the average gradient for each channel by the layer activations. The results are summed over all channels and a ReLU is applied to the output, returning only non-negative attributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_gradcam():\n",
    "    for img, label in zip(images, labels):\n",
    "        ##### These should be the only things you need to modify in this code block #####\n",
    "        module = 'encoder'\n",
    "        idx = 4\n",
    "        #################################################################################\n",
    "        chosen_layer = choose_layer(network, module, idx)\n",
    "        original_img = img[0].permute(1, 2, 0).detach().numpy()\n",
    "        assert isinstance(chosen_layer, torch.nn.Conv2d), 'GradCAM is usually applied to the last ' \\\n",
    "                                                          'convolutional layer in the network.'\n",
    "        if verbose:\n",
    "            print(f\"You have chosen {chosen_layer}\")\n",
    "        layer_gradcam_(network, chosen_layer, img, label, original_img, log_dir, False)\n",
    "\n",
    "layer_gradcam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Conductance\n",
    "Conductance combines the neuron activation with the partial derivatives of both the neuron with respect to the input and the output with respect to the neuron to build a more complete picture of neuron importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_conductance():\n",
    "    for img, label in zip(images, labels):\n",
    "        ##### These should be the only things you need to modify in this code block #####\n",
    "        module = 'encoder'\n",
    "        idx = 2\n",
    "        #################################################################################\n",
    "        chosen_layer = choose_layer(network, module, idx)\n",
    "        if verbose:\n",
    "            print(f\"You have chosen {chosen_layer}\")\n",
    "        layer_conductance_(network, chosen_layer, img, label, log_dir, show_imgs=True, columns=10)\n",
    "\n",
    "layer_conductance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer GradxAct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer Gradient X Activation is the analog of the Input X Gradient method for hidden layers in a network. It element-wise multiplies the layer's activation with the gradients of the target output with respect to the given layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_gradxact():\n",
    "    for img, label in zip(images, labels):\n",
    "        ##### These should be the only things you need to modify in this code block #####\n",
    "        module = 'encoder'\n",
    "        idx = 2\n",
    "        #################################################################################\n",
    "        chosen_layer = choose_layer(network, module, idx)\n",
    "        if verbose:\n",
    "            print(f\"You have chosen {chosen_layer}\")\n",
    "    \n",
    "        layer_act_(network, chosen_layer, LayerGradientXActivation, 'layer_GradXActivation',\n",
    "                   img, log_dir, show_imgs=True, attr_kwargs={'target': label})\n",
    "\n",
    "layer_gradxact()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer Activation is a simple approach for computing layer attribution, returning the activation of each neuron in the identified layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_activation():\n",
    "    for img, label in zip(images, labels):\n",
    "        ##### These should be the only things you need to modify in this code block #####\n",
    "        module = 'encoder'\n",
    "        idx = 2\n",
    "        #################################################################################\n",
    "        chosen_layer = choose_layer(network, module, idx)\n",
    "        if verbose:\n",
    "            print(f\"You have chosen {chosen_layer}\")\n",
    "    \n",
    "        layer_act_(network, chosen_layer, LayerActivation, 'layer_GradXActivation',\n",
    "                   img, log_dir, show_imgs=True, attr_kwargs={})\n",
    "\n",
    "layer_activation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
