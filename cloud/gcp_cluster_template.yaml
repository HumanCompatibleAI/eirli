# Template for launching a cluster on GCP. To use it, make a copy of the file &
# insert appropriate text into the bits that say "XXX-CUSTOMIZE-THIS-XXX".


# Unique identifier for head node and workers of this cluster
cluster_name: il-rep-XXX-CUSTOMIZE-THIS-XXX



#### AUTOSCALER CONFIG ####

# The minimum number of workers nodes to run in addition to the head
min_workers: 1
# The maximum number of workers nodes to run in addition to the head
max_workers: 16
# Number of workers to launch at cluster startup, in addition to the head
initial_workers: 16
# Can be changed to autoscale more aggressively.
autoscaling_mode: default
# The autoscaler will scale up the cluster to this target fraction of resource
# usage (0.0 means always upscale, 1.0 means never upscale even at 100% usage)
target_utilization_fraction: 0.9
# If a node is idle for this many minutes, it will be removed.
idle_timeout_minutes: 5



#### DOCKER, GCP, AUTHENTICATION ####

# Execute docker commands & open necessary ports.
docker:
  # the image below is taken from .circleci/config.yml
  # (if you change this file or that file, then remember to change the other
  # too)
  image: "humancompatibleai/il-representations:2021.10.30-r1"
  container_name: "il-rep-ray-tune"
  # Set to true if to always force-pull the latest image version (no cache).
  pull_before_run: False
  run_options:
    - "--mount"
    - "type=volume,source=project-data-volume,target=/project-data"
    - "--runtime=nvidia"
    - "--ulimit"
    - "core=0"

# Cloud-provider specific configuration.
provider:
  type: gcp
  region: us-west1
  availability_zone: us-west1-b
  project_id: methodical-tea-257021

# How Ray will authenticate with newly launched nodes.
# (Ray will auto-create an SSH key on your machine)
auth:
  ssh_user: ubuntu



#### HEAD AND WORKER NODE CONFIG ###

# Provider-specific config for the head node, e.g. instance type. By default
# Ray will auto-configure unspecified fields such as subnets and ssh-keys.
head_node:
  # n1-standard-2 is a cheap default machine for testing the cluster, but
  # you'll probably need something much beefier (possibly something with GPUs)
  # for real experiments.
  # machineType: n1-standard-8
  machineType: custom-24-65536
  disks:
    - boot: true
      autoDelete: true
      type: PERSISTENT
      initializeParams:
        diskSizeGb: 50
        # See https://cloud.google.com/compute/docs/images for more images
        sourceImage: projects/deeplearning-platform-release/global/images/family/common-cu101
        # sourceImage: projects/methodical-tea-257021/global/images/minecraft-gpu-image-v4

  # This is not strictly needed (it just means that we kill the head node
  # permanently if the underlying machine needs to be turned off for
  # maintenance, as opposed to migrating the head node to another machine, which
  # I assume would break Ray).
  scheduling:
    - onHostMaintenance: TERMINATE

  guestAccelerators:
    - acceleratorType: projects/methodical-tea-257021/zones/us-west1-b/acceleratorTypes/nvidia-tesla-v100
      acceleratorCount: 1
  metadata:
    items:
      - key: install-nvidia-driver
        value: "True"

  # Additional options can be found in in the compute docs at
  # https://cloud.google.com/compute/docs/reference/rest/v1/instances/insert

worker_nodes:
  # As above: you'll want to change the machine type on the workers to be
  # something more useful (possibly the same type as the head node).
  # machineType: n1-standard-32
  machineType: custom-48-167936
  disks:
    - boot: true
      autoDelete: true
      type: PERSISTENT
      initializeParams:
        diskSizeGb: 50
        # See https://cloud.google.com/compute/docs/images for more images
        sourceImage: projects/deeplearning-platform-release/global/images/family/common-cu101
        # sourceImage: projects/methodical-tea-257021/global/images/minecraft-gpu-image-v4
  # Run workers on preemtible instance by default.
  # Comment this out to use on-demand.
  scheduling:
    - preemptible: true
    - onHostMaintenance: TERMINATE

  guestAccelerators:
    - acceleratorType: projects/methodical-tea-257021/zones/us-west1-b/acceleratorTypes/nvidia-tesla-v100
      acceleratorCount: 4
  metadata:
    items:
      - key: install-nvidia-driver
        value: "True"


#### FILE HANDLING ####

# Files or directories to copy to the head and worker nodes
file_mounts: {
  # Format in Docker container:
  # "/path1/on/remote/machine": "/path1/on/local/machine",
  # (on the host, outside the Docker container, everything is prefixed with
  # /tmp/ray_tmp_mount/ for some reason; even absolute paths have this problem)
  "/tmp/ray-init-scripts": "ray-init-scripts",
}
# List of files or directories to copy from the head node to the worker nodes.
# The format is a list of paths. Not recommended.
cluster_synced_files: []
# Whether changes to dirs in file_mounts/cluster_synced_files on head should
# sync to workers continuously.
file_mounts_sync_continuously: False



#### NODE SETUP COMMANDS ####

# List of commands that will be run before `setup_commands`. If docker is
# enabled, they will run outside the container before docker setup.
initialization_commands:
  # mount NFS before starting docker so that we can create a bind mount
  # (the 'mount | grep nfs | â€¦' thing is a hack to get the NFS server's IP. The
  # alternative is to authenticate gcloud so that we can run get_server_ip.
  # nfs_mount.sh gets around this by *baking the IP into the script*; I should
  # consider baking IPs etc. into some separate config file instead.)
  # (also, the 'find /tmp/ray*' thing is to get around the fact that Ray puts
  # file mounts in different places on the head node and worker nodes; I can't
  # figure out a good way to get around it)
  - >-
    sudo bash "$(find /tmp/ray* -type d -name 'ray-init-scripts' -print -quit)/nfs_mount.sh"
    && nfs_server_ip="$(mount | grep nfs | grep -v docker | head -n 1 | cut -d : -f 1)"
    && docker volume create --driver local --opt type=nfs \
      --opt "o=addr=${nfs_server_ip},rw" --opt device=:/vol1 \
      project-data-volume
  # Update nvidia driver to version 450 (needed for Torch)
  - >-
    nfs_mount_path="$(mount | grep nfs | grep -v docker | head -n 1 | cut -d ' ' -f 3)"
    && nv_driver_major_version="$(nvidia-smi -q | grep 'Driver Version.*:.*' | cut -d : -f 2- | sed 's/\s//g' | cut -d . -f 1)"
    && ( [ "$nv_driver_major_version" -ge 450 ] || sudo sh "${nfs_mount_path}/driver/NVIDIA-Linux-x86_64-450.51.06.run" -s )
  # disable auto coredumps (these sometimes take more space than there is on local disk)
  - >-
    sudo sh -c 'echo "kernel.core_pattern=|/bin/false" > /etc/sysctl.d/50-coredump.conf'
    && sudo sysctl -p /etc/sysctl.d/50-coredump.conf
# List of shell commands to run to set up nodes (both head and worker).
setup_commands:
  # Start X server
  - bash /tmp/ray-init-scripts/start_x_server.sh
  # Check out and install source code
  # (remember to insert your desired branch name after the -b!)
  - >-
    test -e ~/il-rep
    || git -c core.sshCommand="ssh -v" \
      clone --depth 1 -b XXX-CUSTOMIZE-THIS-XXX \
      https://github.com/HumanCompatibleAI/il-representations.git ~/il-rep
  # Dependencies are baked into the Docker image, but we pip install
  # requirements.txt just in case there are any new ones pip can easily add.
  - cd ~/il-rep/ && pip install -r requirements.txt
  - cd ~/il-rep/ && pip install -e .
  # Finally, we set up the 'data' and 'runs' directories to point to places on
  # the project NFS volume. Some things to note:
  #
  # - Outside the docker container, the project data NFS volume is mounted at
  #   $CLIENT_MOUNT_POINT (e.g. /data/il-representations/). Inside the
  #   container, it's mounted at /project-data. The script below sets up a
  #   symlink inside the container pointing $CLIENT_MOUNT_POINT to
  #   /project-data. This is not strictly necessary (we could just use
  #   /project-data inside the container), but does make the inside of the
  #   container look a little bit more like the outside of the container.
  # - We put runs in their own special directory under $CLIENT_MOUNT_POINT that
  #   is labelled with the current date. Ideally this should mean that separate
  #   clusters end up with results in separate places.
  - >-
    cd ~/il-rep/
    && source ./cloud/nfs_config.sh
    && mount_parent="$(dirname "$CLIENT_MOUNT_POINT")"
    && mount_no_slash="$(echo "$CLIENT_MOUNT_POINT" | sed 's-/$--')"
    && ( test -e "$mount_parent" || mkdir -p "$mount_parent" )
    && ( test -e "$CLIENT_MOUNT_POINT" || ln -vs /project-data "$mount_no_slash" )
  - >-
    cd ~/il-rep/
    && source ./cloud/nfs_config.sh
    && mkdir -p "${CLIENT_MOUNT_POINT}/il-demos/"
    && ( test -e data || ln -vs "${CLIENT_MOUNT_POINT}/il-demos/" data )
    && cluster_name="cluster-XXX-CUSTOMIZE-THIS-XXX"
    && runs_path="${CLIENT_MOUNT_POINT}/cluster-data/${cluster_name}"
    && ( test -e runs || ( mkdir -p "$runs_path" && chmod -R +rX "$runs_path" && ln -vs "$runs_path" runs ) )
  # Set up MuJoCo
  - >-
    cd ~/il-rep/
    && source ./cloud/nfs_config.sh
    && cp "${CLIENT_MOUNT_POINT}/mjkey.txt" ~/.mujoco/mjkey.txt
# Custom commands that will be run on the head node after common setup.
head_setup_commands: []
# ("pip install google-api-python-client==1.7.8" used to be a head_setup
# command, but it's unnecessary now that it's in requirements.txt)
# Custom commands that will be run on worker nodes after common setup.
worker_setup_commands: []



#### RAY START COMMANDS (both head & workers) ####

# Command to start ray on the head node. You don't need to change this.
head_start_ray_commands:
  - ray stop
  # see https://github.com/ray-project/ray/issues/9368 for an explanation of the
  # AUTOSCALER_MAX_NUM_FAILURES line.
  - >-
    ulimit -n 65536;
    export AUTOSCALER_MAX_NUM_FAILURES=1000;
    ray start
    --head
    --port=6379
    --object-manager-port=8076
    --autoscaling-config=~/ray_bootstrap_config.yaml
# Command to start ray on worker nodes. You don't need to change this.
worker_start_ray_commands:
  - ray stop
  - >-
    ulimit -n 65536;
    export AUTOSCALER_MAX_NUM_FAILURES=1000;
    ray start
    --address=$RAY_HEAD_IP:6379
    --object-manager-port=8076
