# Template for launching a cluster on GCP. To use it, make a copy of the file &
# insert appropriate text into the bits that say "XXX-CUSTOMIZE-THIS-XXX".


# Unique identifier for head node and workers of this cluster
cluster_name: il-rep-XXX-CUSTOMIZE-THIS-XXX



#### AUTOSCALER CONFIG ####

# The minimum number of workers nodes to run in addition to the head
min_workers: 1
# The maximum number of workers nodes to run in addition to the head
max_workers: 5
# Number of workers to launch at cluster startup, in addition to the head
initial_workers: 1
# Can be changed to autoscale more aggressively.
autoscaling_mode: default
# The autoscaler will scale up the cluster to this target fraction of resource
# usage (0.0 means always upscale, 1.0 means never upscale even at 100% usage)
target_utilization_fraction: 0.8
# If a node is idle for this many minutes, it will be removed.
idle_timeout_minutes: 5



#### DOCKER, GCP, AUTHENTICATION ####

# Execute docker commands & open necessary ports.
docker:
  # the image below is taken from .circleci/config.yml
  image: "humancompatibleai/il-representations:2020.09.14-r1"
  container_name: "il-rep-ray-tune"
  # Set to true if to always force-pull the latest image version (no cache).
  pull_before_run: False
  run_options: []  # Extra options to pass into "docker run"

# Cloud-provider specific configuration.
provider:
  type: gcp
  region: us-west1
  availability_zone: us-west1-b
  project_id: methodical-tea-257021

# How Ray will authenticate with newly launched nodes.
# (Ray will auto-create an SSH key on your machine)
auth:
  ssh_user: ubuntu



#### HEAD AND WORKER NODE CONFIG ###

# Provider-specific config for the head node, e.g. instance type. By default
# Ray will auto-configure unspecified fields such as subnets and ssh-keys.
head_node:
  # n1-standard-2 is a cheap default machine for testing the cluster, but
  # you'll probably need something much beefier (possibly something with GPUs)
  # for real experiments.
  machineType: n1-standard-2  # XXX-CUSTOMIZE-THIS-XXX
  disks:
    - boot: true
      autoDelete: true
      type: PERSISTENT
      initializeParams:
        diskSizeGb: 50
        # See https://cloud.google.com/compute/docs/images for more images
        sourceImage: projects/deeplearning-platform-release/global/images/family/common-cu101
        # sourceImage: projects/methodical-tea-257021/global/images/minecraft-gpu-image-v4

    # Additional options can be found in in the compute docs at
    # https://cloud.google.com/compute/docs/reference/rest/v1/instances/insert

worker_nodes:
  # As above: you'll want to change the machine type on the workers to be
  # something more useful (possibly the same type as the head node).
  machineType: n1-standard-2  # XXX-CUSTOMIZE-THIS-XXX
  disks:
    - boot: true
      autoDelete: true
      type: PERSISTENT
      initializeParams:
        diskSizeGb: 50
        # See https://cloud.google.com/compute/docs/images for more images
        sourceImage: projects/deeplearning-platform-release/global/images/family/common-cu101
        # sourceImage: projects/methodical-tea-257021/global/images/minecraft-gpu-image-v4
  # Run workers on preemtible instance by default.
  # Comment this out to use on-demand.
  scheduling:
    - preemptible: true



#### FILE HANDLING ####

# Files or directories to copy to the head and worker nodes
file_mounts: {
  # "/path1/on/remote/machine": "/path1/on/local/machine",
  "/tmp/ssh_deploy_key": "./ssh_deploy_key",
  "/tmp/ssh_deploy_key.pub": "./ssh_deploy_key.pub",
  "/tmp/nfs_mount.sh": "nfs_mount.sh",
  "/tmp/nfs_config.sh": "nfs_config.sh"
}
# List of files or directories to copy from the head node to the worker nodes.
# The format is a list of paths. Not recommended.
cluster_synced_files: []
# Whether changes to dirs in file_mounts/cluster_synced_files on head should
# sync to workers continuously.
file_mounts_sync_continuously: False



#### NODE SETUP COMMANDS ####

# List of commands that will be run before `setup_commands`. If docker is
# enabled, they will run outside the container before docker setup.
initialization_commands:
  # mount NFS before starting docker so that we can create a bind mount
  - bash /tmp/nfs_mount.sh
# List of shell commands to run to set up nodes (both head and worker).
setup_commands:
  # Set up SSH key
  - >-
    ( test -e ~/.ssh || ( mkdir -p ~/.ssh && chmod 0700 ~/.ssh ) )
    && cp -r /tmp/ssh_deploy_key* ~/.ssh
    && chmod 0600 ~/.ssh/ssh_deploy_key*
  # Check out and install source code
  # (remember to insert your desired branch name after the -b!)
  # (Aside: I (Sam) couldn't find an easy alternative to the
  # StrictHostKeyChecking=no hack. If it's any consolation, Ray is at least
  # MITM-vulnerable too, so connection to Github is not our weakest link...)
  - >-
    git -c core.sshCommand="ssh -v -i /root/.ssh/ssh_deploy_key -o StrictHostKeyChecking=no" \
      clone --depth 1 -b branch-name-XXX-CUSTOMIZE-THIS-XXX \
      git@github.com:HumanCompatibleAI/il-representations.git ~/il-rep
  # Dependencies are baked into the Docker image, but we pip install
  # requirements.txt just in case there are any new ones pip can easily add.
  - cd ~/il-rep/ && pip install -r requirements.txt
  - cd ~/il-rep/ && pip install -e .
  # Mount project filesystem
  - cd ~/il-rep/ && ./cloud/nfs_mount.sh
# Custom commands that will be run on the head node after common setup.
head_setup_commands: []
# ("pip install google-api-python-client==1.7.8" used to be a head_setup
# command, but it's unnecessary now that it's in requirements.txt)
# Custom commands that will be run on worker nodes after common setup.
worker_setup_commands: []



#### RAY START COMMANDS (both head & workers) ####

# Command to start ray on the head node. You don't need to change this.
head_start_ray_commands:
  - ray stop
  - >-
    ulimit -n 65536;
    ray start
    --head
    --port=6379
    --object-manager-port=8076
    --autoscaling-config=~/ray_bootstrap_config.yaml
# Command to start ray on worker nodes. You don't need to change this.
worker_start_ray_commands:
  - ray stop
  - >-
    ulimit -n 65536;
    ray start
    --address=$RAY_HEAD_IP:6379
    --object-manager-port=8076
