

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Dataset Creation &amp; Environment Specification &mdash; Representations for Sequence Learning 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Representation Learner Usage" href="representation_learner_usage.html" />
    <link rel="prev" title="Welcome to Representations for Sequence Learning’s documentation!" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Representations for Sequence Learning
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Dataset Creation &amp; Environment Specification</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#sacred-ingredients">Sacred ingredients</a></li>
<li class="toctree-l2"><a class="reference internal" href="#creating-gym-environments">Creating Gym environments</a></li>
<li class="toctree-l2"><a class="reference internal" href="#loading-demonstrations-from-their-native-format">Loading demonstrations from their ‘native’ format</a></li>
<li class="toctree-l2"><a class="reference internal" href="#the-webdataset-format">The webdataset format</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#high-level-interface-and-configuration">High-level interface and configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#on-disk-format">On-disk format</a></li>
<li class="toctree-l3"><a class="reference internal" href="#writing-datasets-in-the-webdataset-format">Writing datasets in the webdataset format</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loading-data-from-shard-to-minibatch">Loading data: from shard to minibatch</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#adding-support-for-a-new-benchmark">Adding support for a new benchmark</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="representation_learner_usage.html">Representation Learner Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="rep_learner_design.html">Representation Learner Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="il_representations.html">il_representations package</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Representations for Sequence Learning</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Dataset Creation &amp; Environment Specification</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/data.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="dataset-creation-environment-specification">
<span id="dataset-and-environment"></span><h1>Dataset Creation &amp; Environment Specification<a class="headerlink" href="#dataset-creation-environment-specification" title="Permalink to this headline">¶</a></h1>
<p><em>This README current as of 2020-11-29. Some aspects of the data-loading
pipeline will probably change eventually, so expect this doc to be
slightly out of date if you’re reading it 2-3 months (or more) in the
future. Bug Sam if there are serious problems with it.</em></p>
<p>This document explains the abstractions that we are using to load
demonstration data and create Gym environments. These abstractions are
intended provide a reasonably uniform internal interface across all of
all the benchmarks supported by the il-representations project (Atari,
dm_control, MAGICAL, Minecraft, etc.).</p>
<div class="section" id="sacred-ingredients">
<h2>Sacred ingredients<a class="headerlink" href="#sacred-ingredients" title="Permalink to this headline">¶</a></h2>
<p>The data-loading pipeline is configured using three different Sacred
ingredients:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/HumanCompatibleAI/il-representations/blob/77b557654d1d48a966e84b22d101b06f8ca5b476/src/il_representations/envs/config.py#L10-L68">env_cfg_ingredient</a>:
In principle, this ingredient contains all the information necessary
to create a Gym environment for a specific combination of benchmark
and task. The two most important config keys are <code class="docutils literal notranslate"><span class="pre">benchmark_name</span></code>
(which identifies whether the current benchmark is MAGICAL, or
dm_control, or something else), and <code class="docutils literal notranslate"><span class="pre">task_name</span></code> (which identifies
the current task within the selected benchmark; e.g. finger-spin or
MoveToCorner-Demo-v0). There are also some benchmark-specific config
keys for, e.g., preprocessing.</p></li>
<li><p><a class="reference external" href="https://github.com/HumanCompatibleAI/il-representations/blob/77b557654d1d48a966e84b22d101b06f8ca5b476/src/il_representations/envs/config.py#L71-L92">venv_opts_ingredient</a>:
Additional options required to construct a vecenv (e.g. the number of
environments to run in parallel).</p></li>
<li><p><a class="reference external" href="https://github.com/HumanCompatibleAI/il-representations/blob/77b557654d1d48a966e84b22d101b06f8ca5b476/src/il_representations/envs/config.py#L95-L173">env_data_ingredient</a>:
Contains paths to data files on disk. Has quite a few
dataset-specific keys, particularly for loading ‘native’-format
datasets (as described further down).</p></li>
</ul>
<p>Not every script requires every one of the above ingredients. For
instance, testing a trained policy requires <code class="docutils literal notranslate"><span class="pre">env_cfg_ingredient</span></code> to
determine which environment to evaluate on, and <code class="docutils literal notranslate"><span class="pre">venv_opts_ingredient</span></code>
to construct a vecenv, but not <code class="docutils literal notranslate"><span class="pre">env_data_ingredient</span></code>. As a result, the
three components have been separated out to minimise the number of
redundant Sacred config options for each script.</p>
</div>
<div class="section" id="creating-gym-environments">
<h2>Creating Gym environments<a class="headerlink" href="#creating-gym-environments" title="Permalink to this headline">¶</a></h2>
<p>Gym environments can be created with
<a class="reference external" href="https://github.com/HumanCompatibleAI/il-representations/blob/77b557654d1d48a966e84b22d101b06f8ca5b476/src/il_representations/envs/auto.py#L68-L109">auto.load_vec_env()</a>.
This uses <code class="docutils literal notranslate"><span class="pre">env_cfg['benchmark_name']</span></code> (from the <code class="docutils literal notranslate"><span class="pre">env_cfg_ingredient</span></code>
Sacred ingredient) to dispatch to a benchmark-specific routine for
creating vecenvs. The benchmark-specific routines make use of both
<code class="docutils literal notranslate"><span class="pre">env_cfg['task_name']</span></code> and (possibly) some benchmark-specific keys in
<code class="docutils literal notranslate"><span class="pre">env_cfg</span></code> to, e.g., apply appropriate preprocessors. In addition,
<code class="docutils literal notranslate"><span class="pre">auto.load_vec_env()</span></code> uses <code class="docutils literal notranslate"><span class="pre">venv_opts</span></code> (from the
<code class="docutils literal notranslate"><span class="pre">venv_opts_ingredient</span></code> Sacred ingredient) to determine, e.g., how many
environments the vecenv should run in parallel.</p>
</div>
<div class="section" id="loading-demonstrations-from-their-native-format">
<h2>Loading demonstrations from their ‘native’ format<a class="headerlink" href="#loading-demonstrations-from-their-native-format" title="Permalink to this headline">¶</a></h2>
<p>Demonstrations for each benchmark were originally generated in a few
different formats. For instance, the MAGICAL reference demonstrations
are distributed as pickles, while the Atari demonstrations were saved as
Numpy <code class="docutils literal notranslate"><span class="pre">.npz</span></code> files (see the <a class="reference external" href="https://docs.google.com/document/d/1YrXFCmCjdK2HK-WFrKNUjx03pwNUfNA6wwkO1QexfwY/edit#heading=h.akt76l1pl1l5">data formats
GDoc</a>
for more detail). The
<a class="reference external" href="https://github.com/HumanCompatibleAI/il-representations/blob/77b557654d1d48a966e84b22d101b06f8ca5b476/src/il_representations/envs/auto.py#L26-L45">auto.load_dataset_dict()</a>
function provides a uniform interface to these formats.</p>
<p>Like <code class="docutils literal notranslate"><span class="pre">auto.load_vec_env()</span></code>, the <code class="docutils literal notranslate"><span class="pre">auto.load_dict_dataset()</span></code> function
uses <code class="docutils literal notranslate"><span class="pre">env_cfg['benchmark_name']</span></code> to dispatch to a benchmark-specific
data-loading function that is able to read the benchmark’s on-disk data
format. Those benchmark-specific loading functions in turn look at
benchmark-specific config keys in <code class="docutils literal notranslate"><span class="pre">env_data</span></code> (from
<code class="docutils literal notranslate"><span class="pre">env_data_ingredient</span></code>) to locate the demonstrations. For example,
<code class="docutils literal notranslate"><span class="pre">benchmark_name=&quot;magical&quot;</span></code> dispatches to
<a class="reference external" href="https://github.com/HumanCompatibleAI/il-representations/blob/77b557654d1d48a966e84b22d101b06f8ca5b476/src/il_representations/envs/magical_envs.py#L25-L100">envs.magical_envs.load_data()</a>,
which looks up the current task name (i.e. <code class="docutils literal notranslate"><span class="pre">env_cfg[&quot;task_name&quot;]</span></code>) in
<code class="docutils literal notranslate"><span class="pre">env_data[&quot;magical_demo_dirs&quot;]</span></code> to determine where the relevant
demonstrations are stored.</p>
<p>Regardless of the value of <code class="docutils literal notranslate"><span class="pre">env_cfg['benchmark_name']</span></code>,
<code class="docutils literal notranslate"><span class="pre">auto.load_dataset_dict()</span></code> always returns a dict with the following
keys:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">obs</span></code>: an <code class="docutils literal notranslate"><span class="pre">N*C*H*W</span></code> array of observations associated with states.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">next_obs</span></code>: an <code class="docutils literal notranslate"><span class="pre">N*C*H*W</span></code> array of observations associated with
the state <em>after</em> the corresponding one in <code class="docutils literal notranslate"><span class="pre">obs</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">acts</span></code>: an <code class="docutils literal notranslate"><span class="pre">N*A_1*A_2*…</span></code> array of actions, where <code class="docutils literal notranslate"><span class="pre">A_1*A_2*…</span></code> is
the shape of the action space.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dones</span></code>: an length-<code class="docutils literal notranslate"><span class="pre">N</span></code> array of bools indicating whether the
corresponding state was terminal.</p></li>
</ul>
<p>Note that <code class="docutils literal notranslate"><span class="pre">N</span></code> here is the sum of the lengths of all trajectories in
the dataset; trajectories are concatenated together to form each value
in the returned dictionary. It is possible to segment the values back
into trajectories by looking at the <code class="docutils literal notranslate"><span class="pre">dones</span></code> array.</p>
<p>Loading all demonstrations into a single dictionary in memory has one
major advantage, but also a few drawbacks. The advantage is that it’s
easy to manipulate the demonstrations: you can figure out how many
trajectories you have with <code class="docutils literal notranslate"><span class="pre">np.sum(data_dict[&quot;dones&quot;])</span></code>, or randomly
index into time steps in order to construct shuffled batches. The three
main disadvantages are:</p>
<ul class="simple">
<li><p>Loading all demonstrations into a dict can use a lot of memory.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">auto.load_dict_dataset()</span></code> relies on the <code class="docutils literal notranslate"><span class="pre">env_cfg_ingredient</span></code>
Sacred ingredient, which only supports specifying a single training
task. Thus it is not easy to extend <code class="docutils literal notranslate"><span class="pre">auto.load_dict_dataset()</span></code> so
that it can load multitask data.</p></li>
<li><p>It’s hard to invert <code class="docutils literal notranslate"><span class="pre">auto.load_dict_dataset()</span></code> into a function for
<em>saving</em> trajectories, since it needs to support several different
(benchmark-specific) data formats. However, it would be convenient to
have such an inverse function, since that would allow us to write
benchmark-agnostic code for generating new repL training data (e.g.
generating training data from random rollouts).</p></li>
</ul>
<p>For these reasons, we also have a second data format…</p>
</div>
<div class="section" id="the-webdataset-format">
<h2>The webdataset format<a class="headerlink" href="#the-webdataset-format" title="Permalink to this headline">¶</a></h2>
<p>In addition to the in-memory dict format generated by
<code class="docutils literal notranslate"><span class="pre">auto.load_dict_dataset()</span></code>, we also have a second set of independent
data-saving and data-loading machinery based on the
<a class="reference external" href="https://github.com/tmbdev/webdataset/">webdataset</a> spec/library.
This section briefly explains how webdataset works, and how we use it to
load data for the <code class="docutils literal notranslate"><span class="pre">run_rep_learner</span></code> script.</p>
<div class="section" id="high-level-interface-and-configuration">
<h3>High-level interface and configuration<a class="headerlink" href="#high-level-interface-and-configuration" title="Permalink to this headline">¶</a></h3>
<p>Within our codebase, the high-level interface for loading datasets in
the webdataset format is the <a class="reference external" href="https://github.com/HumanCompatibleAI/il-representations/blob/77b557654d1d48a966e84b22d101b06f8ca5b476/src/il_representations/envs/auto.py#L126-L200">auto.load_wds_datasets()
function</a>.
This takes a list of configurations for single-task datasets, and
returns an list containing one webdataset <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> for each task. It
is then the responsibility of the calling code to apply any necessary
preprocessing steps to those <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s (e.g. target pair
construction) and to multiplex the datasets with an
<code class="docutils literal notranslate"><span class="pre">InterleavedDataset</span></code>. These abstractions are explained further down
the page.</p>
<p>The configuration syntax for <code class="docutils literal notranslate"><span class="pre">auto.load_wds_datasets()</span></code> is exactly the
syntax used for the <code class="docutils literal notranslate"><span class="pre">dataset_configs</span></code> configuration option in
<code class="docutils literal notranslate"><span class="pre">run_rep_learner.py</span></code>, and as such deserves some further explanation.
Each element of the list passed to <code class="docutils literal notranslate"><span class="pre">auto.load_wds_datasets()</span></code> is a
dict which may contain the following keys:</p>
<blockquote>
<div><dl class="simple">
<dt>{</dt><dd><p># the type of data to be loaded
“type”: “demos” | “random” |
# a dictionary containing some subset of configuration keys from <cite>env_cfg_ingredient</cite>
“env_cfg”: {…},</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>Both the <code class="docutils literal notranslate"><span class="pre">&quot;type&quot;</span></code> key and the <code class="docutils literal notranslate"><span class="pre">&quot;env_cfg&quot;</span></code> key are optional.
<code class="docutils literal notranslate"><span class="pre">&quot;type&quot;</span></code> defaults to <code class="docutils literal notranslate"><span class="pre">&quot;demos&quot;</span></code>, and <code class="docutils literal notranslate"><span class="pre">&quot;env_cfg&quot;</span></code> defaults to the
current configuration of <code class="docutils literal notranslate"><span class="pre">env_cfg_ingredient</span></code>. If any sub-keys are
provided in <code class="docutils literal notranslate"><span class="pre">&quot;env_cfg&quot;</span></code>, then they are recursively combined with the
current configuration of <code class="docutils literal notranslate"><span class="pre">&quot;env_cfg_ingredient&quot;</span></code>. This allows one to
define new dataset configurations that override only some aspects of the
current <code class="docutils literal notranslate"><span class="pre">&quot;env_cfg_ingredient&quot;</span></code> configuration.</p>
<p>This configuration syntax might be clearer with a few examples:</p>
<ul class="simple">
<li><p>Training on random rollouts and demonstrations using the current
benchmark name from <code class="docutils literal notranslate"><span class="pre">env_cfg_ingredient</span></code>:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dataset_configs</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;demos&quot;</span><span class="p">},</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;random&quot;</span><span class="p">}]</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Training on demos from both the default task from
<code class="docutils literal notranslate"><span class="pre">env_cfg_ingredient</span></code>, and another task called “finger-spin”. Notice
that this time the first config dict does not have <em>any</em> keys; this is
equivalent to using <code class="docutils literal notranslate"><span class="pre">{&quot;type&quot;:</span> <span class="pre">&quot;demos&quot;}</span></code> as we did above.
<code class="docutils literal notranslate"><span class="pre">&quot;type&quot;:</span> <span class="pre">&quot;demos&quot;</span></code> is also implicit in the second dict.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dataset_configs</span> <span class="o">=</span> <span class="p">[{},</span> <span class="p">{</span><span class="s2">&quot;env_cfg&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;task_name&quot;</span><span class="p">:</span> <span class="s2">&quot;finger-spin&quot;</span><span class="p">}}]</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Combining the examples above, here is an example that trains on demos
from the current task, random rollouts from the current task, demos from
a second task called <code class="docutils literal notranslate"><span class="pre">&quot;finger-spin&quot;</span></code>, and random rollouts from a third
task called <code class="docutils literal notranslate"><span class="pre">&quot;cheetah-run&quot;</span></code>:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dataset_configs</span> <span class="o">=</span> <span class="p">[</span>
<span class="p">{},</span>
<span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;random&quot;</span><span class="p">},</span>
<span class="p">{</span><span class="s2">&quot;env_cfg&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;task_name&quot;</span><span class="p">:</span> <span class="s2">&quot;finger-spin&quot;</span><span class="p">}},</span>
<span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;random&quot;</span><span class="p">,</span> <span class="s2">&quot;env_cfg&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;task&quot;</span><span class="p">:</span> <span class="s2">&quot;cheetah-run&quot;</span><span class="p">}},</span>
<span class="p">]</span>
</pre></div>
</div>
<p>Since <code class="docutils literal notranslate"><span class="pre">env_cfg_ingredient</span></code> does not allow for specification of data
paths, the configurations passed to <code class="docutils literal notranslate"><span class="pre">auto.load_wds_datasets()</span></code> also do
not allow for paths to be overridden. Instead, the data for a given
configuration will always be loaded using the following path template:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">data_root</span><span class="o">&gt;/</span><span class="n">processed</span><span class="o">/&lt;</span><span class="n">data_type</span><span class="o">&gt;/&lt;</span><span class="n">task_key</span><span class="o">&gt;/&lt;</span><span class="n">benchmark_name</span><span class="o">&gt;</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">data_root</span></code> is a config variable from <code class="docutils literal notranslate"><span class="pre">env_data_ingredient</span></code>, and
<code class="docutils literal notranslate"><span class="pre">data_type</span></code> is the <code class="docutils literal notranslate"><span class="pre">&quot;type&quot;</span></code> defined in the dataset config dict.
<code class="docutils literal notranslate"><span class="pre">&quot;task_key&quot;</span></code> is <code class="docutils literal notranslate"><span class="pre">env_cfg[&quot;task_name&quot;]</span></code> (which is taken from
<code class="docutils literal notranslate"><span class="pre">env_cfg_ingredient</span></code> by default, but can be overridden in any of the
config dicts passed to <code class="docutils literal notranslate"><span class="pre">auto.load_wds_datasets()</span></code>). Likewise,
<code class="docutils literal notranslate"><span class="pre">benchmark_name</span></code> defaults to <code class="docutils literal notranslate"><span class="pre">env_cfg[&quot;benchmark_name&quot;]</span></code>, but can be
overridden by dataset config dicts.</p>
</div>
<div class="section" id="on-disk-format">
<h3>On-disk format<a class="headerlink" href="#on-disk-format" title="Permalink to this headline">¶</a></h3>
<p>The webdataset-based on-disk format (which I’ll just call the
“webdataset format”) is very simple: a dataset is composed of ‘shards’,
each of which is a single tar archive. Each tar archive contains a list
of files like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>_metadata.meta.pickle
frame_000.acts.pickle
frame_000.dones.pickle
frame_000.frame.pickle
frame_000.infos.pickle
frame_000.next_obs.pickle
frame_000.obs.pickle
frame_000.rews.pickle
frame_001.acts.pickle
frame_001.dones.pickle
frame_001.frame.pickle
frame_001.infos.pickle
frame_001.next_obs.pickle
frame_001.obs.pickle
frame_001.rews.pickle
frame_002.acts.pickle
frame_002.dones.pickle
frame_002.frame.pickle
frame_002.infos.pickle
frame_002.next_obs.pickle
…
</pre></div>
</div>
<p>For the datasets generated by our code, all shards begin with a
<code class="docutils literal notranslate"><span class="pre">_metadata.meta.pickle</span></code> file holding metadata identifying a specific
benchmark and task (e.g. it contains the observation space for the task,
as well as a configuration for <code class="docutils literal notranslate"><span class="pre">env_data_ingredient</span></code> that can be used
to re-instantiate the whole Gym environment). The remaining files
represent time steps in a combined set of trajectories. For instance,
the <code class="docutils literal notranslate"><span class="pre">frame_000.*</span></code> files represent the observation encountered at the
first step of the first trajectory, the action taken, the infos dict
returned, the next observation encountered, etc. As with the arrays
returned by <code class="docutils literal notranslate"><span class="pre">auto.load_dict_dataset()</span></code>, trajectories are concatenated
together in the tar file, and can be separated back out by inspecting
the <code class="docutils literal notranslate"><span class="pre">dones</span></code> values.</p>
<p><em>Aside:</em> users of the webdataset library usually do not include
file-level metadata of the kind stored in <code class="docutils literal notranslate"><span class="pre">_metadata.meta.pickle</span></code>. Our
code has some additional abstractions (such as
<code class="docutils literal notranslate"><span class="pre">read_dataset.ILRDataset</span></code>) which ensure that the file-level metadata
is accessible from Python, and which also ensure that
<code class="docutils literal notranslate"><span class="pre">_metadata.meta.pickle</span></code> is not accidentally treated as an additional
“frame” when reading the tar file. This is discussed further below.</p>
</div>
<div class="section" id="writing-datasets-in-the-webdataset-format">
<h3>Writing datasets in the webdataset format<a class="headerlink" href="#writing-datasets-in-the-webdataset-format" title="Permalink to this headline">¶</a></h3>
<p>Convenience functions for writing datasets are located in
<a class="reference external" href="https://github.com/HumanCompatibleAI/il-representations/blob/77b557654d1d48a966e84b22d101b06f8ca5b476/src/il_representations/data/write_dataset.py">data.write_dataset</a>.
In particular, this contains a helper function for extracting metadata
from an <code class="docutils literal notranslate"><span class="pre">env_cfg_ingredient</span></code> configuration
(<a class="reference external" href="https://github.com/HumanCompatibleAI/il-representations/blob/77b557654d1d48a966e84b22d101b06f8ca5b476/src/il_representations/data/write_dataset.py#L21-L49">get_meta_dict()</a>)
and a helper for writing a series of frames to an
appropriately-structured tar archive
(<a class="reference external" href="https://github.com/HumanCompatibleAI/il-representations/blob/77b557654d1d48a966e84b22d101b06f8ca5b476/src/il_representations/data/write_dataset.py#L52-L71">write_frames()</a>).
These helpers are currently used by two scripts, which are good
resources for understanding how to write webdatasets:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/HumanCompatibleAI/il-representations/blob/77b557654d1d48a966e84b22d101b06f8ca5b476/src/il_representations/scripts/mkdataset_demos.py">mkdataset_demos.py</a>:
Converts between dict format and webdataset format. That is, the
script loads a dataset from its ‘native’ on-disk format into a dict
using <code class="docutils literal notranslate"><span class="pre">auto.load_dict_dataset()</span></code>, then writes the data into a new
webdataset.</p></li>
<li><p><a class="reference external" href="https://github.com/HumanCompatibleAI/il-representations/blob/77b557654d1d48a966e84b22d101b06f8ca5b476/src/il_representations/scripts/mkdataset_random.py">mkdataset_random.py</a>:
Generates random rollouts on a specified environment and then saves
them into a webdataset.</p></li>
</ul>
</div>
<div class="section" id="loading-data-from-shard-to-minibatch">
<h3>Loading data: from shard to minibatch<a class="headerlink" href="#loading-data-from-shard-to-minibatch" title="Permalink to this headline">¶</a></h3>
<p>The main abstraction provided by the webdataset library is the
<a class="reference external" href="https://github.com/tmbdev/webdataset/blob/b208b15f6a5b14b8e597d5fc182f6945e6390d84/webdataset/dataset.py#L409-L462">Dataset</a>
class. Given a series of URLs pointing to different shards of a dataset,
this class iterates over the contents over the shards, one URL at a
time. webdataset’s <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> is a valid subclass of Torch’s
<code class="docutils literal notranslate"><span class="pre">IterableDataset</span></code>, so it can be directly passed to Torch’s
<code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>. A webdataset <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> can also be also be composed
with Python generators in order to create a data preprocessing pipeline.
For repL, our pipeline looks something like this:</p>
<ol class="arabic simple">
<li><p><strong>Generic decoding/grouping code:</strong> The first stage of the pipeline
does bookkeeping like decoding <code class="docutils literal notranslate"><span class="pre">.pickle</span></code> files in the shard into
Python objects (instead of yielding raw bytes as training samples!),
and grouping samples with the same frame prefix (e.g. <code class="docutils literal notranslate"><span class="pre">frame000</span></code>,
<code class="docutils literal notranslate"><span class="pre">frame001</span></code>, etc.). Our code also uses a <a class="reference external" href="https://github.com/HumanCompatibleAI/il-representations/blob/77b557654d1d48a966e84b22d101b06f8ca5b476/src/il_representations/data/read_dataset.py#L13-L71">special Dataset
subclass</a>
that makes the contents of <code class="docutils literal notranslate"><span class="pre">_metadata.meta.pickle</span></code> accessible as a
dataset instance attribute.</p></li>
<li><p><strong>Target pair constructor:</strong> After training samples are decoded, they
can be grouped into context and target pairs for the purpose of repL.
The <a class="reference external" href="https://github.com/HumanCompatibleAI/il-representations/blob/77b557654d1d48a966e84b22d101b06f8ca5b476/src/il_representations/algos/pair_constructors.py#L39-L49">TargetPairConstructor
interface</a>
is simply a generator that processes one sample at a time from the
dataset iterator. Since samples are written and read in temporal
order, it is possible for these generators to, e.g., create target
and context pairs out of temporally adjacent pairs
(<a class="reference external" href="https://github.com/HumanCompatibleAI/il-representations/blob/77b557654d1d48a966e84b22d101b06f8ca5b476/src/il_representations/algos/pair_constructors.py#L117-L163">example</a>).</p></li>
<li><p><strong>Optional shuffling:</strong> Since webdataset <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s are
<code class="docutils literal notranslate"><span class="pre">Iterable</span></code> datasets, it is not possible to shuffle the entire
dataset in-memory. Instead, the repL code can optionally apply a
pipeline stage that buffers a small, fixed number of samples in
memory, and pops a randomly-selected sample from this buffer at each
iteration. This introduces a small degree of randomisation that may
be helpful for optimisation. Note that this step also breaks temporal
order, so it must come <em>after</em> target pair construction.</p></li>
<li><p><strong>Interleaving:</strong> Recall that one of the aims of the
<code class="docutils literal notranslate"><span class="pre">webdataset</span></code>-based repL data system was to support multitask
training. In principle, we could do this by passing shards from
different datasets to webdataset’s <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> class. However, since
shards are iterated over sequentially (modulo the shuffle buffer),
this would mean that the network would exclusively see samples from
the first dataset for the first few batches, then exclusively samples
from the second dataset, and so on. Instead, we create a separate
webdataset <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> for each sub-dataset used for multitask
training, and then multiplex those <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s with
<a class="reference external" href="https://github.com/HumanCompatibleAI/il-representations/blob/77b557654d1d48a966e84b22d101b06f8ca5b476/src/il_representations/data/read_dataset.py#L74-L116">InterleavedDataset</a>.
<code class="docutils literal notranslate"><span class="pre">InterleavedDataset</span></code> is an <code class="docutils literal notranslate"><span class="pre">IterableDataset</span></code> that repeatedly
chooses a sub-dataset uniformly at random and yields a single sample
from that. This ensures that the different sub-datasets are equally
represented (on average) in each batch.</p></li>
</ol>
<p>The steps above yield a single <code class="docutils literal notranslate"><span class="pre">IterableDataset</span></code> which can be passed
to Torch’s <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>. The <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> is then responsible for
combining samples from the iterator into batches, just as it would with
any other <code class="docutils literal notranslate"><span class="pre">IterableDataset</span></code>.</p>
</div>
</div>
<div class="section" id="adding-support-for-a-new-benchmark">
<h2>Adding support for a new benchmark<a class="headerlink" href="#adding-support-for-a-new-benchmark" title="Permalink to this headline">¶</a></h2>
<p>These are the rough steps required to add support for a new benchmark:</p>
<ol class="arabic simple">
<li><p>Create benchmark-specific routines for creating vec envs; loading
data in a dict format; and inferring the equivalent Gym name of an
environment. Add these to a module in <code class="docutils literal notranslate"><span class="pre">il_representations.envs</span></code>,
much like <code class="docutils literal notranslate"><span class="pre">il_representations.envs.magical</span></code>.</p></li>
<li><p>Add any required config variables for the new benchmark to
<code class="docutils literal notranslate"><span class="pre">il_representations.envs.config</span></code>, and update
<code class="docutils literal notranslate"><span class="pre">il_representations.envs.auto</span></code> so that the routines make use of the
new config variables to dispatch to the dataset-specific routines in
<code class="docutils literal notranslate"><span class="pre">il_representations.envs.auto</span></code>.</p></li>
<li><p>Update <code class="docutils literal notranslate"><span class="pre">il_representations.scripts.il_test</span></code> to do execute
dataset-specific code is required for evaluation of policies in the
new environment.</p></li>
<li><p>Add demonstrations for the new environment to svm and perceptron (in
<code class="docutils literal notranslate"><span class="pre">/scatch/sam/il-demos</span></code>). Also update
<code class="docutils literal notranslate"><span class="pre">convert_all_to_new_data_format.sh</span></code> (in
<code class="docutils literal notranslate"><span class="pre">il_representations/scripts/</span></code>) to produce webdataset-format
demonstrations for the new benchmark, and add those to svm/perceptron
too. Repeat these steps to copy demonstrations to GCP, too. In
particular, if you copy them to
<code class="docutils literal notranslate"><span class="pre">/scratch/sam/il-representations-gcp-volume/il-demos/</span></code> in svm or
perceptron then they should get automatically synced to GCP.</p></li>
<li><p>Finally, add configs for one environment from the new benchmark to
<code class="docutils literal notranslate"><span class="pre">test_support.py</span></code>, and add test fixtures to <code class="docutils literal notranslate"><span class="pre">tests/data</span></code>. This
will make it possible to unit test the new benchmark. Since these
data fixtures are stored in the repo, I suggest using only 1-2
trajectories for each fixture.</p></li>
</ol>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="representation_learner_usage.html" class="btn btn-neutral float-right" title="Representation Learner Usage" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="index.html" class="btn btn-neutral float-left" title="Welcome to Representations for Sequence Learning’s documentation!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Center for Human-Compatible AI.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>