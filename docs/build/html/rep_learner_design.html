

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Design Principles &mdash; Representations for Imitation Learning 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Interpreting Results" href="interpret.html" />
    <link rel="prev" title="Representation Learner Usage" href="representation_learner_usage.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Representations for Imitation Learning
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="data.html">Dataset Creation &amp; Environment Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="representation_learner_usage.html">Representation Learner Usage</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Design Principles</a></li>
<li class="toctree-l1"><a class="reference internal" href="interpret.html">Interpreting Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="il_representations.html">il_representations package</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Representations for Imitation Learning</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Design Principles</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/rep_learner_design.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="design-principles">
<span id="rep-learner-design"></span><h1>Design Principles<a class="headerlink" href="#design-principles" title="Permalink to this headline">¶</a></h1>
<p>The design of this repo’s core RepresentationLearner abstraction was based around a deconstruction of common RepL learners
into their component parts in a way that we feel strikes a good balance between flexibility and reuseability.</p>
<p>To explain a bit more about the different components, let’s look at four algorithms: a VAE, a Temporal VAE, SimCLR, and Temporal Contrastive Predictive Coding (CPC).
What are the differences between different pairs of these?</p>
<ul class="simple">
<li><p>A VAE and TemporalVAE function basically the same way, except that, instead of your reconstruction target being the
same as the input frame, it’s one frame forward in a trajectory</p></li>
<li><p>Between a VAE and SimCLR, the former tries to reconstruct an input frame after a bottleneck, and the latter tries to
achieve similarity with the representation of a differently-augmented input frame after a bottleneck + projection layer. So
between these two algorithms, you can identify the differences of (1) using augmentation vs not, and (2) using a contrastive
loss rather than a reconstructive one. However, they’re the same insofar as the “target” in both cases is (some modification of)
the input frame itself. This same analogy holds between TemporalVAE and TemporalCPC: one is reconstructive, one contrastive,
but both use a temporally-offset target</p></li>
<li><p>Between SimCLR and TemporalCPC, the central difference is that, instead of calculating a contrastive loss between
augmented versions of the same frame, TemporalCPC calculates a contrastive loss between an augmented frame_t and an augmented
frame t+k</p></li>
</ul>
<p>Now that you’ve got some practice deconstructing algorithms this way, it may be easier to follow the deconstruction we chose for this codebase.
At the most general level, we define representation learners as following the pattern of:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">L</span> <span class="o">=</span> <span class="n">Loss</span><span class="p">(</span><span class="n">Decoder</span><span class="p">(</span><span class="n">Encoder</span><span class="p">(</span><span class="n">Context</span><span class="p">),</span> <span class="n">OptionalExtraContext</span><span class="p">)),</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">Encoder</span><span class="p">(</span><span class="n">Target</span><span class="p">))</span>
</pre></div>
</div>
<p>In our framework, different learners are differentiated from one another by their different implementations of each of
these components.</p>
<ol class="arabic simple">
<li><p>First, we take in a dataset and construct Context, Target pairs from it. This is done by a <cite>TargetPairConstructor</cite> object.
The most common strategies for constructing pairs are  identity (where context and target are the same frame)
or temporal offset (where context and target are temporally-offset frames). However, there are also situations
where the target is not an image input, for example, when we want to predict an action, in the case of
inverse dynamics (ID). In that case, <cite>target</cite> is the action vector. <cite>context</cite> objects are always image frames.
We also need to handle the case where we need two forms of input information to predict the target: for example,
predicting <cite>action</cite> given two contiguous frames in ID, or predicting next frame given current frame and action,
in a Dynamics model. These are stored in an optional <cite>extra_context</cite> object, which some encoders
have logic to deal with, but which others ignore</p></li>
<li><p>We augment our context frame (and optionally our target) according to some strategy defined by an <cite>Augmenter</cite>. This is
a fairly simple process, and the main variation here is (a) whether to augment both context and target or just context,
and (b) what augmentations, if any, to apply.</p></li>
<li><p>Then, we take our possibly-augmented dataset of Context, Target pairs and run a batch through the <cite>Encoder</cite>.
The job of the encoder is to map a context (and optionally also a target) into a <cite>z</cite> representation vector.
This component is what we transfer to downstream models.</p></li>
<li><p>In some cases, we need to have a <cite>Decoder</cite> to do postprocessing on the representation learned by the encoder, before
it is passed to the loss. This component is dropped after RepL training, and not used in downstream finetuning. In the case
of a contrastive loss with a projection head, this might be a simple MLP. Or, in the case of a VAE, where loss is calculated
on a reconstructed image, this may be a more complex network to reconstruct an image from a bottlenecked representation.
Sometimes, it uses <cite>extra_context</cite>, in addition to <cite>context</cite>, to construct an input that can be given to the loss function,
as in the cases of Dynamics and Inverse Dynamics mentioned above, where you want to use action vector or next frame
respectively as part of the prediction of the other quantity.
A decoder may also simply be the identity, in cases where no projection head is used.</p></li>
<li><p>Once we have run our batches through the encoding and decoding processes, it’s time to calculate a loss, with a
<cite>RepresentationLoss</cite> object. This loss takes the decoded context, decoded target, and sometimes the encoded
context as input. (The latter is basically only used in the case of VAE, where part of our loss is pulling
the <cite>p(z|x)</cite> distribution closer to a Gaussian prior).</p></li>
</ol>
<p>Given these components, let’s compare a few of the definitions of algorithms we gave as examples above.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">VariationalAutoencoder</span><span class="p">(</span><span class="n">RepresentationLearner</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A basic variational autoencoder that tries to reconstruct the</span>
<span class="sd">    current frame, and calculates a VAE loss over current frame pixels,</span>
<span class="sd">    using reconstruction loss and a KL divergence between learned</span>
<span class="sd">    z distribution and a normal prior</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># ... &lt;repeated machinery&gt; ...</span>
        <span class="n">algo_hardcoded_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">encoder</span><span class="o">=</span><span class="n">VAEEncoder</span><span class="p">,</span>
                                     <span class="n">decoder</span><span class="o">=</span><span class="n">PixelDecoder</span><span class="p">,</span>
                                     <span class="n">batch_extender</span><span class="o">=</span><span class="n">IdentityBatchExtender</span><span class="p">,</span>
                                     <span class="n">augmenter</span><span class="o">=</span><span class="n">NoAugmentation</span><span class="p">,</span>
                                     <span class="n">loss_calculator</span><span class="o">=</span><span class="n">VAELoss</span><span class="p">,</span>
                                     <span class="n">target_pair_constructor</span><span class="o">=</span><span class="n">IdentityPairConstructor</span><span class="p">,</span>
                                     <span class="n">decoder_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">observation_space</span><span class="o">=</span><span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;observation_space&#39;</span><span class="p">],</span>
                                                         <span class="n">encoder_arch_key</span><span class="o">=</span><span class="n">dec_encoder_cls_key</span><span class="p">,</span>
                                                         <span class="n">sample</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="interpret.html" class="btn btn-neutral float-right" title="Interpreting Results" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="representation_learner_usage.html" class="btn btn-neutral float-left" title="Representation Learner Usage" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Center for Human-Compatible AI.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>