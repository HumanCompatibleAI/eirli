import os
import os.path as osp
from sacred import Experiment
from sacred.observers import FileStorageObserver
import ray
from ray import tune
from il_representations.scripts.run_rep_learner import represent_ex
from il_representations.scripts.il_train import il_train_ex
from il_representations.scripts.il_test import il_test_ex
from utils import sacred_copy, update, detect_ec2, trial_name_string

chain_ex = Experiment('chain', ingredients=[represent_ex, il_train_ex, il_test_ex])
output_root = 'runs/chain_runs'
cwd = os.getcwd()


@ray.remote
def run_single_exp(inner_ex_config, config, log_dir, exp_name):
    """
    Run a specified experiment. We could not pass each Sacred experiment in because they are not pickle serializable,
    which is not supported by Ray.

    params:
        inner_ex_config: The current experiment's default config.
        config: The config generated by Ray tune for hyperparameter tuning
        log_dir: The log directory of current chain experiment.
        exp_name: Specify the experiment type in ['rep', 'il_train', 'il_test']
    """
    from il_representations.scripts.run_rep_learner import represent_ex
    from il_representations.scripts.il_train import il_train_ex
    from il_representations.scripts.il_test import il_test_ex

    assert exp_name in ['rep', 'il_train', 'il_test']
    if exp_name == 'rep':
        inner_ex = represent_ex
    elif exp_name == 'il_train':
        inner_ex = il_train_ex
    elif exp_name == 'il_test':
        inner_ex = il_test_ex
    else:
        raise

    inner_ex_dict = dict(inner_ex_config)
    merged_config = update(inner_ex_dict, config)
    observer = FileStorageObserver(osp.join(log_dir, exp_name))
    inner_ex.observers.append(observer)
    ret_val = inner_ex.run(config_updates=merged_config)
    return ret_val.result


def run_end2end_exp(rep_ex_config, il_train_ex_config, il_test_ex_config, config, log_dir):
    """
    Run representation learning, imitation learning's training and testing sequentially.

    Params:
        rep_ex_config: Config of represent_ex. It's the default config plus any modifications we might have made
                       in an macro_experiment config update.
        il_train_ex_config: Config of il_train_ex. It's the default config plus any modifications we might have made
                       in an macro_experiment config update.
        il_test_ex_config: Config of il_test_ex. It's the default config plus any modifications we might have made
                       in an macro_experiment config update.
        config: The config generated by Ray tune for hyperparameter tuning
        log_dir: The log directory of current chain experiment.
    """

    # Run representation learning
    pretrain_handles = run_single_exp.remote(rep_ex_config, config['rep'], log_dir, 'rep')
    pretrain_result = ray.get(pretrain_handles)

    # Run il train
    config['il_train'].update({'encoder_path': osp.join(cwd, pretrain_result['encoder_path'])})
    il_train_handles = run_single_exp.remote(il_train_ex_config, config['il_train'], log_dir, 'il_train')
    il_train_result = ray.get(il_train_handles)

    # Run il test
    config['il_test'].update({'policy_path': osp.join(cwd, il_train_result['model_path'])})
    il_test_handles = run_single_exp.remote(il_test_ex_config, config['il_test'], log_dir, 'il_test')
    il_test_result = ray.get(il_test_handles)

    tune.report(loss=il_test_result['loss'])
    tune.report(reward=il_test_result['reward'])


@chain_ex.config
def base_config(representation_learning, il_train, il_test):
    exp_name = "grid_search"
    metric = 'reward'
    assert metric in ['reward', 'loss']
    spec = {
        'rep': {
            'algo': tune.grid_search(['MoCo', 'SimCLR']),
        },
        'il_train': {
            'algo': tune.grid_search(['bc']),
        },
        'il_test': {

        }
    }

    rep_ex_config = dict(representation_learning)
    rep_ex_config['pretrain_epochs'] = 1
    rep_ex_config['root_dir'] = cwd

    il_train_ex_config = dict(il_train)
    il_train_ex_config['bc_n_epochs'] = 1
    il_train_ex_config['root_dir'] = cwd

    il_test_ex_config = dict(il_test)


@chain_ex.main
def run(exp_name, metric, spec, rep_ex_config, il_train_ex_config, il_test_ex_config):
    rep_ex_config = sacred_copy(rep_ex_config)
    il_train_ex_config = sacred_copy(il_train_ex_config)
    il_test_ex_config = sacred_copy(il_test_ex_config)
    spec = sacred_copy(spec)
    log_dir = chain_ex.observers[0].dir

    def trainable_function(config):
        run_end2end_exp(rep_ex_config, il_train_ex_config, il_test_ex_config, config, log_dir)

    if detect_ec2():
        ray.init(address="auto")
    else:
        ray.init()

    rep_run = tune.run(
        trainable_function,
        name=exp_name,
        config=spec
    )

    best_config = rep_run.get_best_config(metric=metric)
    print(f"Best config is: {best_config}")
    print("Results available at: ")
    print(rep_run._get_trial_paths())


def main():
    observer = FileStorageObserver('runs/chain_runs')
    chain_ex.observers.append(observer)
    chain_ex.run_commandline()


if __name__ == '__main__':
    main()
