"""Tools for reading datasets"""

import os
import pickle

import webdataset as wds
from webdataset.dataset import group_by_keys


class ILRDataset(wds.Dataset):
    """Modification of webdataset's `Dataset` class so that it is suitable for
    il-representations. Specific changes:

    - It looks for a `_metadata.meta.pickle` file at the beginning of the data
      archive & makes it available as a class attribute (`.meta`). This is
      useful for storing dataset-global metadata.
    - (possibly some other stuff, I haven't figured it out yet)
    """
    _meta = None  # this will be set by __init__.new_pipeline

    def __init__(self, urls, *args, initial_pipeline=None, **kwargs):
        def meta_pipeline(data_iter):
            """Pipeline that extracts the first element of the archive to use
            as metadata, then lets the remainder go through the rest of the
            pipeline."""
            first_item = next(data_iter)
            assert isinstance(first_item, tuple) and len(first_item) == 2
            first_item_name, first_item_data = first_item
            if first_item_name != '_metadata.meta.pickle':
                raise ValueError(
                    "data archive does not have '_metadata.meta.pickle' as its "
                    "first file; was it produced with one of the mkdataset_* "
                    "scripts?")
            metadata = pickle.loads(first_item_data)
            self._meta = metadata
            # TODO(sam): maybe add a check to ensure that each produced item is
            # *not* another _metadata.meta.pickle file (which would be weird).
            yield from data_iter

        # this only supports one data source because I'm not sure how to handle
        # inconsistent metadata between data sources
        assert len(urls) == 1, \
            f"for now this only supports one data source (urls={urls!r})"

        if initial_pipeline is None:
            # group_by_keys is part of the default initial pipeline, so we
            # include it here as a default too
            init_pipeline = [meta_pipeline, group_by_keys()]
        else:
            # if initial_pipeline is given to Dataset, then it does not use
            # group_by_keys, so we omit it in the analogous case here
            init_pipeline = [meta_pipeline, initial_pipeline]

        super().__init__(urls, *args, initial_pipeline=init_pipeline, **kwargs)

    @property
    def meta(self):
        if not self._meta:
            # load dataset just to make sure we have metadata
            next(iter(self))
            assert self._meta is not None, \
                "self._meta should be populated on first sample draw, " \
                "but it was not (may be bug in this class, or empty dataset)"
        return self._meta


def strip_extensions(dataset):
    """Strip extensions from the keys in dicts generated by ILRDataset."""
    for item in dataset:
        # this might fail if user did not use group_keys() transform on dataset
        # (this is applied by default, but can disappear if you override
        # initial_pipeline)
        assert isinstance(item, dict), type(item)
        new_item = {
            k.split('.', 1)[0]: v for k, v in item.items()
        }
        yield new_item


def load_ilr_dataset(file_path):
    abs_path = os.path.abspath(file_path)
    # wds doesn't use standard URL parser for some reason. I think they're just
    # looking for a file: prefix and then treating the rest of the string as a
    # file path.
    url = 'file:' + abs_path
    return ILRDataset([url]) \
        .decode() \
        .pipe(strip_extensions)
